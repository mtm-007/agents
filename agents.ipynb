{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fc7dd25-c0c5-4529-a500-db3e87975b50",
   "metadata": {},
   "source": [
    "### RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb95e953-63a2-421f-b4a6-9a9a946da18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "\n",
    "docs_url = 'https://github.com/alexeygrigorev/llm-rag-workshop/raw/main/notebooks/documents.json'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()\n",
    "\n",
    "documents = []\n",
    "\n",
    "for course in documents_raw:\n",
    "    course_name = course['course']\n",
    "\n",
    "    for doc in course['documents']:\n",
    "        doc['course'] = course_name\n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8e7a2a8-40da-4700-b535-86e6149a0d99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.append.AppendableIndex at 0x7eae24859220>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from minsearch import AppendableIndex\n",
    "\n",
    "index = AppendableIndex(\n",
    "    text_fields = [\"question\", \"text\", \"section\"],\n",
    "    keyword_fields = [\"course\"]\n",
    ")\n",
    "\n",
    "index.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b25824b-16a5-42fd-a509-c902cafc898c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'While following tutorial 13.2 , when running ./spark-submit.sh streaming.py, encountered the following error:\\n…\\n24/03/11 09:48:36 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077...\\n24/03/11 09:48:36 INFO TransportClientFactory: Successfully created connection to localhost/127.0.0.1:7077 after 10 ms (0 ms spent in bootstraps)\\n24/03/11 09:48:54 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\\n24/03/11 09:48:56 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077…\\n24/03/11 09:49:16 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077...\\n24/03/11 09:49:36 WARN StandaloneSchedulerBackend: Application ID is not initialized yet.\\n24/03/11 09:49:36 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.\\n…\\npy4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.sql.SparkSession.\\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\\n…\\nSolution:\\nDowngrade your local PySpark to 3.3.1 (same as Dockerfile)\\nThe reason for the failed connection in my case was the mismatch of PySpark versions. You can see that from the logs of spark-master in the docker container.\\nSolution 2:\\nCheck what Spark version your local machine has\\npyspark –version\\nspark-submit –version\\nAdd your version to SPARK_VERSION in build.sh',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Python Kafka: ./spark-submit.sh streaming.py - ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'Start a new terminal\\nRun: docker ps\\nCopy the CONTAINER ID of the spark-master container\\nRun: docker exec -it <spark_master_container_id> bash\\nRun: cat logs/spark-master.out\\nCheck for the log when the error happened\\nGoogle the error message from there',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Python Kafka: ./spark-submit.sh streaming.py - How to check why Spark master connection fails',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'If you have this error, it most likely that your kafka broker docker container is not working.\\nUse docker ps to confirm\\nThen in the docker compose yaml file folder, run docker compose up -d to start all the instances.',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'kafka.errors.NoBrokersAvailable: NoBrokersAvailable',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'tip:As the videos have low audio so I downloaded them and used VLC media player with putting the audio to the max 200% of original audio and the audio became quite good or try to use auto caption generated on Youtube directly.\\nKafka Python Videos - Rides.csv\\nThere is no clear explanation of the rides.csv data that the producer.py python programs use. You can find that here https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/2bd33e89906181e424f7b12a299b70b19b7cfcd5/week_6_stream_processing/python/resources/rides.csv.',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Kafka- python videos have low audio and hard to follow up',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'For example, when running JsonConsumer.java, got:\\nConsuming form kafka started\\nRESULTS:::0\\nRESULTS:::0\\nRESULTS:::0\\nOr when running JsonProducer.java, got:\\nException in thread \"main\" java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.SaslAuthenticationException: Authentication failed\\nSolution:\\nMake sure in the scripts in src/main/java/org/example/ that you are running (e.g. JsonConsumer.java, JsonProducer.java), the StreamsConfig.BOOTSTRAP_SERVERS_CONFIG is the correct server url (e.g. europe-west3 from example vs europe-west2)\\nMake sure cluster key and secrets are updated in src/main/java/org/example/Secrets.java (KAFKA_CLUSTER_KEY and KAFKA_CLUSTER_SECRET)',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Java Kafka: When running the producer/consumer/etc java scripts, no results retrieved or no message sent',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'confluent-kafka: `pip install confluent-kafka` or `conda install conda-forge::python-confluent-kafka`\\nfastavro: pip install fastavro\\nAbhirup Ghosh\\nCan install Faust Library for Module 6 Python Version due to dependency conflicts?\\nThe Faust repository and library is no longer maintained - https://github.com/robinhood/faust\\nIf you do not know Java, you now have the option to follow the Python Videos 6.13 & 6.14 here https://www.youtube.com/watch?v=BgAlVknDFlQ&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=80  and follow the RedPanda Python version here https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/06-streaming/python/redpanda_example - NOTE: I highly recommend watching the Java videos to understand the concept of streaming but you can skip the coding parts - all will become clear when you get to the Python videos and RedPanda files.',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Python Kafka: Installing dependencies for python3 06-streaming/python/avro_example/producer.py',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'In my set up, all of the dependencies listed in gradle.build were not installed in <project_name>-1.0-SNAPSHOT.jar.\\nSolution:\\nIn build.gradle file, I added the following at the end:\\nshadowJar {\\narchiveBaseName = \"java-kafka-rides\"\\narchiveClassifier = \\'\\'\\n}\\nAnd then in the command line ran ‘gradle shadowjar’, and run the script from java-kafka-rides-1.0-SNAPSHOT.jar created by the shadowjar',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Java Kafka: <project_name>-1.0-SNAPSHOT.jar errors: package xxx does not exist even after gradle build',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'Run this command in terminal in the same directory (/docker/spark):\\nchmod +x build.sh',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Python Kafka: ./build.sh: Permission denied Error',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'If you get this error, know that you have not built your sparks and juypter images. This images aren’t readily available on dockerHub.\\nIn the spark folder, run ./build.sh from a bash cli to to build all images before running docker compose',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': \"How to fix docker compose error: Error response from daemon: pull access denied for spark-3.3.1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\",\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'You can check the version of your local spark using spark-submit --version. In the build.sh file of the Python folder, make sure that SPARK_VERSION matches your local version. Similarly, make sure the pyspark you pip installed also matches this version.',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'How do I check compatibility of local and container Spark versions?',\n",
       "  'course': 'data-engineering-zoomcamp'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.search(\"how to use kafka with spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd2d7d5c-f703-48e7-8083-9fd9250f849f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    boost = {'question': 3.0, 'section': 0.5}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={'course': 'data-engineering-zoomcamp'},\n",
    "        boost_dict=boost,\n",
    "        num_results=5,\n",
    "        output_ids=True\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33cd3a60-1729-4dcc-beb5-fd311cdf0d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"how to use kafka with spark\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a8fc61d-d08b-456f-ad1d-6d6a42b1642c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "<QUESTION>\n",
    "{question}\n",
    "</QUESTION>\n",
    "\n",
    "<CONTEXT>\n",
    "{context}\n",
    "</CONTEXT>\n",
    "\"\"\".strip()\n",
    "\n",
    "def build_prompt(query, search_results):\n",
    "    context = \"\"\n",
    "\n",
    "    for doc in search_results:\n",
    "        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\"\n",
    "    \n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9ee9f5c-d9e4-4638-9eca-5c9207a033a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = search(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "107bfa23-cda8-4ef3-bc0d-07a2efee876a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = build_prompt(question, search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdd9be5a-2d6b-431f-a282-bb6485a93f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
      "Use only the facts from the CONTEXT when answering the QUESTION.\n",
      "\n",
      "<QUESTION>\n",
      "how to use kafka with spark\n",
      "</QUESTION>\n",
      "\n",
      "<CONTEXT>\n",
      "section: Module 6: streaming with kafka\n",
      "question: Python Kafka: ./spark-submit.sh streaming.py - ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.\n",
      "answer: While following tutorial 13.2 , when running ./spark-submit.sh streaming.py, encountered the following error:\n",
      "…\n",
      "24/03/11 09:48:36 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077...\n",
      "24/03/11 09:48:36 INFO TransportClientFactory: Successfully created connection to localhost/127.0.0.1:7077 after 10 ms (0 ms spent in bootstraps)\n",
      "24/03/11 09:48:54 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "24/03/11 09:48:56 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077…\n",
      "24/03/11 09:49:16 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077...\n",
      "24/03/11 09:49:36 WARN StandaloneSchedulerBackend: Application ID is not initialized yet.\n",
      "24/03/11 09:49:36 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.\n",
      "…\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.sql.SparkSession.\n",
      ": java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n",
      "…\n",
      "Solution:\n",
      "Downgrade your local PySpark to 3.3.1 (same as Dockerfile)\n",
      "The reason for the failed connection in my case was the mismatch of PySpark versions. You can see that from the logs of spark-master in the docker container.\n",
      "Solution 2:\n",
      "Check what Spark version your local machine has\n",
      "pyspark –version\n",
      "spark-submit –version\n",
      "Add your version to SPARK_VERSION in build.sh\n",
      "\n",
      "section: Module 6: streaming with kafka\n",
      "question: Python Kafka: ./spark-submit.sh streaming.py - How to check why Spark master connection fails\n",
      "answer: Start a new terminal\n",
      "Run: docker ps\n",
      "Copy the CONTAINER ID of the spark-master container\n",
      "Run: docker exec -it <spark_master_container_id> bash\n",
      "Run: cat logs/spark-master.out\n",
      "Check for the log when the error happened\n",
      "Google the error message from there\n",
      "\n",
      "section: Module 6: streaming with kafka\n",
      "question: kafka.errors.NoBrokersAvailable: NoBrokersAvailable\n",
      "answer: If you have this error, it most likely that your kafka broker docker container is not working.\n",
      "Use docker ps to confirm\n",
      "Then in the docker compose yaml file folder, run docker compose up -d to start all the instances.\n",
      "\n",
      "section: Module 6: streaming with kafka\n",
      "question: Kafka- python videos have low audio and hard to follow up\n",
      "answer: tip:As the videos have low audio so I downloaded them and used VLC media player with putting the audio to the max 200% of original audio and the audio became quite good or try to use auto caption generated on Youtube directly.\n",
      "Kafka Python Videos - Rides.csv\n",
      "There is no clear explanation of the rides.csv data that the producer.py python programs use. You can find that here https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/2bd33e89906181e424f7b12a299b70b19b7cfcd5/week_6_stream_processing/python/resources/rides.csv.\n",
      "\n",
      "section: Module 6: streaming with kafka\n",
      "question: Java Kafka: <project_name>-1.0-SNAPSHOT.jar errors: package xxx does not exist even after gradle build\n",
      "answer: In my set up, all of the dependencies listed in gradle.build were not installed in <project_name>-1.0-SNAPSHOT.jar.\n",
      "Solution:\n",
      "In build.gradle file, I added the following at the end:\n",
      "shadowJar {\n",
      "archiveBaseName = \"java-kafka-rides\"\n",
      "archiveClassifier = ''\n",
      "}\n",
      "And then in the command line ran ‘gradle shadowjar’, and run the script from java-kafka-rides-1.0-SNAPSHOT.jar created by the shadowjar\n",
      "\n",
      "\n",
      "</CONTEXT>\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c1bc124-3bb6-4ba2-a06f-7ce208cb4aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def llm(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def rag(query):\n",
    "    search_results = search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    answer = llm(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "90deb85e-d269-47dd-93b7-a4f51c83f990",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8a0d682-3437-4dd5-8ba9-e6fc99b70e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use Kafka with Spark, you should follow the steps in the relevant module that covers streaming with Kafka. Make sure you have the appropriate dependencies set up, and check for any version mismatches between your local PySpark and the intended version (3.3.1 as indicated).\n",
      "\n",
      "Additionally, if you encounter connection issues to the Spark master, you can troubleshoot by running the following commands in your terminal:\n",
      "\n",
      "1. Start a new terminal.\n",
      "2. Run `docker ps` to check active containers.\n",
      "3. Copy the CONTAINER ID of the spark-master container.\n",
      "4. Run `docker exec -it <spark_master_container_id> bash` to access the container.\n",
      "5. Run `cat logs/spark-master.out` to view the logs and identify errors.\n",
      "\n",
      "If you have issues with Kafka, such as the error `kafka.errors.NoBrokersAvailable`, ensure that your Kafka broker Docker container is up and running by confirming with `docker ps`. If it's not running, navigate to the folder with your Docker compose yaml file and run `docker compose up -d` to start the necessary instances.\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ada1dbd2-8ada-4dcc-b5f6-0b66bcacc893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To use Kafka with Spark, you should ensure that your Kafka broker is running properly and that Spark can connect to it without any issues. Here are some steps to help you set up and troubleshoot your environment:\\n\\n1. **Check Kafka Broker Status**: \\n   - Use the command `docker ps` to confirm that your Kafka broker Docker container is running. \\n\\n2. **Start Kafka Broker if Necessary**: \\n   - If the Kafka broker is not running, navigate to the directory containing your Docker Compose YAML file and run the command:\\n     ```\\n     docker compose up -d\\n     ```\\n   - This will start all the required instances, including Kafka.\\n\\n3. **Spark Submission**:\\n   - You'll typically submit your Spark streaming application using a command similar to:\\n     ```\\n     ./spark-submit.sh streaming.py\\n     ```\\n   - Make sure you are running this command while your Kafka broker is active.\\n\\n4. **Version Compatibility**:\\n   - Check that your local PySpark version matches the version specified in your project. You can use:\\n     ```\\n     pyspark --version\\n     spark-submit --version\\n     ```\\n   - If there is a mismatch, you may need to downgrade PySpark or adjust your configuration accordingly.\\n\\n5. **Check Logs for Errors**: \\n   - If you encounter connection issues, you can check the Spark master logs for errors by running:\\n     ```\\n     docker exec -it <spark_master_container_id> bash\\n     cat logs/spark-master.out\\n     ```\\n   - Look for any specific error messages that indicate what might be causing the connection failure.\\n\\nBy following these steps, you should be able to successfully use Kafka with Spark and address any connection issues that may arise.\""
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e88740-b4f1-45e1-87da-f3308733796d",
   "metadata": {},
   "source": [
    "#### 'Agentic' RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc9a26bf-f640-42ce-a773-8c3273fac948",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You are a course teaching assistant.\n",
    "\n",
    "You are given a Question from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
    "At the beginning the context is EMPTY.\n",
    "\n",
    "<QUESTION>\n",
    "{question}\n",
    "</QUESTION>\n",
    "\n",
    "<CONTEXT>\n",
    "{context}\n",
    "</CONTEXT>\n",
    "\n",
    "If CONTEXT is EMPTY, you can use your FAQ database.\n",
    "In this case, use the following output template:\n",
    "\n",
    "{{\n",
    "\"action\": \"SEARCH\",\n",
    "\"reasoning\": \"<add your reasoning here>\"\n",
    "}}\n",
    "\n",
    "If you can answer the QUESTION using CONTEXT, use this template:\n",
    "\n",
    "{{\n",
    "\"action\": \"ANSWER\",\n",
    "\"answer\": \"<your answer>\",\n",
    "\"source\" : \"CONTEXT\"\n",
    "}}\n",
    "\n",
    "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
    "\n",
    "{{\n",
    "\"action\" : \"ANSWER\",\n",
    "\"answer\" : \"<your answer>\",\n",
    "\"source\" : \"OWN_KNOWLEDGE\"\n",
    "}}\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a7bd20e-0f69-4260-9c00-7b1ad9551622",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Can i still join the course?\"\n",
    "context = \"EMPTY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1903578f-e1f7-4e95-87c2-86d0fbe79950",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a course teaching assistant.\n",
      "\n",
      "You are given a Question from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "At the beginning the context is EMPTY.\n",
      "\n",
      "<QUESTION>\n",
      "Can i still join the course?\n",
      "</QUESTION>\n",
      "\n",
      "<CONTEXT>\n",
      "EMPTY\n",
      "</CONTEXT>\n",
      "\n",
      "If CONTEXT is EMPTY, you can use your FAQ database.\n",
      "In this case, use the following output template:\n",
      "\n",
      "{\n",
      "\"action\": \"SEARCH\",\n",
      "\"reasoning\": \"<add your reasoning here>\"\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\" : \"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\" : \"ANSWER\",\n",
      "\"answer\" : \"<your answer>\",\n",
      "\"source\" : \"OWN_KNOWLEDGE\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "prompt = prompt_template.format(question=question, context=context)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d378fa25-e1a6-4ec3-85c2-acf7fddc04e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_json = llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec652cb6-1566-4b1b-a7de-1fd991f1e495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8dbf2409-3072-45f2-a166-b6de5f76d0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = json.loads(answer_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ec3c12c-84fb-47ff-923d-9b424e8fb5c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SEARCH'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer[\"action\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a44cc728-d30a-4414-acc2-1e2c0792c36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_context(search_results):\n",
    "    context = \"\"\n",
    "\n",
    "    for doc in search_results:\n",
    "        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\"\n",
    "    \n",
    "    return context.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4d4d4898-156d-44ec-96b3-ec7ba80db9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = search(question)\n",
    "context = build_context(search_results)\n",
    "prompt = prompt_template.format(question=question, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e043985e-8559-46d9-9773-ce52af3dd340",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_json = llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a809f869-c075-446a-8812-2731e28a7ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"Yes, you can still join the course after the start date. Even if you don't register, you're allowed to submit your homeworks. However, be mindful of deadlines for the final projects, so try not to leave everything for the last minute.\",\n",
      "\"source\": \"CONTEXT\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(answer_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1033a310-c67f-4597-93b8-613c854bb52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agentic_rag_v1(question):\n",
    "    context = \"EMPTY\"\n",
    "    prompt = prompt_template.format(question=question, context=context)\n",
    "    answer_json = llm(prompt)\n",
    "    answer = json.loads(answer_json)\n",
    "    print(answer)\n",
    "\n",
    "    if answer[\"action\"]==\"SEARCH\":\n",
    "        print(\"need to perform search...\")\n",
    "        search_results = search(question)\n",
    "        context = build_context(search_results)\n",
    "\n",
    "        prompt = prompt_template.format(question=question, context=context)\n",
    "        answer_json = llm(prompt)\n",
    "        answer = json.loads(answer_json)\n",
    "        print(answer)\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ab1e3a6-af05-40ac-9829-10dbde2f81e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'action': 'SEARCH', 'reasoning': 'The question is about how to join the course, which is a general inquiry that typically would be found in an FAQ database.'}\n",
      "need to perform search...\n",
      "{'action': 'ANSWER', 'answer': \"To join the course, you need to register before the course starts using the provided registration link. Although you can still submit homeworks after the course has started, it's important to register to ensure you get all necessary updates and can participate fully in live sessions.\", 'source': 'CONTEXT'}\n",
      "CPU times: user 21.3 ms, sys: 1.79 ms, total: 23.1 ms\n",
      "Wall time: 3.24 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'action': 'ANSWER',\n",
       " 'answer': \"To join the course, you need to register before the course starts using the provided registration link. Although you can still submit homeworks after the course has started, it's important to register to ensure you get all necessary updates and can participate fully in live sessions.\",\n",
       " 'source': 'CONTEXT'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "agentic_rag_v1(\"how do i join the course?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f56dd4-6975-4d46-b13e-e7338e3d21eb",
   "metadata": {},
   "source": [
    "#### Agentic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c500652d-62e4-4f8f-81ac-259043dbc3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dedub(seq):\n",
    "    \"\"\"\n",
    "    deduplicates by skipping the repeating element('_id')\n",
    "    \"\"\"\n",
    "    seen = set()\n",
    "    result = []\n",
    "    for el in seq:\n",
    "        _id = el['_id']\n",
    "        if _id in seen:\n",
    "            continue\n",
    "        seen.add(_id)\n",
    "        result.append(el)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2a8b0793-d42b-4e0d-9c4e-843aedc00558",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You are a course teaching assistant.\n",
    "\n",
    "You are given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
    "\n",
    "The CONTEXT is build with the documents from our FAQ database.\n",
    "SEARCH_QUERIES contains the queries that were used to retrieve the documents\n",
    "from FAQ to and add them to the context.\n",
    "PREVIOUS_ACTIONS contains the actions you already performed.\n",
    "\n",
    "At the beginning the CONTEXT is empty.\n",
    "\n",
    "You can perform the following actions:\n",
    "\n",
    "- Search in the FAQ database to get more data for the CONTEXT\n",
    "- Answer the question using the CONTEXT\n",
    "- Answer the question using your own knowledge\n",
    "\n",
    "For the SEARCH action, build search requests based on the CONTEXT and the QUESTION.\n",
    "Carefully analyze the CONTEXT and generate the requests to deeply explore the topic.\n",
    "\n",
    "Don't use search queries used at the previous iterations.\n",
    "\n",
    "Don't repeat previously performed actions.\n",
    "\n",
    "Don't perform more than {max_iterations} iterations for a given student question.\n",
    "The current iteration number: {iteration_number}. If we exceed the allowed number\n",
    "of iterations, give the best possible answer with the provided information.\n",
    "\n",
    "Output templates:\n",
    "\n",
    "If you want to perform search, use this template:\n",
    "\n",
    "{{\n",
    "\"action\" : \"SEARCH\",\n",
    "\"reasoning\" : \"<add your reasoning here>\",\n",
    "\"keywords\" : [\"search query 1\", \"search query 2\", ...]\n",
    "}}\n",
    "\n",
    "If you can answer the QUESTION using CONTEXT, use this template:\n",
    "\n",
    "{{\n",
    "\"action\" : \"ANSWER_CONTEXT\",\n",
    "\"answer\" : \"<your answer>\",\n",
    "\"source\" :\"CONTEXT\"\n",
    "}}\n",
    "\n",
    "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
    "\n",
    "{{\n",
    "\"action\" : \"ANSWER\",\n",
    "\"answer\" : \"<your answer>\",\n",
    "\"source\" : \"OWN_KNOWLEDGE\"\n",
    "}}\n",
    "\n",
    "<QUESTION>\n",
    "{question}\n",
    "</QUESTION>\n",
    "\n",
    "<SEARCH_QUERIES>\n",
    "{search_queries}\n",
    "</SEARCH_QUERIES>\n",
    "\n",
    "<CONTEXT>\n",
    "{context}\n",
    "</CONTEXT>\n",
    "\n",
    "<PREVIOUS_ACTIONS>\n",
    "{previous_actions}\n",
    "</PREVIOUS_ACTIONS>\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4a52001d-e882-42ad-8c8a-1bed7e435b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"how do I do well on module 1\"\n",
    "\n",
    "max_iterations = 3\n",
    "iteration_number = 0\n",
    "search_queries = []\n",
    "search_results = []\n",
    "previous_actions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d55f6e25-5d48-453e-a2ad-8daed7a07c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = build_context(search_results)\n",
    "\n",
    "prompt = prompt_template.format(\n",
    "    question = question,\n",
    "    context = context,\n",
    "    search_queries = \"\\n\".join(search_queries),\n",
    "    previous_actions = \"\\n\".join([json.dumps(a) for a in previous_actions]),\n",
    "    max_iterations=max_iterations,\n",
    "    iteration_number=iteration_number,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "324e17b1-72d8-4d65-9424-9a6c80f4acd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_json = llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "7542a4d2-2702-4a48-b494-2a38997a1490",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = json.loads(answer_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "c6d7af6f-8e7a-489c-a5bd-54d3d86669c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'action': 'SEARCH',\n",
       " 'reasoning': 'The context currently lacks specific tips or strategies for succeeding in Module 1. I will search for particular advice on how to excel in that module to provide a comprehensive answer.',\n",
       " 'keywords': ['success in Module 1',\n",
       "  'Module 1 study tips',\n",
       "  'how to do well in Module 1']}"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "48b0257d-e366-428c-98f9-e67f3abea160",
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_actions.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "4c012b71-037b-4663-8788-39546624eb79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'action': 'SEARCH',\n",
       "  'reasoning': 'To provide the best advice on excelling in module 1, I need to gather specific strategies or tips related to that module from the FAQ database.',\n",
       "  'keywords': ['module 1 success tips',\n",
       "   'how to excel in module 1',\n",
       "   'module 1 study strategies']},\n",
       " {'action': 'SEARCH',\n",
       "  'reasoning': 'The context currently lacks specific tips or strategies for succeeding in Module 1. I will search for particular advice on how to excel in that module to provide a comprehensive answer.',\n",
       "  'keywords': ['success in Module 1',\n",
       "   'Module 1 study tips',\n",
       "   'how to do well in Module 1']}]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previous_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "95ff6a22-10bb-46e8-8ec4-e50021eb04e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = answer['keywords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "2a52d8a7-7e4f-4169-af75-984f6341ec34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['success in Module 1', 'Module 1 study tips', 'how to do well in Module 1']\n"
     ]
    }
   ],
   "source": [
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "1c4ddbe5-e3c0-4a45-8c52-3f53472cff90",
   "metadata": {},
   "outputs": [],
   "source": [
    "for kw in keywords:\n",
    "    search_queries.append(kw)\n",
    "    sr = search(kw)\n",
    "    search_results.extend(sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "437f178a-21fd-4974-875b-eeebe47d1f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = dedub(search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "062d4761-c4ae-4659-8622-537b4751a9c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c032302d-da5c-4b0f-b2e5-2aa8f5f4faac",
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration_number = 2\n",
    "\n",
    "context = build_context(search_results)\n",
    "\n",
    "prompt = prompt_template.format(\n",
    "    question = question,\n",
    "    context = context,\n",
    "    search_queries = \"\\n\".join(search_queries),\n",
    "    previous_actions = \"\\n\".join([json.dumps(a) for a in previous_actions]),\n",
    "    max_iterations=max_iterations,\n",
    "    iteration_number=iteration_number,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f5da2459-6d51-4518-9582-094a215e3598",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a course teaching assistant.\n",
      "\n",
      "You are given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "\n",
      "The CONTEXT is build with the documents from our FAQ database.\n",
      "SEARCH_QUERIES contains the queries that were used to retrieve the documents\n",
      "from FAQ to and add them to the context.\n",
      "PREVIOUS_ACTIONS contains the actions you already performed.\n",
      "\n",
      "At the beginning the CONTEXT is empty.\n",
      "\n",
      "You can perform the following actions:\n",
      "\n",
      "- Search in the FAQ database to get more data for the CONTEXT\n",
      "- Answer the question using the CONTEXT\n",
      "- Answer the question using your own knowledge\n",
      "\n",
      "For the SEARCH action, build search requests based on the CONTEXT and the QUESTION.\n",
      "Carefully analyze the CONTEXT and generate the requests to deeply explore the topic.\n",
      "\n",
      "Don't use search queries used at the previous iterations.\n",
      "\n",
      "Don't repeat previously performed actions.\n",
      "\n",
      "Don't perform more than 3 iterations for a given student question.\n",
      "The current iteration number: 2. If we exceed the allowed number\n",
      "of iterations, give the best possible answer with the provided information.\n",
      "\n",
      "Output templates:\n",
      "\n",
      "If you want to perform search, use this template:\n",
      "\n",
      "{\n",
      "\"action\" : \"SEARCH\",\n",
      "\"reasoning\" : \"<add your reasoning here>\",\n",
      "\"keywords\" : [\"search query 1\", \"search query 2\", ...]\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\" : \"ANSWER_CONTEXT\",\n",
      "\"answer\" : \"<your answer>\",\n",
      "\"source\" :\"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\" : \"ANSWER\",\n",
      "\"answer\" : \"<your answer>\",\n",
      "\"source\" : \"OWN_KNOWLEDGE\"\n",
      "}\n",
      "\n",
      "<QUESTION>\n",
      "how do I do well on module 1\n",
      "</QUESTION>\n",
      "\n",
      "<SEARCH_QUERIES>\n",
      "module 1 success tips\n",
      "how to excel in module 1\n",
      "module 1 study strategies\n",
      "success in Module 1\n",
      "Module 1 study tips\n",
      "how to do well in Module 1\n",
      "</SEARCH_QUERIES>\n",
      "\n",
      "<CONTEXT>\n",
      "section: Module 5: pyspark\n",
      "question: Module Not Found Error in Jupyter Notebook .\n",
      "answer: Even after installing pyspark correctly on linux machine (VM ) as per course instructions, faced a module not found error in jupyter notebook .\n",
      "The solution which worked for me(use following in jupyter notebook) :\n",
      "!pip install findspark\n",
      "import findspark\n",
      "findspark.init()\n",
      "Thereafter , import pyspark and create spark contex<<t as usual\n",
      "None of the solutions above worked for me till I ran !pip3 install pyspark instead !pip install pyspark.\n",
      "Filter based on conditions based on multiple columns\n",
      "from pyspark.sql.functions import col\n",
      "new_final.filter((new_final.a_zone==\"Murray Hill\") & (new_final.b_zone==\"Midwood\")).show()\n",
      "Krishna Anand\n",
      "\n",
      "section: Module 5: pyspark\n",
      "question: Py4JJavaError - ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`\n",
      "answer: You need to look for the Py4J file and note the version of the filename. Once you know the version, you can update the export command accordingly, this is how you check yours:\n",
      "` ls ${SPARK_HOME}/python/lib/ ` and then you add it in the export command, mine was:\n",
      "export PYTHONPATH=”${SPARK_HOME}/python/lib/Py4J-0.10.9.5-src.zip:${PYTHONPATH}”\n",
      "Make sure that the version under `${SPARK_HOME}/python/lib/` matches the filename of py4j or you will encounter `ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`.\n",
      "For instance, if the file under `${SPARK_HOME}/python/lib/` was `py4j-0.10.9.3-src.zip`.\n",
      "Then the export PYTHONPATH statement above should be changed to `export PYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH\"` appropriately.\n",
      "Additionally, you can check for the version of ‘py4j’ of the spark you’re using from here and update as mentioned above.\n",
      "~ Abhijit Chakraborty: Sometimes, even with adding the correct version of py4j might not solve the problem. Simply run pip install py4j and problem should be resolved.\n",
      "\n",
      "section: Module 4: analytics engineering with dbt\n",
      "question: DBT - Error: No module named 'pytz' while setting up dbt with docker\n",
      "answer: Following dbt with BigQuery on Docker readme.md, after `docker-compose build` and `docker-compose run dbt-bq-dtc init`, encountered error `ModuleNotFoundError: No module named 'pytz'`\n",
      "Solution:\n",
      "Add `RUN python -m pip install --no-cache pytz` in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Postgres - ModuleNotFoundError: No module named 'psycopg2'\n",
      "answer: Issue:\n",
      "e…\n",
      "Solution:\n",
      "pip install psycopg2-binary\n",
      "If you already have it, you might need to update it:\n",
      "pip install psycopg2-binary --upgrade\n",
      "Other methods, if the above fails:\n",
      "if you are getting the “ ModuleNotFoundError: No module named 'psycopg2' “ error even after the above installation, then try updating conda using the command conda update -n base -c defaults conda. Or if you are using pip, then try updating it before installing the psycopg packages i.e\n",
      "First uninstall the psycopg package\n",
      "Then update conda or pip\n",
      "Then install psycopg again using pip.\n",
      "if you are still facing error with r pcycopg2 and showing pg_config not found then you will have to install postgresql. in MAC it is brew install postgresql\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Python - SQLALchemy - TypeError 'module' object is not callable\n",
      "answer: create_engine('postgresql://root:root@localhost:5432/ny_taxi')  I get the error \"TypeError: 'module' object is not callable\"\n",
      "Solution:\n",
      "conn_string = \"postgresql+psycopg://root:root@localhost:5432/ny_taxi\"\n",
      "engine = create_engine(conn_string)\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Python - SQLAlchemy - ModuleNotFoundError: No module named 'psycopg2'.\n",
      "answer: Error raised during the jupyter notebook’s cell execution:\n",
      "engine = create_engine('postgresql://root:root@localhost:5432/ny_taxi').\n",
      "Solution: Need to install Python module “psycopg2”. Can be installed by Conda or pip.\n",
      "</CONTEXT>\n",
      "\n",
      "<PREVIOUS_ACTIONS>\n",
      "{\"action\": \"SEARCH\", \"reasoning\": \"To provide the best advice on excelling in module 1, I need to gather specific strategies or tips related to that module from the FAQ database.\", \"keywords\": [\"module 1 success tips\", \"how to excel in module 1\", \"module 1 study strategies\"]}\n",
      "{\"action\": \"SEARCH\", \"reasoning\": \"The context currently lacks specific tips or strategies for succeeding in Module 1. I will search for particular advice on how to excel in that module to provide a comprehensive answer.\", \"keywords\": [\"success in Module 1\", \"Module 1 study tips\", \"how to do well in Module 1\"]}\n",
      "</PREVIOUS_ACTIONS>\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "68f347d2-2e0a-4416-b368-3fb4cda40c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_json = llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "4ddf7ced-88d1-4890-b5a6-5869d566d0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"To do well in Module 1, which focuses on Docker and Terraform, it's important to grasp the foundational concepts of containerization and infrastructure as code. Ensure you understand how Docker works, including commands like `docker build`, `docker run`, and how images and containers interact. Familiarize yourself with Docker files and the syntax as you will use them to create your containers. Additionally, when it comes to Terraform, focus on understanding the configuration language, how to define infrastructure as code, and the `terraform apply` command. Practice building sample projects to reinforce your learning. Also, make sure to troubleshoot any issues you encounter, as this is a great way to deepen your understanding. Lastly, make use of online resources and community forums for additional help and insights from peers.\",\"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(answer_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "287ebd52-aed7-4fc8-b444-e2d64f3616b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = json.loads(answer_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "71818a41-5295-46eb-8e8b-539911729bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To do well in Module 1, which focuses on Docker and Terraform, it's important to grasp the foundational concepts of containerization and infrastructure as code. Ensure you understand how Docker works, including commands like `docker build`, `docker run`, and how images and containers interact. Familiarize yourself with Docker files and the syntax as you will use them to create your containers. Additionally, when it comes to Terraform, focus on understanding the configuration language, how to define infrastructure as code, and the `terraform apply` command. Practice building sample projects to reinforce your learning. Also, make sure to troubleshoot any issues you encounter, as this is a great way to deepen your understanding. Lastly, make use of online resources and community forums for additional help and insights from peers.\n"
     ]
    }
   ],
   "source": [
    "print(answer['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ea4d11-c744-4949-b55f-5c2eab79ac9e",
   "metadata": {},
   "source": [
    "#### Automating in a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "571e850b-daf5-4ad8-84bf-cb33af51b321",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION #0...\n",
      "You are a course teaching assistant.\n",
      "\n",
      "You are given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "\n",
      "The CONTEXT is build with the documents from our FAQ database.\n",
      "SEARCH_QUERIES contains the queries that were used to retrieve the documents\n",
      "from FAQ to and add them to the context.\n",
      "PREVIOUS_ACTIONS contains the actions you already performed.\n",
      "\n",
      "At the beginning the CONTEXT is empty.\n",
      "\n",
      "You can perform the following actions:\n",
      "\n",
      "- Search in the FAQ database to get more data for the CONTEXT\n",
      "- Answer the question using the CONTEXT\n",
      "- Answer the question using your own knowledge\n",
      "\n",
      "For the SEARCH action, build search requests based on the CONTEXT and the QUESTION.\n",
      "Carefully analyze the CONTEXT and generate the requests to deeply explore the topic.\n",
      "\n",
      "Don't use search queries used at the previous iterations.\n",
      "\n",
      "Don't repeat previously performed actions.\n",
      "\n",
      "Don't perform more than 3 iterations for a given student question.\n",
      "The current iteration number: 0. If we exceed the allowed number\n",
      "of iterations, give the best possible answer with the provided information.\n",
      "\n",
      "Output templates:\n",
      "\n",
      "If you want to perform search, use this template:\n",
      "\n",
      "{\n",
      "\"action\" : \"SEARCH\",\n",
      "\"reasoning\" : \"<add your reasoning here>\",\n",
      "\"keywords\" : [\"search query 1\", \"search query 2\", ...]\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\" : \"ANSWER_CONTEXT\",\n",
      "\"answer\" : \"<your answer>\",\n",
      "\"source\" :\"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\" : \"ANSWER\",\n",
      "\"answer\" : \"<your answer>\",\n",
      "\"source\" : \"OWN_KNOWLEDGE\"\n",
      "}\n",
      "\n",
      "<QUESTION>\n",
      "what do I need to do to be succesful at module 1?\n",
      "</QUESTION>\n",
      "\n",
      "<SEARCH_QUERIES>\n",
      "\n",
      "</SEARCH_QUERIES>\n",
      "\n",
      "<CONTEXT>\n",
      "\n",
      "</CONTEXT>\n",
      "\n",
      "<PREVIOUS_ACTIONS>\n",
      "\n",
      "</PREVIOUS_ACTIONS>\n",
      "{\n",
      "  \"action\": \"SEARCH\",\n",
      "  \"reasoning\": \"To provide a comprehensive answer on how to be successful in module 1, I need to gather relevant information from the FAQ database that might outline strategies, resources, or tips for success in this module.\",\n",
      "  \"keywords\": [\n",
      "    \"success tips for module 1\",\n",
      "    \"how to excel module 1\",\n",
      "    \"module 1 study strategies\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "ITERATION #1...\n",
      "You are a course teaching assistant.\n",
      "\n",
      "You are given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "\n",
      "The CONTEXT is build with the documents from our FAQ database.\n",
      "SEARCH_QUERIES contains the queries that were used to retrieve the documents\n",
      "from FAQ to and add them to the context.\n",
      "PREVIOUS_ACTIONS contains the actions you already performed.\n",
      "\n",
      "At the beginning the CONTEXT is empty.\n",
      "\n",
      "You can perform the following actions:\n",
      "\n",
      "- Search in the FAQ database to get more data for the CONTEXT\n",
      "- Answer the question using the CONTEXT\n",
      "- Answer the question using your own knowledge\n",
      "\n",
      "For the SEARCH action, build search requests based on the CONTEXT and the QUESTION.\n",
      "Carefully analyze the CONTEXT and generate the requests to deeply explore the topic.\n",
      "\n",
      "Don't use search queries used at the previous iterations.\n",
      "\n",
      "Don't repeat previously performed actions.\n",
      "\n",
      "Don't perform more than 3 iterations for a given student question.\n",
      "The current iteration number: 1. If we exceed the allowed number\n",
      "of iterations, give the best possible answer with the provided information.\n",
      "\n",
      "Output templates:\n",
      "\n",
      "If you want to perform search, use this template:\n",
      "\n",
      "{\n",
      "\"action\" : \"SEARCH\",\n",
      "\"reasoning\" : \"<add your reasoning here>\",\n",
      "\"keywords\" : [\"search query 1\", \"search query 2\", ...]\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\" : \"ANSWER_CONTEXT\",\n",
      "\"answer\" : \"<your answer>\",\n",
      "\"source\" :\"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\" : \"ANSWER\",\n",
      "\"answer\" : \"<your answer>\",\n",
      "\"source\" : \"OWN_KNOWLEDGE\"\n",
      "}\n",
      "\n",
      "<QUESTION>\n",
      "what do I need to do to be succesful at module 1?\n",
      "</QUESTION>\n",
      "\n",
      "<SEARCH_QUERIES>\n",
      "success tips for module 1\n",
      "module 1 study strategies\n",
      "how to excel module 1\n",
      "</SEARCH_QUERIES>\n",
      "\n",
      "<CONTEXT>\n",
      "section: Module 5: pyspark\n",
      "question: Module Not Found Error in Jupyter Notebook .\n",
      "answer: Even after installing pyspark correctly on linux machine (VM ) as per course instructions, faced a module not found error in jupyter notebook .\n",
      "The solution which worked for me(use following in jupyter notebook) :\n",
      "!pip install findspark\n",
      "import findspark\n",
      "findspark.init()\n",
      "Thereafter , import pyspark and create spark contex<<t as usual\n",
      "None of the solutions above worked for me till I ran !pip3 install pyspark instead !pip install pyspark.\n",
      "Filter based on conditions based on multiple columns\n",
      "from pyspark.sql.functions import col\n",
      "new_final.filter((new_final.a_zone==\"Murray Hill\") & (new_final.b_zone==\"Midwood\")).show()\n",
      "Krishna Anand\n",
      "\n",
      "section: Module 5: pyspark\n",
      "question: Py4JJavaError - ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`\n",
      "answer: You need to look for the Py4J file and note the version of the filename. Once you know the version, you can update the export command accordingly, this is how you check yours:\n",
      "` ls ${SPARK_HOME}/python/lib/ ` and then you add it in the export command, mine was:\n",
      "export PYTHONPATH=”${SPARK_HOME}/python/lib/Py4J-0.10.9.5-src.zip:${PYTHONPATH}”\n",
      "Make sure that the version under `${SPARK_HOME}/python/lib/` matches the filename of py4j or you will encounter `ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`.\n",
      "For instance, if the file under `${SPARK_HOME}/python/lib/` was `py4j-0.10.9.3-src.zip`.\n",
      "Then the export PYTHONPATH statement above should be changed to `export PYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH\"` appropriately.\n",
      "Additionally, you can check for the version of ‘py4j’ of the spark you’re using from here and update as mentioned above.\n",
      "~ Abhijit Chakraborty: Sometimes, even with adding the correct version of py4j might not solve the problem. Simply run pip install py4j and problem should be resolved.\n",
      "\n",
      "section: Module 4: analytics engineering with dbt\n",
      "question: DBT - Error: No module named 'pytz' while setting up dbt with docker\n",
      "answer: Following dbt with BigQuery on Docker readme.md, after `docker-compose build` and `docker-compose run dbt-bq-dtc init`, encountered error `ModuleNotFoundError: No module named 'pytz'`\n",
      "Solution:\n",
      "Add `RUN python -m pip install --no-cache pytz` in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Postgres - ModuleNotFoundError: No module named 'psycopg2'\n",
      "answer: Issue:\n",
      "e…\n",
      "Solution:\n",
      "pip install psycopg2-binary\n",
      "If you already have it, you might need to update it:\n",
      "pip install psycopg2-binary --upgrade\n",
      "Other methods, if the above fails:\n",
      "if you are getting the “ ModuleNotFoundError: No module named 'psycopg2' “ error even after the above installation, then try updating conda using the command conda update -n base -c defaults conda. Or if you are using pip, then try updating it before installing the psycopg packages i.e\n",
      "First uninstall the psycopg package\n",
      "Then update conda or pip\n",
      "Then install psycopg again using pip.\n",
      "if you are still facing error with r pcycopg2 and showing pg_config not found then you will have to install postgresql. in MAC it is brew install postgresql\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Python - SQLALchemy - TypeError 'module' object is not callable\n",
      "answer: create_engine('postgresql://root:root@localhost:5432/ny_taxi')  I get the error \"TypeError: 'module' object is not callable\"\n",
      "Solution:\n",
      "conn_string = \"postgresql+psycopg://root:root@localhost:5432/ny_taxi\"\n",
      "engine = create_engine(conn_string)\n",
      "</CONTEXT>\n",
      "\n",
      "<PREVIOUS_ACTIONS>\n",
      "{\"action\": \"SEARCH\", \"reasoning\": \"To provide a comprehensive answer on how to be successful in module 1, I need to gather relevant information from the FAQ database that might outline strategies, resources, or tips for success in this module.\", \"keywords\": [\"success tips for module 1\", \"how to excel module 1\", \"module 1 study strategies\"]}\n",
      "</PREVIOUS_ACTIONS>\n",
      "{\n",
      "  \"action\": \"SEARCH\",\n",
      "  \"reasoning\": \"To find more specific strategies or tips for success in Module 1, I will focus on searching for general best practices on studying and succeeding in technical courses, as well as any resources that may have been highlighted for Module 1.\",\n",
      "  \"keywords\": [\n",
      "    \"best practices for succeeding in technical modules\",\n",
      "    \"study tips for IT courses\",\n",
      "    \"resources for Module 1 Docker and Terraform\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "ITERATION #2...\n",
      "You are a course teaching assistant.\n",
      "\n",
      "You are given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "\n",
      "The CONTEXT is build with the documents from our FAQ database.\n",
      "SEARCH_QUERIES contains the queries that were used to retrieve the documents\n",
      "from FAQ to and add them to the context.\n",
      "PREVIOUS_ACTIONS contains the actions you already performed.\n",
      "\n",
      "At the beginning the CONTEXT is empty.\n",
      "\n",
      "You can perform the following actions:\n",
      "\n",
      "- Search in the FAQ database to get more data for the CONTEXT\n",
      "- Answer the question using the CONTEXT\n",
      "- Answer the question using your own knowledge\n",
      "\n",
      "For the SEARCH action, build search requests based on the CONTEXT and the QUESTION.\n",
      "Carefully analyze the CONTEXT and generate the requests to deeply explore the topic.\n",
      "\n",
      "Don't use search queries used at the previous iterations.\n",
      "\n",
      "Don't repeat previously performed actions.\n",
      "\n",
      "Don't perform more than 3 iterations for a given student question.\n",
      "The current iteration number: 2. If we exceed the allowed number\n",
      "of iterations, give the best possible answer with the provided information.\n",
      "\n",
      "Output templates:\n",
      "\n",
      "If you want to perform search, use this template:\n",
      "\n",
      "{\n",
      "\"action\" : \"SEARCH\",\n",
      "\"reasoning\" : \"<add your reasoning here>\",\n",
      "\"keywords\" : [\"search query 1\", \"search query 2\", ...]\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\" : \"ANSWER_CONTEXT\",\n",
      "\"answer\" : \"<your answer>\",\n",
      "\"source\" :\"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\" : \"ANSWER\",\n",
      "\"answer\" : \"<your answer>\",\n",
      "\"source\" : \"OWN_KNOWLEDGE\"\n",
      "}\n",
      "\n",
      "<QUESTION>\n",
      "what do I need to do to be succesful at module 1?\n",
      "</QUESTION>\n",
      "\n",
      "<SEARCH_QUERIES>\n",
      "best practices for succeeding in technical modules\n",
      "module 1 study strategies\n",
      "study tips for IT courses\n",
      "success tips for module 1\n",
      "resources for Module 1 Docker and Terraform\n",
      "how to excel module 1\n",
      "</SEARCH_QUERIES>\n",
      "\n",
      "<CONTEXT>\n",
      "section: Module 5: pyspark\n",
      "question: Module Not Found Error in Jupyter Notebook .\n",
      "answer: Even after installing pyspark correctly on linux machine (VM ) as per course instructions, faced a module not found error in jupyter notebook .\n",
      "The solution which worked for me(use following in jupyter notebook) :\n",
      "!pip install findspark\n",
      "import findspark\n",
      "findspark.init()\n",
      "Thereafter , import pyspark and create spark contex<<t as usual\n",
      "None of the solutions above worked for me till I ran !pip3 install pyspark instead !pip install pyspark.\n",
      "Filter based on conditions based on multiple columns\n",
      "from pyspark.sql.functions import col\n",
      "new_final.filter((new_final.a_zone==\"Murray Hill\") & (new_final.b_zone==\"Midwood\")).show()\n",
      "Krishna Anand\n",
      "\n",
      "section: Module 5: pyspark\n",
      "question: Py4JJavaError - ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`\n",
      "answer: You need to look for the Py4J file and note the version of the filename. Once you know the version, you can update the export command accordingly, this is how you check yours:\n",
      "` ls ${SPARK_HOME}/python/lib/ ` and then you add it in the export command, mine was:\n",
      "export PYTHONPATH=”${SPARK_HOME}/python/lib/Py4J-0.10.9.5-src.zip:${PYTHONPATH}”\n",
      "Make sure that the version under `${SPARK_HOME}/python/lib/` matches the filename of py4j or you will encounter `ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`.\n",
      "For instance, if the file under `${SPARK_HOME}/python/lib/` was `py4j-0.10.9.3-src.zip`.\n",
      "Then the export PYTHONPATH statement above should be changed to `export PYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH\"` appropriately.\n",
      "Additionally, you can check for the version of ‘py4j’ of the spark you’re using from here and update as mentioned above.\n",
      "~ Abhijit Chakraborty: Sometimes, even with adding the correct version of py4j might not solve the problem. Simply run pip install py4j and problem should be resolved.\n",
      "\n",
      "section: Module 4: analytics engineering with dbt\n",
      "question: DBT - Error: No module named 'pytz' while setting up dbt with docker\n",
      "answer: Following dbt with BigQuery on Docker readme.md, after `docker-compose build` and `docker-compose run dbt-bq-dtc init`, encountered error `ModuleNotFoundError: No module named 'pytz'`\n",
      "Solution:\n",
      "Add `RUN python -m pip install --no-cache pytz` in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Postgres - ModuleNotFoundError: No module named 'psycopg2'\n",
      "answer: Issue:\n",
      "e…\n",
      "Solution:\n",
      "pip install psycopg2-binary\n",
      "If you already have it, you might need to update it:\n",
      "pip install psycopg2-binary --upgrade\n",
      "Other methods, if the above fails:\n",
      "if you are getting the “ ModuleNotFoundError: No module named 'psycopg2' “ error even after the above installation, then try updating conda using the command conda update -n base -c defaults conda. Or if you are using pip, then try updating it before installing the psycopg packages i.e\n",
      "First uninstall the psycopg package\n",
      "Then update conda or pip\n",
      "Then install psycopg again using pip.\n",
      "if you are still facing error with r pcycopg2 and showing pg_config not found then you will have to install postgresql. in MAC it is brew install postgresql\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Python - SQLALchemy - TypeError 'module' object is not callable\n",
      "answer: create_engine('postgresql://root:root@localhost:5432/ny_taxi')  I get the error \"TypeError: 'module' object is not callable\"\n",
      "Solution:\n",
      "conn_string = \"postgresql+psycopg://root:root@localhost:5432/ny_taxi\"\n",
      "engine = create_engine(conn_string)\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Docker - cs to store all code in your default Linux distro to get the best out of file system performance (since Docker runs on WSL2 backend by default for Windows 10 Home / Windows 11 Home users).\n",
      "answer: More info in the Docker Docs on Best Practises\n",
      "\n",
      "section: General course-related questions\n",
      "question: Environment - Roadblock for Windows users in modules with *.sh (shell scripts).\n",
      "answer: Have no idea how past cohorts got past this as I haven't read old slack messages, and no FAQ entries that I can find.\n",
      "Later modules (module-05 & RisingWave workshop) use shell scripts in *.sh files and most Windows users not using WSL would hit a wall and cannot continue, even in git bash or MINGW64. This is why WSL environment setup is recommended from the start.\n",
      "\n",
      "section: Triggers in Mage via CLI\n",
      "question: Basic Commands\n",
      "answer: Docker Commands\n",
      "# Create a Docker Image from a base image\n",
      "Docker run -it ubuntu bash\n",
      "#List docker images\n",
      "Docker images list\n",
      "#List  Running containers\n",
      "Docker ps -a\n",
      "#List with full container ids\n",
      "Docker ps -a --no-trunc\n",
      "#Add onto existing image to create new image\n",
      "Docker commit -a <User_Name> -m \"Message\" container_id New_Image_Name\n",
      "# Create a Docker Image with an entrypoint from a base image\n",
      "Docker run -it --entry_point=bash python:3.11\n",
      "#Attach to a stopped container\n",
      "Docker start -ai <Container_Name>\n",
      "#Attach to a running container\n",
      "docker exec -it <Container_ID> bash\n",
      "#copying from host to container\n",
      "Docker cp <SRC_PATH/file> <containerid>:<dest_path>\n",
      "#copying from container to host\n",
      "Docker cp <containerid>:<Srct_path> <Dest Path on host/file>\n",
      "#Create an image from a docker file\n",
      "Docker build -t <Image_Name> <Location of Dockerfile>\n",
      "#DockerFile Options and best practices\n",
      "https://devopscube.com/build-docker-image/\n",
      "#Docker delete all images forcefully\n",
      "docker rmi -f $(docker images -aq)\n",
      "#Docker delete all containers forcefully\n",
      "docker rm -f $(docker ps -qa)\n",
      "#docker compose creation\n",
      "https://www.composerize.com/\n",
      "GCP Commands\n",
      "1.     Create SSH Keys\n",
      "2.     Added to the Settings of Compute Engine VM Instance\n",
      "3.     SSH-ed into the VM Instance with a config similar to following\n",
      "Host my-website.com\n",
      "HostName my-website.com\n",
      "User my-user\n",
      "IdentityFile ~/.ssh/id_rsa\n",
      "4.     Installed Anaconda by installing the sh file through bash <Anaconda.sh>\n",
      "5.     Install Docker after\n",
      "a.     Sudo apt-get update\n",
      "b.     Sudo apt-get docker\n",
      "6.     To run Docker without SUDO permissions\n",
      "a.     https://github.com/sindresorhus/guides/blob/main/docker-without-sudo.md\n",
      "7.     Google cloud remote copy\n",
      "a.     gcloud compute scp LOCAL_FILE_PATHVM_NAME:REMOTE_DIR\n",
      "Install GCP Cloud SDK on Docker Machine\n",
      "https://stackoverflow.com/questions/23247943/trouble-installing-google-cloud-sdk-in-ubuntu\n",
      "sudo apt-get install apt-transport-https ca-certificates gnupg && echo \"deb [signed-by=/usr/share/keyrings/cloud.google.gpg] https://packages.cloud.google.com/apt cloud-sdk main\"| sudo tee -a /etc/apt/sources.list.d/google-cloud-sdk.list&& curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key --keyring /usr/share/keyrings/cloud.google.gpg add - && sudo apt-get update && sudo apt-get install google-cloud-sdk && sudo apt-get install google-cloud-sdk-app-engine-java && sudo apt-get install google-cloud-sdk-app-engine-python && gcloud init\n",
      "Anaconda Commands\n",
      "#Activate environment\n",
      "Conda Activate <environment_name>\n",
      "#DeActivate environment\n",
      "Conda DeActivate <environment_name>\n",
      "#Start iterm without conda environment\n",
      "conda config --set auto_activate_base false\n",
      "# Using Conda forge as default (Community driven packaging recipes and solutions)\n",
      "https://conda-forge.org/docs/user/introduction.html\n",
      "conda --version\n",
      "conda update conda\n",
      "conda config --add channels conda-forge\n",
      "conda config --set channel_priority strict\n",
      "#Using Libmamba as Solver\n",
      "conda install pgcli  --solver=libmamba\n",
      "Linux/MAC Commands\n",
      "Starting and Stopping Services on Linux\n",
      "●  \tsudo systemctl start postgresql\n",
      "●  \tsudo systemctl stop postgresql\n",
      "Starting and Stopping Services on MAC\n",
      "●      launchctl start postgresql\n",
      "●      launchctl stop postgresql\n",
      "Identifying processes listening to a Port across MAC/Linux\n",
      "sudo lsof -i -P -n | grep LISTEN\n",
      "$ sudo netstat -tulpn | grep LISTEN\n",
      "$ sudo ss -tulpn | grep LISTEN\n",
      "$ sudo lsof -i:22 ## see a specific port such as 22 ##\n",
      "$ sudo nmap -sTU -O IP-address-Here\n",
      "Installing a package on Debian\n",
      "sudo apt install <packagename>\n",
      "Listing all package on Debian\n",
      "Dpkg -l | grep <packagename>\n",
      "UnInstalling a package on Debian\n",
      "Sudo apt remove <packagename>\n",
      "Sudo apt autoclean  && sudo apt autoremove\n",
      "List all Processes on Debian/Ubuntu\n",
      "Ps -aux\n",
      "apt-get update && apt-get install procps\n",
      "apt-get install iproute2 for ss -tulpn\n",
      "#Postgres Install\n",
      "sudo sh -c 'echo \"deb https://apt.postgresql.org/pub/repos/apt $(lsb_release -cs)-pgdg main\" > /etc/apt/sources.list.d/pgdg.list'\n",
      "wget --quiet -O - https://www.postgresql.org/media/keys/ACCC4CF8.asc | sudo apt-key add -\n",
      "sudo apt-get update\n",
      "sudo apt-get -y install postgresql\n",
      "#Changing Postgresql port to 5432\n",
      "- sudo service postgresql stop - sed -e 's/^port.*/port = 5432/' /etc/postgresql/10/main/postgresql.conf > postgresql.conf\n",
      "- sudo chown postgres postgresql.conf\n",
      "- sudo mv postgresql.conf /etc/postgresql/10/main\n",
      "- sudo systemctl restart postgresql\n",
      "\n",
      "section: General course-related questions\n",
      "question: Course - When will the course start?\n",
      "answer: The purpose of this document is to capture frequently asked technical questions\n",
      "The exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\n",
      "Subscribe to course public Google Calendar (it works from Desktop only).\n",
      "Register before the course starts using this link.\n",
      "Join the course Telegram channel with announcements.\n",
      "Don’t forget to register in DataTalks.Club's Slack and join the channel.\n",
      "\n",
      "section: error: Error while reading table: trips_data_all.external_fhv_tripdata, error message: Parquet column 'DOlocationID' has type INT64 which does not match the target cpp_type DOUBLE.\n",
      "question: GCP BQ - How to handle type error from big query and parquet data?\n",
      "answer: Problem: When you inject data into GCS using Pandas, there is a chance that some dataset has missing values on  DOlocationID and PUlocationID. Pandas by default will cast these columns as float data type, causing inconsistent data type between parquet in GCS and schema defined in big query. You will see something like this:\n",
      "Solution:\n",
      "Fix the data type issue in data pipeline\n",
      "Before injecting data into GCS, use astype and Int64 (which is different from int64 and accept both missing value and integer exist in the column) to cast the columns.\n",
      "Something like:\n",
      "df[\"PUlocationID\"] = df.PUlocationID.astype(\"Int64\")\n",
      "df[\"DOlocationID\"] = df.DOlocationID.astype(\"Int64\")\n",
      "NOTE: It is best to define the data type of all the columns in the Transformation section of the ETL pipeline before loading to BigQuery\n",
      "\n",
      "section: Module 5: pyspark\n",
      "question: Hadoop - Exception in thread \"main\" java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\n",
      "answer: If you are seeing this (or similar) error when attempting to write to parquet, it is likely an issue with your path variables.\n",
      "For Windows, create a new User Variable “HADOOP_HOME” that points to your Hadoop directory. Then add “%HADOOP_HOME%\\bin” to the PATH variable.\n",
      "Additional tips can be found here: https://stackoverflow.com/questions/41851066/exception-in-thread-main-java-lang-unsatisfiedlinkerror-org-apache-hadoop-io\n",
      "\n",
      "section: Module 3: Data Warehousing\n",
      "question: GCS Bucket - I query my dataset and get a Bad character (ASCII 0) error?\n",
      "answer: Check the Schema\n",
      "You might have a wrong formatting\n",
      "Try to upload the CSV.GZ files without formatting or going through pandas via wget\n",
      "See this Slack conversation for helpful tips\n",
      "\n",
      "section: General course-related questions\n",
      "question: Leaderboard - I am not on the leaderboard / how do I know which one I am on the leaderboard?\n",
      "answer: When you set up your account you are automatically assigned a random name such as “Lucid Elbakyan” for example. If you want to see what your Display name is.\n",
      "Go to the Homework submission link →  https://courses.datatalks.club/de-zoomcamp-2024/homework/hw2 - Log in > Click on ‘Data Engineering Zoom Camp 2024’ > click on ‘Edit Course Profile’ - your display name is here, you can also change it should you wish:\n",
      "\n",
      "section: General course-related questions\n",
      "question: Course - how many Zoomcamps in a year?\n",
      "answer: There are 3 Zoom Camps in a year, as of 2024. However, they are for separate courses:\n",
      "Data-Engineering (Jan - Apr)\n",
      "MLOps (May - Aug)\n",
      "Machine Learning (Sep - Jan)\n",
      "There's only one Data-Engineering Zoomcamp “live” cohort per year, for the certification. Same as for the other Zoomcamps.\n",
      "They follow pretty much the same schedule for each cohort per zoomcamp. For Data-Engineering it is (generally) from Jan-Apr of the year. If you’re not interested in the Certificate, you can take any zoom camps at any time, at your own pace, out of sync with any “live” cohort.\n",
      "\n",
      "section: Module 6: streaming with kafka\n",
      "question: data-engineering-zoomcamp/week_6_stream_processing/python/resources/rides.csv is missing\n",
      "answer: Copy the file found in the Java example: data-engineering-zoomcamp/week_6_stream_processing/java/kafka_examples/src/main/resources/rides.csv\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Terraform - Error: Failed to query available provider packages │ Could not retrieve the list of available versions for provider hashicorp/google: could not query │ provider registry for registry.terrafogorm.io/hashicorp/google: the request failed after 2 attempts, │ please try again later\n",
      "answer: It is an internet connectivity error, terraform is somehow not able to access the online registry. Check your VPN/Firewall settings (or just clear cookies or restart your network). Try terraform init again after this, it should work.\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Terraform - Error creating Bucket: googleapi: Error 403: Permission denied to access ‘storage.buckets.create’\n",
      "answer: The error:\n",
      "Error: googleapi: Error 403: terraform-trans-campus@trans-campus-410115.iam.gserviceaccount.com does not have storage.buckets.create access to the Google Cloud project. Permission 'storage.buckets.create' denied on resource (or it may not exist)., forbidden\n",
      "The solution:\n",
      "You have to declare the project name as your Project ID, and not your Project name, available on GCP console Dashboard.\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Terraform - Where can I find the Terraform 1.1.3 Linux (AMD 64)?\n",
      "answer: Here: https://releases.hashicorp.com/terraform/1.1.3/terraform_1.1.3_linux_amd64.zip\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Terraform - Error:Post \"https://storage.googleapis.com/storage/v1/b?alt=json&prettyPrint=false&project=coherent-ascent-379901\": oauth2: cannot fetch token: Post \"https://oauth2.googleapis.com/token\": dial tcp 172.217.163.42:443: i/o timeout\n",
      "answer: The issue was with the network. Google is not accessible in my country, I am using a VPN. And The terminal program does not automatically follow the system proxy and requires separate proxy configuration settings.I opened a Enhanced Mode in Clash, which is a VPN app, and 'terraform apply' works! So if you encounter the same issue, you can ask help for your vpn provider.\n",
      "</CONTEXT>\n",
      "\n",
      "<PREVIOUS_ACTIONS>\n",
      "{\"action\": \"SEARCH\", \"reasoning\": \"To provide a comprehensive answer on how to be successful in module 1, I need to gather relevant information from the FAQ database that might outline strategies, resources, or tips for success in this module.\", \"keywords\": [\"success tips for module 1\", \"how to excel module 1\", \"module 1 study strategies\"]}\n",
      "{\"action\": \"SEARCH\", \"reasoning\": \"To find more specific strategies or tips for success in Module 1, I will focus on searching for general best practices on studying and succeeding in technical courses, as well as any resources that may have been highlighted for Module 1.\", \"keywords\": [\"best practices for succeeding in technical modules\", \"study tips for IT courses\", \"resources for Module 1 Docker and Terraform\"]}\n",
      "</PREVIOUS_ACTIONS>\n",
      "{\n",
      "  \"action\": \"ANSWER\",\n",
      "  \"answer\": \"To be successful in Module 1 (Docker and Terraform), consider these strategies:\\n\\n1. **Hands-On Practice**: Engage with the hands-on labs and projects in the module. Docker and Terraform are practical tools that require operational knowledge, so practice building and deploying applications using these technologies.\\n\\n2. **Utilize Resources**: Make use of the resources provided in the course, such as video lectures, reading materials, and any supplementary materials.\\n\\n3. **Join Discussions**: Participate in forums or discussion groups with peers. This can enhance understanding and expose you to different perspectives on problem-solving.\\n\\n4. **Stay Organized**: Keep comprehensive notes on concepts and commands you interact with, as both Docker and Terraform might require remembering various commands and configurations.\\n\\n5. **Troubleshooting Skills**: Familiarize yourself with common problems and their solutions, as shown in the FAQs. For example, you might encounter errors related to module dependencies or network configuration issues.\\n\\n6. **Ask Questions**: Don\\u2019t hesitate to reach out for help, whether from instructors, peers, or the course's support channels.\\n\\n7. **Regularly Review Concepts**: Revisit core concepts frequently to reinforce your understanding and retention of the material. Use practice exercises to test your knowledge.\\n\\nBy following these strategies, you can enhance your learning experience and improve your chances of success in Module 1.\",\n",
      "  \"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "question = \"what do I need to do to be succesful at module 1?\"\n",
    "\n",
    "search_queries = []\n",
    "search_results = []\n",
    "previous_actions = []\n",
    "\n",
    "iteration = 0\n",
    "\n",
    "while True:\n",
    "    print(f'ITERATION #{iteration}...')\n",
    "\n",
    "    context = build_context(search_results)\n",
    "    prompt = prompt_template.format(\n",
    "        question = question,\n",
    "        context = context,\n",
    "        search_queries = \"\\n\".join(search_queries),\n",
    "        previous_actions = \"\\n\".join([json.dumps(a) for a in previous_actions]),\n",
    "        max_iterations = 3,\n",
    "        iteration_number = iteration\n",
    "    )\n",
    "\n",
    "    print(prompt)\n",
    "\n",
    "    answer_json = llm(prompt)\n",
    "    answer = json.loads(answer_json)\n",
    "    print(json.dumps(answer, indent=2))\n",
    "\n",
    "    previous_actions.append(answer)\n",
    "\n",
    "    action = answer['action']\n",
    "    if action != 'SEARCH':\n",
    "        break\n",
    "\n",
    "    keywords = answer['keywords']\n",
    "    search_queries = list(set(search_queries) | set(keywords))\n",
    "\n",
    "    for k in keywords:\n",
    "        res = search(k)\n",
    "        search_results.extend(res)\n",
    "\n",
    "    search_results = dedub(search_results)\n",
    "\n",
    "    iteration = iteration + 1\n",
    "    if iteration >= 4:\n",
    "        break\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "4d145cf5-3fc3-4297-98ad-e0d604963e45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'action': 'ANSWER',\n",
       " 'answer': \"To be successful in Module 1 (Docker and Terraform), consider these strategies:\\n\\n1. **Hands-On Practice**: Engage with the hands-on labs and projects in the module. Docker and Terraform are practical tools that require operational knowledge, so practice building and deploying applications using these technologies.\\n\\n2. **Utilize Resources**: Make use of the resources provided in the course, such as video lectures, reading materials, and any supplementary materials.\\n\\n3. **Join Discussions**: Participate in forums or discussion groups with peers. This can enhance understanding and expose you to different perspectives on problem-solving.\\n\\n4. **Stay Organized**: Keep comprehensive notes on concepts and commands you interact with, as both Docker and Terraform might require remembering various commands and configurations.\\n\\n5. **Troubleshooting Skills**: Familiarize yourself with common problems and their solutions, as shown in the FAQs. For example, you might encounter errors related to module dependencies or network configuration issues.\\n\\n6. **Ask Questions**: Don’t hesitate to reach out for help, whether from instructors, peers, or the course's support channels.\\n\\n7. **Regularly Review Concepts**: Revisit core concepts frequently to reinforce your understanding and retention of the material. Use practice exercises to test your knowledge.\\n\\nBy following these strategies, you can enhance your learning experience and improve your chances of success in Module 1.\",\n",
       " 'source': 'OWN_KNOWLEDGE'}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "80a1bf41-4585-42e3-9490-e9611fb077b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "1893bd9c-67ec-446e-8c60-acbc47b301d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agentic_search(question):\n",
    "    search_queries = []\n",
    "    search_results = []\n",
    "    previous_actions = []\n",
    "    \n",
    "    iteration = 0\n",
    "    \n",
    "    while True:\n",
    "        print(f'ITERATION #{iteration}...')\n",
    "    \n",
    "        context = build_context(search_results)\n",
    "        prompt = prompt_template.format(\n",
    "            question = question,\n",
    "            context = context,\n",
    "            search_queries = \"\\n\".join(search_queries),\n",
    "            previous_actions = \"\\n\".join([json.dumps(a) for a in previous_actions]),\n",
    "            max_iterations = 3,\n",
    "            iteration_number = iteration\n",
    "        )\n",
    "    \n",
    "        print(prompt)\n",
    "    \n",
    "        answer_json = llm(prompt)\n",
    "        answer = json.loads(answer_json)\n",
    "        print(json.dumps(answer, indent=2))\n",
    "    \n",
    "        previous_actions.append(answer)\n",
    "    \n",
    "        action = answer['action']\n",
    "        if action != 'SEARCH':\n",
    "            break\n",
    "    \n",
    "        keywords = answer['keywords']\n",
    "        search_queries = list(set(search_queries) | set(keywords))\n",
    "    \n",
    "        for k in keywords:\n",
    "            res = search(k)\n",
    "            search_results.extend(res)\n",
    "    \n",
    "        search_results = dedub(search_results)\n",
    "    \n",
    "        iteration = iteration + 1\n",
    "        if iteration >= 4:\n",
    "            break\n",
    "        print()\n",
    "        \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "ed378ea2-a4d2-4b5e-8aa0-d8395c10df35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION #0...\n",
      "You are a course teaching assistant.\n",
      "\n",
      "You are given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "\n",
      "The CONTEXT is build with the documents from our FAQ database.\n",
      "SEARCH_QUERIES contains the queries that were used to retrieve the documents\n",
      "from FAQ to and add them to the context.\n",
      "PREVIOUS_ACTIONS contains the actions you already performed.\n",
      "\n",
      "At the beginning the CONTEXT is empty.\n",
      "\n",
      "You can perform the following actions:\n",
      "\n",
      "- Search in the FAQ database to get more data for the CONTEXT\n",
      "- Answer the question using the CONTEXT\n",
      "- Answer the question using your own knowledge\n",
      "\n",
      "For the SEARCH action, build search requests based on the CONTEXT and the QUESTION.\n",
      "Carefully analyze the CONTEXT and generate the requests to deeply explore the topic.\n",
      "\n",
      "Don't use search queries used at the previous iterations.\n",
      "\n",
      "Don't repeat previously performed actions.\n",
      "\n",
      "Don't perform more than 3 iterations for a given student question.\n",
      "The current iteration number: 0. If we exceed the allowed number\n",
      "of iterations, give the best possible answer with the provided information.\n",
      "\n",
      "Output templates:\n",
      "\n",
      "If you want to perform search, use this template:\n",
      "\n",
      "{\n",
      "\"action\" : \"SEARCH\",\n",
      "\"reasoning\" : \"<add your reasoning here>\",\n",
      "\"keywords\" : [\"search query 1\", \"search query 2\", ...]\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\" : \"ANSWER_CONTEXT\",\n",
      "\"answer\" : \"<your answer>\",\n",
      "\"source\" :\"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\" : \"ANSWER\",\n",
      "\"answer\" : \"<your answer>\",\n",
      "\"source\" : \"OWN_KNOWLEDGE\"\n",
      "}\n",
      "\n",
      "<QUESTION>\n",
      "how do I prepare for the course?\n",
      "</QUESTION>\n",
      "\n",
      "<SEARCH_QUERIES>\n",
      "\n",
      "</SEARCH_QUERIES>\n",
      "\n",
      "<CONTEXT>\n",
      "\n",
      "</CONTEXT>\n",
      "\n",
      "<PREVIOUS_ACTIONS>\n",
      "\n",
      "</PREVIOUS_ACTIONS>\n",
      "{\n",
      "  \"action\": \"SEARCH\",\n",
      "  \"reasoning\": \"To provide a comprehensive answer about how to prepare for the course, I need to gather specific recommendations or guidelines that may be included in the FAQ. This will help ensure that students have all the necessary information to prepare effectively.\",\n",
      "  \"keywords\": [\n",
      "    \"preparing for the course\",\n",
      "    \"course preparation tips\",\n",
      "    \"how to prepare for classes\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "ITERATION #1...\n",
      "You are a course teaching assistant.\n",
      "\n",
      "You are given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "\n",
      "The CONTEXT is build with the documents from our FAQ database.\n",
      "SEARCH_QUERIES contains the queries that were used to retrieve the documents\n",
      "from FAQ to and add them to the context.\n",
      "PREVIOUS_ACTIONS contains the actions you already performed.\n",
      "\n",
      "At the beginning the CONTEXT is empty.\n",
      "\n",
      "You can perform the following actions:\n",
      "\n",
      "- Search in the FAQ database to get more data for the CONTEXT\n",
      "- Answer the question using the CONTEXT\n",
      "- Answer the question using your own knowledge\n",
      "\n",
      "For the SEARCH action, build search requests based on the CONTEXT and the QUESTION.\n",
      "Carefully analyze the CONTEXT and generate the requests to deeply explore the topic.\n",
      "\n",
      "Don't use search queries used at the previous iterations.\n",
      "\n",
      "Don't repeat previously performed actions.\n",
      "\n",
      "Don't perform more than 3 iterations for a given student question.\n",
      "The current iteration number: 1. If we exceed the allowed number\n",
      "of iterations, give the best possible answer with the provided information.\n",
      "\n",
      "Output templates:\n",
      "\n",
      "If you want to perform search, use this template:\n",
      "\n",
      "{\n",
      "\"action\" : \"SEARCH\",\n",
      "\"reasoning\" : \"<add your reasoning here>\",\n",
      "\"keywords\" : [\"search query 1\", \"search query 2\", ...]\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\" : \"ANSWER_CONTEXT\",\n",
      "\"answer\" : \"<your answer>\",\n",
      "\"source\" :\"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\" : \"ANSWER\",\n",
      "\"answer\" : \"<your answer>\",\n",
      "\"source\" : \"OWN_KNOWLEDGE\"\n",
      "}\n",
      "\n",
      "<QUESTION>\n",
      "how do I prepare for the course?\n",
      "</QUESTION>\n",
      "\n",
      "<SEARCH_QUERIES>\n",
      "how to prepare for classes\n",
      "course preparation tips\n",
      "preparing for the course\n",
      "</SEARCH_QUERIES>\n",
      "\n",
      "<CONTEXT>\n",
      "section: General course-related questions\n",
      "question: Course - Can I follow the course after it finishes?\n",
      "answer: Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\n",
      "You can also continue looking at the homeworks and continue preparing for the next cohort. I guess you can also start working on your final capstone project.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Course - When will the course start?\n",
      "answer: The purpose of this document is to capture frequently asked technical questions\n",
      "The exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\n",
      "Subscribe to course public Google Calendar (it works from Desktop only).\n",
      "Register before the course starts using this link.\n",
      "Join the course Telegram channel with announcements.\n",
      "Don’t forget to register in DataTalks.Club's Slack and join the channel.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Certificate - Can I follow the course in a self-paced mode and get a certificate?\n",
      "answer: No, you can only get a certificate if you finish the course with a “live” cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Is it possible to use tool “X” instead of the one tool you use in the course?\n",
      "answer: Yes, this applies if you want to use Airflow or Prefect instead of Mage, AWS or Snowflake instead of GCP products or Tableau instead of Metabase or Google data studio.\n",
      "The course covers 2 alternative data stacks, one using GCP and one using local installation of everything. You can use one of them or use your tool of choice.\n",
      "Should you consider it instead of the one tool you use? That we can’t support you if you choose to use a different stack, also you would need to explain the different choices of tool for the peer review of your capstone project.\n",
      "\n",
      "section: General course-related questions\n",
      "question: How do I use Git / GitHub for this course?\n",
      "answer: After you create a GitHub account, you should clone the course repo to your local machine using the process outlined in this video: Git for Everybody: How to Clone a Repository from GitHub\n",
      "Having this local repository on your computer will make it easy for you to access the instructors’ code and make pull requests (if you want to add your own notes or make changes to the course content).\n",
      "You will probably also create your own repositories that host your notes, versions of your file, to do this. Here is a great tutorial that shows you how to do this: https://www.atlassian.com/git/tutorials/setting-up-a-repository\n",
      "Remember to ignore large database, .csv, and .gz files, and other files that should not be saved to a repository. Use .gitignore for this: https://www.atlassian.com/git/tutorials/saving-changes/gitignore NEVER store passwords or keys in a git repo (even if that repo is set to private).\n",
      "This is also a great resource: https://dangitgit.com/\n",
      "\n",
      "section: Module 5: pyspark\n",
      "question: Spark Cloud Storage connector\n",
      "answer: Link to Slack Thread : has anyone figured out how to read from GCP data lake instead of downloading all the taxi data again?\n",
      "There’s a few extra steps to go into reading from GCS with PySpark\n",
      "1.)  IMPORTANT: Download the Cloud Storage connector for Hadoop here: https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage#clusters\n",
      "As the name implies, this .jar file is what essentially connects PySpark with your GCS\n",
      "2.) Move the .jar file to your Spark file directory. I installed Spark using homebrew on my MacOS machine and I had to create a /jars directory under \"/opt/homebrew/Cellar/apache-spark/3.2.1/ (where my spark dir is located)\n",
      "3.) In your Python script, there are a few extra classes you’ll have to import:\n",
      "import pyspark\n",
      "from pyspark.sql import SparkSession\n",
      "from pyspark.conf import SparkConf\n",
      "from pyspark.context import SparkContext\n",
      "4.) You must set up your configurations before building your SparkSession. Here’s my code snippet:\n",
      "conf = SparkConf() \\\n",
      ".setMaster('local[*]') \\\n",
      ".setAppName('test') \\\n",
      ".set(\"spark.jars\", \"/opt/homebrew/Cellar/apache-spark/3.2.1/jars/gcs-connector-hadoop3-latest.jar\") \\\n",
      ".set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
      ".set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", \"path/to/google_credentials.json\")\n",
      "sc = SparkContext(conf=conf)\n",
      "sc._jsc.hadoopConfiguration().set(\"fs.AbstractFileSystem.gs.impl\",  \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
      "sc._jsc.hadoopConfiguration().set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
      "sc._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.json.keyfile\", \"path/to/google_credentials.json\")\n",
      "sc._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.enable\", \"true\")\n",
      "5.) Once you run that, build your SparkSession with the new parameters we’d just instantiated in the previous step:\n",
      "spark = SparkSession.builder \\\n",
      ".config(conf=sc.getConf()) \\\n",
      ".getOrCreate()\n",
      "6.) Finally, you’re able to read your files straight from GCS!\n",
      "df_green = spark.read.parquet(\"gs://{BUCKET}/green/202*/\")\n",
      "\n",
      "section: Module 6: streaming with kafka\n",
      "question: Java Kafka: Tests are not picked up in VSCode\n",
      "answer: Situation: in VS Code, usually there will be a triangle icon next to each test. I couldn’t see it at first and had to do some fixes.\n",
      "Solution:\n",
      "(Source)\n",
      "VS Code\n",
      "→ Explorer (first icon on the left navigation bar)\n",
      "→ JAVA PROJECTS (bottom collapsable)\n",
      "→  icon next in the rightmost position to JAVA PROJECTS\n",
      "→  clean Workspace\n",
      "→ Confirm by clicking Reload and Delete\n",
      "Now you will be able to see the triangle icon next to each test like what you normally see in python tests.\n",
      "E.g.:\n",
      "You can also add classes and packages in this window instead of creating files in the project directory\n",
      "</CONTEXT>\n",
      "\n",
      "<PREVIOUS_ACTIONS>\n",
      "{\"action\": \"SEARCH\", \"reasoning\": \"To provide a comprehensive answer about how to prepare for the course, I need to gather specific recommendations or guidelines that may be included in the FAQ. This will help ensure that students have all the necessary information to prepare effectively.\", \"keywords\": [\"preparing for the course\", \"course preparation tips\", \"how to prepare for classes\"]}\n",
      "</PREVIOUS_ACTIONS>\n",
      "{\n",
      "  \"action\": \"SEARCH\",\n",
      "  \"reasoning\": \"Since the previous search didn't yield specific information on course preparation, I will look for general tips or guidelines on preparing for classes, focusing on any additional resources or recommendations that could assist students in effectively getting ready for the course.\",\n",
      "  \"keywords\": [\n",
      "    \"course preparation guidelines\",\n",
      "    \"tips for studying for a course\",\n",
      "    \"how to prepare for a live cohort course\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "ITERATION #2...\n",
      "You are a course teaching assistant.\n",
      "\n",
      "You are given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "\n",
      "The CONTEXT is build with the documents from our FAQ database.\n",
      "SEARCH_QUERIES contains the queries that were used to retrieve the documents\n",
      "from FAQ to and add them to the context.\n",
      "PREVIOUS_ACTIONS contains the actions you already performed.\n",
      "\n",
      "At the beginning the CONTEXT is empty.\n",
      "\n",
      "You can perform the following actions:\n",
      "\n",
      "- Search in the FAQ database to get more data for the CONTEXT\n",
      "- Answer the question using the CONTEXT\n",
      "- Answer the question using your own knowledge\n",
      "\n",
      "For the SEARCH action, build search requests based on the CONTEXT and the QUESTION.\n",
      "Carefully analyze the CONTEXT and generate the requests to deeply explore the topic.\n",
      "\n",
      "Don't use search queries used at the previous iterations.\n",
      "\n",
      "Don't repeat previously performed actions.\n",
      "\n",
      "Don't perform more than 3 iterations for a given student question.\n",
      "The current iteration number: 2. If we exceed the allowed number\n",
      "of iterations, give the best possible answer with the provided information.\n",
      "\n",
      "Output templates:\n",
      "\n",
      "If you want to perform search, use this template:\n",
      "\n",
      "{\n",
      "\"action\" : \"SEARCH\",\n",
      "\"reasoning\" : \"<add your reasoning here>\",\n",
      "\"keywords\" : [\"search query 1\", \"search query 2\", ...]\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\" : \"ANSWER_CONTEXT\",\n",
      "\"answer\" : \"<your answer>\",\n",
      "\"source\" :\"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\" : \"ANSWER\",\n",
      "\"answer\" : \"<your answer>\",\n",
      "\"source\" : \"OWN_KNOWLEDGE\"\n",
      "}\n",
      "\n",
      "<QUESTION>\n",
      "how do I prepare for the course?\n",
      "</QUESTION>\n",
      "\n",
      "<SEARCH_QUERIES>\n",
      "how to prepare for classes\n",
      "tips for studying for a course\n",
      "course preparation guidelines\n",
      "how to prepare for a live cohort course\n",
      "course preparation tips\n",
      "preparing for the course\n",
      "</SEARCH_QUERIES>\n",
      "\n",
      "<CONTEXT>\n",
      "section: General course-related questions\n",
      "question: Course - Can I follow the course after it finishes?\n",
      "answer: Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\n",
      "You can also continue looking at the homeworks and continue preparing for the next cohort. I guess you can also start working on your final capstone project.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Course - When will the course start?\n",
      "answer: The purpose of this document is to capture frequently asked technical questions\n",
      "The exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\n",
      "Subscribe to course public Google Calendar (it works from Desktop only).\n",
      "Register before the course starts using this link.\n",
      "Join the course Telegram channel with announcements.\n",
      "Don’t forget to register in DataTalks.Club's Slack and join the channel.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Certificate - Can I follow the course in a self-paced mode and get a certificate?\n",
      "answer: No, you can only get a certificate if you finish the course with a “live” cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Is it possible to use tool “X” instead of the one tool you use in the course?\n",
      "answer: Yes, this applies if you want to use Airflow or Prefect instead of Mage, AWS or Snowflake instead of GCP products or Tableau instead of Metabase or Google data studio.\n",
      "The course covers 2 alternative data stacks, one using GCP and one using local installation of everything. You can use one of them or use your tool of choice.\n",
      "Should you consider it instead of the one tool you use? That we can’t support you if you choose to use a different stack, also you would need to explain the different choices of tool for the peer review of your capstone project.\n",
      "\n",
      "section: General course-related questions\n",
      "question: How do I use Git / GitHub for this course?\n",
      "answer: After you create a GitHub account, you should clone the course repo to your local machine using the process outlined in this video: Git for Everybody: How to Clone a Repository from GitHub\n",
      "Having this local repository on your computer will make it easy for you to access the instructors’ code and make pull requests (if you want to add your own notes or make changes to the course content).\n",
      "You will probably also create your own repositories that host your notes, versions of your file, to do this. Here is a great tutorial that shows you how to do this: https://www.atlassian.com/git/tutorials/setting-up-a-repository\n",
      "Remember to ignore large database, .csv, and .gz files, and other files that should not be saved to a repository. Use .gitignore for this: https://www.atlassian.com/git/tutorials/saving-changes/gitignore NEVER store passwords or keys in a git repo (even if that repo is set to private).\n",
      "This is also a great resource: https://dangitgit.com/\n",
      "\n",
      "section: Module 5: pyspark\n",
      "question: Spark Cloud Storage connector\n",
      "answer: Link to Slack Thread : has anyone figured out how to read from GCP data lake instead of downloading all the taxi data again?\n",
      "There’s a few extra steps to go into reading from GCS with PySpark\n",
      "1.)  IMPORTANT: Download the Cloud Storage connector for Hadoop here: https://cloud.google.com/dataproc/docs/concepts/connectors/cloud-storage#clusters\n",
      "As the name implies, this .jar file is what essentially connects PySpark with your GCS\n",
      "2.) Move the .jar file to your Spark file directory. I installed Spark using homebrew on my MacOS machine and I had to create a /jars directory under \"/opt/homebrew/Cellar/apache-spark/3.2.1/ (where my spark dir is located)\n",
      "3.) In your Python script, there are a few extra classes you’ll have to import:\n",
      "import pyspark\n",
      "from pyspark.sql import SparkSession\n",
      "from pyspark.conf import SparkConf\n",
      "from pyspark.context import SparkContext\n",
      "4.) You must set up your configurations before building your SparkSession. Here’s my code snippet:\n",
      "conf = SparkConf() \\\n",
      ".setMaster('local[*]') \\\n",
      ".setAppName('test') \\\n",
      ".set(\"spark.jars\", \"/opt/homebrew/Cellar/apache-spark/3.2.1/jars/gcs-connector-hadoop3-latest.jar\") \\\n",
      ".set(\"spark.hadoop.google.cloud.auth.service.account.enable\", \"true\") \\\n",
      ".set(\"spark.hadoop.google.cloud.auth.service.account.json.keyfile\", \"path/to/google_credentials.json\")\n",
      "sc = SparkContext(conf=conf)\n",
      "sc._jsc.hadoopConfiguration().set(\"fs.AbstractFileSystem.gs.impl\",  \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFS\")\n",
      "sc._jsc.hadoopConfiguration().set(\"fs.gs.impl\", \"com.google.cloud.hadoop.fs.gcs.GoogleHadoopFileSystem\")\n",
      "sc._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.json.keyfile\", \"path/to/google_credentials.json\")\n",
      "sc._jsc.hadoopConfiguration().set(\"fs.gs.auth.service.account.enable\", \"true\")\n",
      "5.) Once you run that, build your SparkSession with the new parameters we’d just instantiated in the previous step:\n",
      "spark = SparkSession.builder \\\n",
      ".config(conf=sc.getConf()) \\\n",
      ".getOrCreate()\n",
      "6.) Finally, you’re able to read your files straight from GCS!\n",
      "df_green = spark.read.parquet(\"gs://{BUCKET}/green/202*/\")\n",
      "\n",
      "section: Module 6: streaming with kafka\n",
      "question: Java Kafka: Tests are not picked up in VSCode\n",
      "answer: Situation: in VS Code, usually there will be a triangle icon next to each test. I couldn’t see it at first and had to do some fixes.\n",
      "Solution:\n",
      "(Source)\n",
      "VS Code\n",
      "→ Explorer (first icon on the left navigation bar)\n",
      "→ JAVA PROJECTS (bottom collapsable)\n",
      "→  icon next in the rightmost position to JAVA PROJECTS\n",
      "→  clean Workspace\n",
      "→ Confirm by clicking Reload and Delete\n",
      "Now you will be able to see the triangle icon next to each test like what you normally see in python tests.\n",
      "E.g.:\n",
      "You can also add classes and packages in this window instead of creating files in the project directory\n",
      "\n",
      "section: General course-related questions\n",
      "question: Course - Is the current cohort going to be different from the previous cohort?\n",
      "answer: Yes. For the 2024 edition we are using Mage AI instead of Prefect and re-recorded the terraform videos, For 2023, we used Prefect instead of Airflow..\n",
      "\n",
      "section: General course-related questions\n",
      "question: Course - how many Zoomcamps in a year?\n",
      "answer: There are 3 Zoom Camps in a year, as of 2024. However, they are for separate courses:\n",
      "Data-Engineering (Jan - Apr)\n",
      "MLOps (May - Aug)\n",
      "Machine Learning (Sep - Jan)\n",
      "There's only one Data-Engineering Zoomcamp “live” cohort per year, for the certification. Same as for the other Zoomcamps.\n",
      "They follow pretty much the same schedule for each cohort per zoomcamp. For Data-Engineering it is (generally) from Jan-Apr of the year. If you’re not interested in the Certificate, you can take any zoom camps at any time, at your own pace, out of sync with any “live” cohort.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Besides the “Office Hour” which are the live zoom calls?\n",
      "answer: We will probably have some calls during the Capstone period to clear some questions but it will be announced in advance if that happens.\n",
      "</CONTEXT>\n",
      "\n",
      "<PREVIOUS_ACTIONS>\n",
      "{\"action\": \"SEARCH\", \"reasoning\": \"To provide a comprehensive answer about how to prepare for the course, I need to gather specific recommendations or guidelines that may be included in the FAQ. This will help ensure that students have all the necessary information to prepare effectively.\", \"keywords\": [\"preparing for the course\", \"course preparation tips\", \"how to prepare for classes\"]}\n",
      "{\"action\": \"SEARCH\", \"reasoning\": \"Since the previous search didn't yield specific information on course preparation, I will look for general tips or guidelines on preparing for classes, focusing on any additional resources or recommendations that could assist students in effectively getting ready for the course.\", \"keywords\": [\"course preparation guidelines\", \"tips for studying for a course\", \"how to prepare for a live cohort course\"]}\n",
      "</PREVIOUS_ACTIONS>\n",
      "{\n",
      "  \"action\": \"ANSWER\",\n",
      "  \"answer\": \"To prepare for the course effectively, here are some general tips: \\n1. **Register Early**: Make sure to register for the course before it starts, using the provided link. \\n2. **Join Communication Channels**: Subscribe to the Google Calendar for course schedules and join the Telegram channel for announcements. Additionally, registering on the DataTalks.Club's Slack and joining relevant channels will keep you updated. \\n3. **Familiarize Yourself with Required Tools**: Ensure you have access to the tools and software required for the course. Review any documentation or tutorials related to the tools mentioned in the course structure. \\n4. **Establish a Study Schedule**: Allocate dedicated times each week for coursework, reviewing materials, and participating in discussions during live sessions. \\n5. **Prepare Your Workspace**: Set up a comfortable and distraction-free environment for study sessions. \\n6. **Engage with Peers**: Connecting with fellow students can provide support and motivation, as well as insights into course material.  \\n7. **Review Past Materials**: If available, look at materials from previous cohorts to understand what to expect.\",\n",
      "  \"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'action': 'ANSWER',\n",
       " 'answer': \"To prepare for the course effectively, here are some general tips: \\n1. **Register Early**: Make sure to register for the course before it starts, using the provided link. \\n2. **Join Communication Channels**: Subscribe to the Google Calendar for course schedules and join the Telegram channel for announcements. Additionally, registering on the DataTalks.Club's Slack and joining relevant channels will keep you updated. \\n3. **Familiarize Yourself with Required Tools**: Ensure you have access to the tools and software required for the course. Review any documentation or tutorials related to the tools mentioned in the course structure. \\n4. **Establish a Study Schedule**: Allocate dedicated times each week for coursework, reviewing materials, and participating in discussions during live sessions. \\n5. **Prepare Your Workspace**: Set up a comfortable and distraction-free environment for study sessions. \\n6. **Engage with Peers**: Connecting with fellow students can provide support and motivation, as well as insights into course material.  \\n7. **Review Past Materials**: If available, look at materials from previous cohorts to understand what to expect.\",\n",
       " 'source': 'OWN_KNOWLEDGE'}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agentic_search(\"how do I prepare for the course?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "d0c7c43f-333d-4f47-bde4-f4dd6340e406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To prepare for the course effectively, here are some general tips: \n",
      "1. **Register Early**: Make sure to register for the course before it starts, using the provided link. \n",
      "2. **Join Communication Channels**: Subscribe to the Google Calendar for course schedules and join the Telegram channel for announcements. Additionally, registering on the DataTalks.Club's Slack and joining relevant channels will keep you updated. \n",
      "3. **Familiarize Yourself with Required Tools**: Ensure you have access to the tools and software required for the course. Review any documentation or tutorials related to the tools mentioned in the course structure. \n",
      "4. **Establish a Study Schedule**: Allocate dedicated times each week for coursework, reviewing materials, and participating in discussions during live sessions. \n",
      "5. **Prepare Your Workspace**: Set up a comfortable and distraction-free environment for study sessions. \n",
      "6. **Engage with Peers**: Connecting with fellow students can provide support and motivation, as well as insights into course material.  \n",
      "7. **Review Past Materials**: If available, look at materials from previous cohorts to understand what to expect.\n"
     ]
    }
   ],
   "source": [
    "print(_['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66ff8de-57b9-4c6b-999f-4e7330f47a9e",
   "metadata": {},
   "source": [
    "#### Function calling (\"tool use\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "872f005f-75b5-4697-b2db-e9c0553d90c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    boost = {'question': 3.0, 'section': 0.5}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={'course': 'data-engineering-zoomcamp'},\n",
    "        boost_dict=boost,\n",
    "        num_results=5,\n",
    "        output_ids=True\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "1aafca73-9e92-4309-a94e-5c21ccf5abca",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_tool = {\n",
    "    \"type\" : \"function\",\n",
    "    \"name\" : \"search\",\n",
    "    \"description\" : \"Search the FAQ database\",\n",
    "    \"parameters\" : {\n",
    "        \"type\" : \"object\",\n",
    "        \"properties\" : {\n",
    "            \"query\" : {\n",
    "                \"type\" : \"string\",\n",
    "                \"description\" : \"Search query text to look up in the course FAQ.\"\n",
    "            }\n",
    "        },\n",
    "        \"required\" : [\"query\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "92a19cd2-6e4b-4e30-8707-0fdbee3ccac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ResponseFunctionToolCall(arguments='{\"query\":\"module 1 strengths\"}', call_id='call_VzH6cLNCGKH9D7Flc0s3ZCZg', name='search', type='function_call', id='fc_689ff2b3e18481908683cecd7de4cc050ca86754c69b442f', status='completed')]"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"what do I do well in module 1?\"\n",
    "\n",
    "developer_prompt = \"\"\"\n",
    "You're a course teaching assistant.\n",
    "You're given a question from a course student and your task is to answer it.\n",
    "\"\"\".strip()\n",
    "\n",
    "tools = [search_tool]\n",
    "\n",
    "chat_messages = [\n",
    "    {\"role\" : \"developer\", \"content\" : developer_prompt},\n",
    "    {\"role\" : \"user\", \"content\" : question}\n",
    "]\n",
    "\n",
    "response = client.responses.create(\n",
    "    model = 'gpt-4o-mini',\n",
    "    input = chat_messages,\n",
    "    tools = tools\n",
    ")\n",
    "response.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "df42a458-6a99-45fc-a316-15ea352aa148",
   "metadata": {},
   "outputs": [],
   "source": [
    "calls = response.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "dfb269f3-5ec2-4e30-b1d8-9001f1b12a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "call = calls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "823836d1-6518-4abc-b8d1-3b6b3649b00a",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_name = call.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "1cb18449-3aec-4786-a614-c57b064853ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments = json.loads(call.arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "43d6807c-4208-499f-9b7b-f92533d9235c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'module 1 strengths'}"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "8e57e037-7c76-4961-b06f-45f2a407161c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.search(query)>"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globals()[f_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "d2330142-af80-4f3f-81ff-e84ba02c0538",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.search(query)>"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globals()['search']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "f8409562-beef-4a69-a159-74486ddd6d45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'type': 'function',\n",
       " 'name': 'search',\n",
       " 'description': 'Search the FAQ database',\n",
       " 'parameters': {'type': 'object',\n",
       "  'properties': {'query': {'type': 'string',\n",
       "    'description': 'Search query text to look up in the course FAQ.'}},\n",
       "  'required': ['query'],\n",
       "  'additionalProperties': False}}"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globals()['search_tool']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "59ec7a74-34ea-48c2-8467-4fbcda7277ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = globals()[f_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "ebdff1f0-3130-462a-9a1f-f69594f0f165",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "results = f(**arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ab9fe1f3-1a7b-465d-9980-0226b016d77d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'You need to look for the Py4J file and note the version of the filename. Once you know the version, you can update the export command accordingly, this is how you check yours:\\n` ls ${SPARK_HOME}/python/lib/ ` and then you add it in the export command, mine was:\\nexport PYTHONPATH=”${SPARK_HOME}/python/lib/Py4J-0.10.9.5-src.zip:${PYTHONPATH}”\\nMake sure that the version under `${SPARK_HOME}/python/lib/` matches the filename of py4j or you will encounter `ModuleNotFoundError: No module named \\'py4j\\'` while executing `import pyspark`.\\nFor instance, if the file under `${SPARK_HOME}/python/lib/` was `py4j-0.10.9.3-src.zip`.\\nThen the export PYTHONPATH statement above should be changed to `export PYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH\"` appropriately.\\nAdditionally, you can check for the version of ‘py4j’ of the spark you’re using from here and update as mentioned above.\\n~ Abhijit Chakraborty: Sometimes, even with adding the correct version of py4j might not solve the problem. Simply run pip install py4j and problem should be resolved.',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': \"Py4JJavaError - ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`\",\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  '_id': 323},\n",
       " {'text': \"Following dbt with BigQuery on Docker readme.md, after `docker-compose build` and `docker-compose run dbt-bq-dtc init`, encountered error `ModuleNotFoundError: No module named 'pytz'`\\nSolution:\\nAdd `RUN python -m pip install --no-cache pytz` in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`\",\n",
       "  'section': 'Module 4: analytics engineering with dbt',\n",
       "  'question': \"DBT - Error: No module named 'pytz' while setting up dbt with docker\",\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  '_id': 299},\n",
       " {'text': 'Even after installing pyspark correctly on linux machine (VM ) as per course instructions, faced a module not found error in jupyter notebook .\\nThe solution which worked for me(use following in jupyter notebook) :\\n!pip install findspark\\nimport findspark\\nfindspark.init()\\nThereafter , import pyspark and create spark contex<<t as usual\\nNone of the solutions above worked for me till I ran !pip3 install pyspark instead !pip install pyspark.\\nFilter based on conditions based on multiple columns\\nfrom pyspark.sql.functions import col\\nnew_final.filter((new_final.a_zone==\"Murray Hill\") & (new_final.b_zone==\"Midwood\")).show()\\nKrishna Anand',\n",
       "  'section': 'Module 5: pyspark',\n",
       "  'question': 'Module Not Found Error in Jupyter Notebook .',\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  '_id': 322},\n",
       " {'text': \"Issue:\\ne…\\nSolution:\\npip install psycopg2-binary\\nIf you already have it, you might need to update it:\\npip install psycopg2-binary --upgrade\\nOther methods, if the above fails:\\nif you are getting the “ ModuleNotFoundError: No module named 'psycopg2' “ error even after the above installation, then try updating conda using the command conda update -n base -c defaults conda. Or if you are using pip, then try updating it before installing the psycopg packages i.e\\nFirst uninstall the psycopg package\\nThen update conda or pip\\nThen install psycopg again using pip.\\nif you are still facing error with r pcycopg2 and showing pg_config not found then you will have to install postgresql. in MAC it is brew install postgresql\",\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': \"Postgres - ModuleNotFoundError: No module named 'psycopg2'\",\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  '_id': 112},\n",
       " {'text': 'create_engine(\\'postgresql://root:root@localhost:5432/ny_taxi\\')  I get the error \"TypeError: \\'module\\' object is not callable\"\\nSolution:\\nconn_string = \"postgresql+psycopg://root:root@localhost:5432/ny_taxi\"\\nengine = create_engine(conn_string)',\n",
       "  'section': 'Module 1: Docker and Terraform',\n",
       "  'question': \"Python - SQLALchemy - TypeError 'module' object is not callable\",\n",
       "  'course': 'data-engineering-zoomcamp',\n",
       "  '_id': 124}]"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47cd4b84-fc1f-4a42-aa10-bcb8e1ddc48d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
