{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb95e953-63a2-421f-b4a6-9a9a946da18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "\n",
    "docs_url = 'https://github.com/alexeygrigorev/llm-rag-workshop/raw/main/notebooks/documents.json'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()\n",
    "\n",
    "documents = []\n",
    "\n",
    "for course in documents_raw:\n",
    "    course_name = course['course']\n",
    "\n",
    "    for doc in course['documents']:\n",
    "        doc['course'] = course_name\n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e8e7a2a8-40da-4700-b535-86e6149a0d99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.append.AppendableIndex at 0x7cc78859b0e0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from minsearch import AppendableIndex\n",
    "\n",
    "index = AppendableIndex(\n",
    "    text_fields = [\"question\", \"text\", \"section\"],\n",
    "    keyword_fields = [\"course\"]\n",
    ")\n",
    "\n",
    "index.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b25824b-16a5-42fd-a509-c902cafc898c",
   "metadata": {},
   "outputs": [],
   "source": [
    "index.search(\"how to use kafka with spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd2d7d5c-f703-48e7-8083-9fd9250f849f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    boost = {'question': 3.0, 'section': 0.5}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={'course': 'data-engineering-zoomcamp'},\n",
    "        boost_dict=boost,\n",
    "        num_results=5,\n",
    "        output_ids=True\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33cd3a60-1729-4dcc-beb5-fd311cdf0d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"how to use kafka with spark\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1a8fc61d-d08b-456f-ad1d-6d6a42b1642c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "<QUESTION>\n",
    "{question}\n",
    "</QUESTION>\n",
    "\n",
    "<CONTEXT>\n",
    "{context}\n",
    "</CONTEXT>\n",
    "\"\"\".strip()\n",
    "\n",
    "def build_prompt(query, search_results):\n",
    "    context = \"\"\n",
    "\n",
    "    for doc in search_results:\n",
    "        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\"\n",
    "    \n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e9ee9f5c-d9e4-4638-9eca-5c9207a033a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = search(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "107bfa23-cda8-4ef3-bc0d-07a2efee876a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = build_prompt(question, search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fdd9be5a-2d6b-431f-a282-bb6485a93f14",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
      "Use only the facts from the CONTEXT when answering the QUESTION.\n",
      "\n",
      "<QUESTION>\n",
      "how to use kafka with spark\n",
      "</QUESTION>\n",
      "\n",
      "<CONTEXT>\n",
      "section: Module 6: streaming with kafka\n",
      "question: Python Kafka: ./spark-submit.sh streaming.py - ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.\n",
      "answer: While following tutorial 13.2 , when running ./spark-submit.sh streaming.py, encountered the following error:\n",
      "…\n",
      "24/03/11 09:48:36 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077...\n",
      "24/03/11 09:48:36 INFO TransportClientFactory: Successfully created connection to localhost/127.0.0.1:7077 after 10 ms (0 ms spent in bootstraps)\n",
      "24/03/11 09:48:54 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\n",
      "24/03/11 09:48:56 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077…\n",
      "24/03/11 09:49:16 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077...\n",
      "24/03/11 09:49:36 WARN StandaloneSchedulerBackend: Application ID is not initialized yet.\n",
      "24/03/11 09:49:36 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.\n",
      "…\n",
      "py4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.sql.SparkSession.\n",
      ": java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\n",
      "…\n",
      "Solution:\n",
      "Downgrade your local PySpark to 3.3.1 (same as Dockerfile)\n",
      "The reason for the failed connection in my case was the mismatch of PySpark versions. You can see that from the logs of spark-master in the docker container.\n",
      "Solution 2:\n",
      "Check what Spark version your local machine has\n",
      "pyspark –version\n",
      "spark-submit –version\n",
      "Add your version to SPARK_VERSION in build.sh\n",
      "\n",
      "section: Module 6: streaming with kafka\n",
      "question: Python Kafka: ./spark-submit.sh streaming.py - How to check why Spark master connection fails\n",
      "answer: Start a new terminal\n",
      "Run: docker ps\n",
      "Copy the CONTAINER ID of the spark-master container\n",
      "Run: docker exec -it <spark_master_container_id> bash\n",
      "Run: cat logs/spark-master.out\n",
      "Check for the log when the error happened\n",
      "Google the error message from there\n",
      "\n",
      "section: Module 6: streaming with kafka\n",
      "question: kafka.errors.NoBrokersAvailable: NoBrokersAvailable\n",
      "answer: If you have this error, it most likely that your kafka broker docker container is not working.\n",
      "Use docker ps to confirm\n",
      "Then in the docker compose yaml file folder, run docker compose up -d to start all the instances.\n",
      "\n",
      "section: Module 6: streaming with kafka\n",
      "question: Kafka- python videos have low audio and hard to follow up\n",
      "answer: tip:As the videos have low audio so I downloaded them and used VLC media player with putting the audio to the max 200% of original audio and the audio became quite good or try to use auto caption generated on Youtube directly.\n",
      "Kafka Python Videos - Rides.csv\n",
      "There is no clear explanation of the rides.csv data that the producer.py python programs use. You can find that here https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/2bd33e89906181e424f7b12a299b70b19b7cfcd5/week_6_stream_processing/python/resources/rides.csv.\n",
      "\n",
      "section: Module 6: streaming with kafka\n",
      "question: Java Kafka: <project_name>-1.0-SNAPSHOT.jar errors: package xxx does not exist even after gradle build\n",
      "answer: In my set up, all of the dependencies listed in gradle.build were not installed in <project_name>-1.0-SNAPSHOT.jar.\n",
      "Solution:\n",
      "In build.gradle file, I added the following at the end:\n",
      "shadowJar {\n",
      "archiveBaseName = \"java-kafka-rides\"\n",
      "archiveClassifier = ''\n",
      "}\n",
      "And then in the command line ran ‘gradle shadowjar’, and run the script from java-kafka-rides-1.0-SNAPSHOT.jar created by the shadowjar\n",
      "\n",
      "\n",
      "</CONTEXT>\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c1bc124-3bb6-4ba2-a06f-7ce208cb4aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def llm(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def rag(query):\n",
    "    search_results = search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    answer = llm(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "90deb85e-d269-47dd-93b7-a4f51c83f990",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c8a0d682-3437-4dd5-8ba9-e6fc99b70e4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To use Kafka with Spark, first ensure that your Kafka broker is running properly; you can check its status with `docker ps`. If your Kafka container is not working, start it by navigating to the folder containing your docker compose YAML file and running `docker compose up -d`.\n",
      "\n",
      "Next, for streaming with Spark, you might need to check that your Spark environment is correctly set up. If you encounter any connection issues with the Spark master, you can start a new terminal session, get the container ID of the Spark master with `docker ps`, and then view the logs using:\n",
      "\n",
      "```\n",
      "docker exec -it <spark_master_container_id> bash\n",
      "cat logs/spark-master.out\n",
      "```\n",
      "\n",
      "By checking the logs, you can diagnose issues related to the Spark master connection.\n",
      "\n",
      "Additionally, ensure that your PySpark version matches your intended setup; a mismatch can lead to connection errors. To check the PySpark version on your local machine, use:\n",
      "\n",
      "```\n",
      "pyspark --version\n",
      "spark-submit --version\n",
      "```\n",
      "\n",
      "If necessary, downgrade your PySpark or update the version in your `build.sh` file.\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ada1dbd2-8ada-4dcc-b5f6-0b66bcacc893",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"To use Kafka with Spark, you can refer to the materials in Module 6: Streaming with Kafka. If you're running a Python application with Spark and Kafka, you may need to troubleshoot errors such as connection issues with the Spark master or Kafka broker. \\n\\nFor instance, if you get an error indicating that no brokers are available, ensure that your Kafka broker Docker container is running by using the command `docker ps`. If it is not running, you can start it by navigating to the docker compose YAML file folder and running `docker compose up -d` to start all instances.\\n\\nIf you encounter errors with the Spark master connection, you can start a new terminal, find the Spark master container ID with `docker ps`, and check the logs for errors using:\\n\\n```bash\\ndocker exec -it <spark_master_container_id> bash\\ncat logs/spark-master.out\\n```\\n\\nThis will provide you with insights into any connection issues. Additionally, ensure that your PySpark version matches the expected version to avoid compatibility issues.\""
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea06c36-3065-4d8c-81f5-b7837e08707c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
