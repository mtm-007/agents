{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3fc7dd25-c0c5-4529-a500-db3e87975b50",
   "metadata": {},
   "source": [
    "### RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fef82a35-8d5c-44fd-8be2-fbd2660bfb21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "import json\n",
    "\n",
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fb95e953-63a2-421f-b4a6-9a9a946da18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "docs_url = 'https://github.com/alexeygrigorev/llm-rag-workshop/raw/main/notebooks/documents.json'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()\n",
    "\n",
    "documents = []\n",
    "\n",
    "for course in documents_raw:\n",
    "    course_name = course['course']\n",
    "\n",
    "    for doc in course['documents']:\n",
    "        doc['course'] = course_name\n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8e7a2a8-40da-4700-b535-86e6149a0d99",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.append.AppendableIndex at 0x712b10744a40>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from minsearch import AppendableIndex\n",
    "\n",
    "index = AppendableIndex(\n",
    "    text_fields = [\"question\", \"text\", \"section\"],\n",
    "    keyword_fields = [\"course\"]\n",
    ")\n",
    "\n",
    "index.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3b25824b-16a5-42fd-a509-c902cafc898c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text': 'While following tutorial 13.2 , when running ./spark-submit.sh streaming.py, encountered the following error:\\n…\\n24/03/11 09:48:36 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077...\\n24/03/11 09:48:36 INFO TransportClientFactory: Successfully created connection to localhost/127.0.0.1:7077 after 10 ms (0 ms spent in bootstraps)\\n24/03/11 09:48:54 WARN GarbageCollectionMetrics: To enable non-built-in garbage collector(s) List(G1 Concurrent GC), users should configure it(them) to spark.eventLog.gcMetrics.youngGenerationGarbageCollectors or spark.eventLog.gcMetrics.oldGenerationGarbageCollectors\\n24/03/11 09:48:56 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077…\\n24/03/11 09:49:16 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://localhost:7077...\\n24/03/11 09:49:36 WARN StandaloneSchedulerBackend: Application ID is not initialized yet.\\n24/03/11 09:49:36 ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.\\n…\\npy4j.protocol.Py4JJavaError: An error occurred while calling None.org.apache.spark.sql.SparkSession.\\n: java.lang.IllegalStateException: Cannot call methods on a stopped SparkContext.\\n…\\nSolution:\\nDowngrade your local PySpark to 3.3.1 (same as Dockerfile)\\nThe reason for the failed connection in my case was the mismatch of PySpark versions. You can see that from the logs of spark-master in the docker container.\\nSolution 2:\\nCheck what Spark version your local machine has\\npyspark –version\\nspark-submit –version\\nAdd your version to SPARK_VERSION in build.sh',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Python Kafka: ./spark-submit.sh streaming.py - ERROR StandaloneSchedulerBackend: Application has been killed. Reason: All masters are unresponsive! Giving up.',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'Start a new terminal\\nRun: docker ps\\nCopy the CONTAINER ID of the spark-master container\\nRun: docker exec -it <spark_master_container_id> bash\\nRun: cat logs/spark-master.out\\nCheck for the log when the error happened\\nGoogle the error message from there',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Python Kafka: ./spark-submit.sh streaming.py - How to check why Spark master connection fails',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'If you have this error, it most likely that your kafka broker docker container is not working.\\nUse docker ps to confirm\\nThen in the docker compose yaml file folder, run docker compose up -d to start all the instances.',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'kafka.errors.NoBrokersAvailable: NoBrokersAvailable',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'tip:As the videos have low audio so I downloaded them and used VLC media player with putting the audio to the max 200% of original audio and the audio became quite good or try to use auto caption generated on Youtube directly.\\nKafka Python Videos - Rides.csv\\nThere is no clear explanation of the rides.csv data that the producer.py python programs use. You can find that here https://raw.githubusercontent.com/DataTalksClub/data-engineering-zoomcamp/2bd33e89906181e424f7b12a299b70b19b7cfcd5/week_6_stream_processing/python/resources/rides.csv.',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Kafka- python videos have low audio and hard to follow up',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'For example, when running JsonConsumer.java, got:\\nConsuming form kafka started\\nRESULTS:::0\\nRESULTS:::0\\nRESULTS:::0\\nOr when running JsonProducer.java, got:\\nException in thread \"main\" java.util.concurrent.ExecutionException: org.apache.kafka.common.errors.SaslAuthenticationException: Authentication failed\\nSolution:\\nMake sure in the scripts in src/main/java/org/example/ that you are running (e.g. JsonConsumer.java, JsonProducer.java), the StreamsConfig.BOOTSTRAP_SERVERS_CONFIG is the correct server url (e.g. europe-west3 from example vs europe-west2)\\nMake sure cluster key and secrets are updated in src/main/java/org/example/Secrets.java (KAFKA_CLUSTER_KEY and KAFKA_CLUSTER_SECRET)',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Java Kafka: When running the producer/consumer/etc java scripts, no results retrieved or no message sent',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'confluent-kafka: `pip install confluent-kafka` or `conda install conda-forge::python-confluent-kafka`\\nfastavro: pip install fastavro\\nAbhirup Ghosh\\nCan install Faust Library for Module 6 Python Version due to dependency conflicts?\\nThe Faust repository and library is no longer maintained - https://github.com/robinhood/faust\\nIf you do not know Java, you now have the option to follow the Python Videos 6.13 & 6.14 here https://www.youtube.com/watch?v=BgAlVknDFlQ&list=PL3MmuxUbc_hJed7dXYoJw8DoCuVHhGEQb&index=80  and follow the RedPanda Python version here https://github.com/DataTalksClub/data-engineering-zoomcamp/tree/main/06-streaming/python/redpanda_example - NOTE: I highly recommend watching the Java videos to understand the concept of streaming but you can skip the coding parts - all will become clear when you get to the Python videos and RedPanda files.',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Python Kafka: Installing dependencies for python3 06-streaming/python/avro_example/producer.py',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'In my set up, all of the dependencies listed in gradle.build were not installed in <project_name>-1.0-SNAPSHOT.jar.\\nSolution:\\nIn build.gradle file, I added the following at the end:\\nshadowJar {\\narchiveBaseName = \"java-kafka-rides\"\\narchiveClassifier = \\'\\'\\n}\\nAnd then in the command line ran ‘gradle shadowjar’, and run the script from java-kafka-rides-1.0-SNAPSHOT.jar created by the shadowjar',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Java Kafka: <project_name>-1.0-SNAPSHOT.jar errors: package xxx does not exist even after gradle build',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'Run this command in terminal in the same directory (/docker/spark):\\nchmod +x build.sh',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'Python Kafka: ./build.sh: Permission denied Error',\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'If you get this error, know that you have not built your sparks and juypter images. This images aren’t readily available on dockerHub.\\nIn the spark folder, run ./build.sh from a bash cli to to build all images before running docker compose',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': \"How to fix docker compose error: Error response from daemon: pull access denied for spark-3.3.1, repository does not exist or may require 'docker login': denied: requested access to the resource is denied\",\n",
       "  'course': 'data-engineering-zoomcamp'},\n",
       " {'text': 'You can check the version of your local spark using spark-submit --version. In the build.sh file of the Python folder, make sure that SPARK_VERSION matches your local version. Similarly, make sure the pyspark you pip installed also matches this version.',\n",
       "  'section': 'Module 6: streaming with kafka',\n",
       "  'question': 'How do I check compatibility of local and container Spark versions?',\n",
       "  'course': 'data-engineering-zoomcamp'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index.search(\"how to use kafka with spark\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "fd2d7d5c-f703-48e7-8083-9fd9250f849f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    boost = {'question': 3.0, 'section': 0.5}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={'course': 'data-engineering-zoomcamp'},\n",
    "        boost_dict=boost,\n",
    "        num_results=5,\n",
    "        output_ids=True\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33cd3a60-1729-4dcc-beb5-fd311cdf0d98",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"how to use kafka with spark\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1a8fc61d-d08b-456f-ad1d-6d6a42b1642c",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "<QUESTION>\n",
    "{question}\n",
    "</QUESTION>\n",
    "\n",
    "<CONTEXT>\n",
    "{context}\n",
    "</CONTEXT>\n",
    "\"\"\".strip()\n",
    "\n",
    "def build_prompt(query, search_results):\n",
    "    context = \"\"\n",
    "\n",
    "    for doc in search_results:\n",
    "        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\"\n",
    "    \n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e9ee9f5c-d9e4-4638-9eca-5c9207a033a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = search(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "107bfa23-cda8-4ef3-bc0d-07a2efee876a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = build_prompt(question, search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fdd9be5a-2d6b-431f-a282-bb6485a93f14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
      "Use only the facts from the CONTEXT when answering the QUESTION.\n",
      "\n",
      "<QUESTION>\n",
      "how do I do well in module 1?\n",
      "</QUESTION>\n",
      "\n",
      "<CONTEXT>\n",
      "section: Module 5: pyspark\n",
      "question: Module Not Found Error in Jupyter Notebook .\n",
      "answer: Even after installing pyspark correctly on linux machine (VM ) as per course instructions, faced a module not found error in jupyter notebook .\n",
      "The solution which worked for me(use following in jupyter notebook) :\n",
      "!pip install findspark\n",
      "import findspark\n",
      "findspark.init()\n",
      "Thereafter , import pyspark and create spark contex<<t as usual\n",
      "None of the solutions above worked for me till I ran !pip3 install pyspark instead !pip install pyspark.\n",
      "Filter based on conditions based on multiple columns\n",
      "from pyspark.sql.functions import col\n",
      "new_final.filter((new_final.a_zone==\"Murray Hill\") & (new_final.b_zone==\"Midwood\")).show()\n",
      "Krishna Anand\n",
      "\n",
      "section: Module 5: pyspark\n",
      "question: Py4JJavaError - ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`\n",
      "answer: You need to look for the Py4J file and note the version of the filename. Once you know the version, you can update the export command accordingly, this is how you check yours:\n",
      "` ls ${SPARK_HOME}/python/lib/ ` and then you add it in the export command, mine was:\n",
      "export PYTHONPATH=”${SPARK_HOME}/python/lib/Py4J-0.10.9.5-src.zip:${PYTHONPATH}”\n",
      "Make sure that the version under `${SPARK_HOME}/python/lib/` matches the filename of py4j or you will encounter `ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`.\n",
      "For instance, if the file under `${SPARK_HOME}/python/lib/` was `py4j-0.10.9.3-src.zip`.\n",
      "Then the export PYTHONPATH statement above should be changed to `export PYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH\"` appropriately.\n",
      "Additionally, you can check for the version of ‘py4j’ of the spark you’re using from here and update as mentioned above.\n",
      "~ Abhijit Chakraborty: Sometimes, even with adding the correct version of py4j might not solve the problem. Simply run pip install py4j and problem should be resolved.\n",
      "\n",
      "section: Module 4: analytics engineering with dbt\n",
      "question: DBT - Error: No module named 'pytz' while setting up dbt with docker\n",
      "answer: Following dbt with BigQuery on Docker readme.md, after `docker-compose build` and `docker-compose run dbt-bq-dtc init`, encountered error `ModuleNotFoundError: No module named 'pytz'`\n",
      "Solution:\n",
      "Add `RUN python -m pip install --no-cache pytz` in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Python - SQLALchemy - TypeError 'module' object is not callable\n",
      "answer: create_engine('postgresql://root:root@localhost:5432/ny_taxi')  I get the error \"TypeError: 'module' object is not callable\"\n",
      "Solution:\n",
      "conn_string = \"postgresql+psycopg://root:root@localhost:5432/ny_taxi\"\n",
      "engine = create_engine(conn_string)\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Python - SQLAlchemy - ModuleNotFoundError: No module named 'psycopg2'.\n",
      "answer: Error raised during the jupyter notebook’s cell execution:\n",
      "engine = create_engine('postgresql://root:root@localhost:5432/ny_taxi').\n",
      "Solution: Need to install Python module “psycopg2”. Can be installed by Conda or pip.\n",
      "\n",
      "\n",
      "</CONTEXT>\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2c1bc124-3bb6-4ba2-a06f-7ce208cb4aca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "client = OpenAI()\n",
    "\n",
    "def llm(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4o-mini',\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "def rag(query):\n",
    "    search_results = search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    answer = llm(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "90deb85e-d269-47dd-93b7-a4f51c83f990",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c8a0d682-3437-4dd5-8ba9-e6fc99b70e4d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To do well in Module 1, focus on the following points:\n",
      "\n",
      "1. **Ensure All Modules Are Installed**: Make sure you have all the required modules installed. Specifically, if you encounter a `ModuleNotFoundError` related to `psycopg2`, you will need to install it using either Conda or pip.\n",
      "\n",
      "2. **Use the Correct Connection String**: When working with SQLAlchemy, be aware of the connection string format. Instead of using `create_engine('postgresql://root:root@localhost:5432/ny_taxi')`, use `conn_string = \"postgresql+psycopg://root:root@localhost:5432/ny_taxi\"` and then create the engine.\n",
      "\n",
      "By following these steps, you can effectively navigate the challenges presented in Module 1.\n"
     ]
    }
   ],
   "source": [
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ada1dbd2-8ada-4dcc-b5f6-0b66bcacc893",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'To use Kafka with Spark, you can follow these general steps:\\n\\n1. **Set Up Kafka and Spark**: Ensure that you have Kafka and Spark running, typically in Docker containers. You can confirm their status using `docker ps` to see if the necessary containers are operational.\\n\\n2. **Check Kafka Broker**: If you encounter the error `kafka.errors.NoBrokersAvailable`, it indicates that your Kafka broker might not be working. You can start the broker by navigating to the folder containing your `docker-compose.yaml` file and running `docker compose up -d`.\\n\\n3. **Submit Spark Job**: Use the command `./spark-submit.sh streaming.py` to run your Spark job. Ensure that the versions of PySpark on your local machine and any Docker images being used are compatible to avoid connection issues (e.g., if there are errors about the Spark master being unresponsive, it may be caused by version mismatches).\\n\\n4. **Log Checking**: If you encounter issues with Spark master connection, open a new terminal and check the logs of the Spark master container for any error messages, which can be done with the commands:\\n   - `docker exec -it <spark_master_container_id> bash`\\n   - `cat logs/spark-master.out`\\n\\n5. **Monitor Errors**: Look for specific error messages in the logs and search for solutions, as these can guide you on what adjustments or fixes might be needed next.\\n\\nBy following these steps, you should be able to establish a functioning connection between Kafka and Spark in your streaming application.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rag(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e88740-b4f1-45e1-87da-f3308733796d",
   "metadata": {},
   "source": [
    "#### 'Agentic' RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cc9a26bf-f640-42ce-a773-8c3273fac948",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You are a course teaching assistant.\n",
    "\n",
    "You are given a Question from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
    "At the beginning the context is EMPTY.\n",
    "\n",
    "<QUESTION>\n",
    "{question}\n",
    "</QUESTION>\n",
    "\n",
    "<CONTEXT>\n",
    "{context}\n",
    "</CONTEXT>\n",
    "\n",
    "If CONTEXT is EMPTY, you can use your FAQ database.\n",
    "In this case, use the following output template:\n",
    "\n",
    "{{\n",
    "\"action\": \"SEARCH\",\n",
    "\"reasoning\": \"<add your reasoning here>\"\n",
    "}}\n",
    "\n",
    "If you can answer the QUESTION using CONTEXT, use this template:\n",
    "\n",
    "{{\n",
    "\"action\": \"ANSWER\",\n",
    "\"answer\": \"<your answer>\",\n",
    "\"source\" : \"CONTEXT\"\n",
    "}}\n",
    "\n",
    "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
    "\n",
    "{{\n",
    "\"action\" : \"ANSWER\",\n",
    "\"answer\" : \"<your answer>\",\n",
    "\"source\" : \"OWN_KNOWLEDGE\"\n",
    "}}\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4a7bd20e-0f69-4260-9c00-7b1ad9551622",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"Can i still join the course?\"\n",
    "context = \"EMPTY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1903578f-e1f7-4e95-87c2-86d0fbe79950",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a course teaching assistant.\n",
      "\n",
      "You are given a Question from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "At the beginning the context is EMPTY.\n",
      "\n",
      "<QUESTION>\n",
      "Can i still join the course?\n",
      "</QUESTION>\n",
      "\n",
      "<CONTEXT>\n",
      "EMPTY\n",
      "</CONTEXT>\n",
      "\n",
      "If CONTEXT is EMPTY, you can use your FAQ database.\n",
      "In this case, use the following output template:\n",
      "\n",
      "{\n",
      "\"action\": \"SEARCH\",\n",
      "\"reasoning\": \"<add your reasoning here>\"\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\" : \"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\" : \"ANSWER\",\n",
      "\"answer\" : \"<your answer>\",\n",
      "\"source\" : \"OWN_KNOWLEDGE\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "prompt = prompt_template.format(question=question, context=context)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d378fa25-e1a6-4ec3-85c2-acf7fddc04e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_json = llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ec652cb6-1566-4b1b-a7de-1fd991f1e495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8dbf2409-3072-45f2-a166-b6de5f76d0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = json.loads(answer_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9ec3c12c-84fb-47ff-923d-9b424e8fb5c1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SEARCH'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer[\"action\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a44cc728-d30a-4414-acc2-1e2c0792c36d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_context(search_results):\n",
    "    context = \"\"\n",
    "\n",
    "    for doc in search_results:\n",
    "        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\"\n",
    "    \n",
    "    return context.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4d4d4898-156d-44ec-96b3-ec7ba80db9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = search(question)\n",
    "context = build_context(search_results)\n",
    "prompt = prompt_template.format(question=question, context=context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e043985e-8559-46d9-9773-ce52af3dd340",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_json = llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a809f869-c075-446a-8812-2731e28a7ca3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"Yes, you can still join the course after the start date, even if you haven't officially registered. You are eligible to submit homework assignments, but keep in mind that there will be deadlines for the final projects that you should not overlook.\",\n",
      "\"source\" : \"CONTEXT\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(answer_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1033a310-c67f-4597-93b8-613c854bb52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agentic_rag_v1(question):\n",
    "    context = \"EMPTY\"\n",
    "    prompt = prompt_template.format(question=question, context=context)\n",
    "    answer_json = llm(prompt)\n",
    "    answer = json.loads(answer_json)\n",
    "    print(answer)\n",
    "\n",
    "    if answer[\"action\"]==\"SEARCH\":\n",
    "        print(\"need to perform search...\")\n",
    "        search_results = search(question)\n",
    "        context = build_context(search_results)\n",
    "\n",
    "        prompt = prompt_template.format(question=question, context=context)\n",
    "        answer_json = llm(prompt)\n",
    "        answer = json.loads(answer_json)\n",
    "        print(answer)\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7ab1e3a6-af05-40ac-9829-10dbde2f81e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'action': 'ANSWER', 'answer': \"To join the course, you typically need to enroll through the course website or platform where it is hosted. Look for a button or link that says 'Enroll Now' or 'Join the Course'. You may also need to create an account if you haven't already. If there are prerequisites or specific requirements, make sure to complete those as well.\", 'source': 'OWN_KNOWLEDGE'}\n",
      "CPU times: user 3.46 ms, sys: 953 μs, total: 4.42 ms\n",
      "Wall time: 2.27 s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'action': 'ANSWER',\n",
       " 'answer': \"To join the course, you typically need to enroll through the course website or platform where it is hosted. Look for a button or link that says 'Enroll Now' or 'Join the Course'. You may also need to create an account if you haven't already. If there are prerequisites or specific requirements, make sure to complete those as well.\",\n",
       " 'source': 'OWN_KNOWLEDGE'}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%time\n",
    "agentic_rag_v1(\"how do i join the course?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f56dd4-6975-4d46-b13e-e7338e3d21eb",
   "metadata": {},
   "source": [
    "#### Agentic search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c500652d-62e4-4f8f-81ac-259043dbc3b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dedub(seq):\n",
    "    \"\"\"\n",
    "    deduplicates by skipping the repeating element('_id')\n",
    "    \"\"\"\n",
    "    seen = set()\n",
    "    result = []\n",
    "    for el in seq:\n",
    "        _id = el['_id']\n",
    "        if _id in seen:\n",
    "            continue\n",
    "        seen.add(_id)\n",
    "        result.append(el)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2a8b0793-d42b-4e0d-9c4e-843aedc00558",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You are a course teaching assistant.\n",
    "\n",
    "You are given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
    "\n",
    "The CONTEXT is build with the documents from our FAQ database.\n",
    "SEARCH_QUERIES contains the queries that were used to retrieve the documents\n",
    "from FAQ to and add them to the context.\n",
    "PREVIOUS_ACTIONS contains the actions you already performed.\n",
    "\n",
    "At the beginning the CONTEXT is empty.\n",
    "\n",
    "You can perform the following actions:\n",
    "\n",
    "- Search in the FAQ database to get more data for the CONTEXT\n",
    "- Answer the question using the CONTEXT\n",
    "- Answer the question using your own knowledge\n",
    "\n",
    "For the SEARCH action, build search requests based on the CONTEXT and the QUESTION.\n",
    "Carefully analyze the CONTEXT and generate the requests to deeply explore the topic.\n",
    "\n",
    "Don't use search queries used at the previous iterations.\n",
    "\n",
    "Don't repeat previously performed actions.\n",
    "\n",
    "Don't perform more than {max_iterations} iterations for a given student question.\n",
    "The current iteration number: {iteration_number}. If we exceed the allowed number\n",
    "of iterations, give the best possible answer with the provided information.\n",
    "\n",
    "Output templates:\n",
    "\n",
    "If you want to perform search, use this template:\n",
    "\n",
    "{{\n",
    "\"action\" : \"SEARCH\",\n",
    "\"reasoning\" : \"<add your reasoning here>\",\n",
    "\"keywords\" : [\"search query 1\", \"search query 2\", ...]\n",
    "}}\n",
    "\n",
    "If you can answer the QUESTION using CONTEXT, use this template:\n",
    "\n",
    "{{\n",
    "\"action\" : \"ANSWER_CONTEXT\",\n",
    "\"answer\" : \"<your answer>\",\n",
    "\"source\" :\"CONTEXT\"\n",
    "}}\n",
    "\n",
    "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
    "\n",
    "{{\n",
    "\"action\" : \"ANSWER\",\n",
    "\"answer\" : \"<your answer>\",\n",
    "\"source\" : \"OWN_KNOWLEDGE\"\n",
    "}}\n",
    "\n",
    "<QUESTION>\n",
    "{question}\n",
    "</QUESTION>\n",
    "\n",
    "<SEARCH_QUERIES>\n",
    "{search_queries}\n",
    "</SEARCH_QUERIES>\n",
    "\n",
    "<CONTEXT>\n",
    "{context}\n",
    "</CONTEXT>\n",
    "\n",
    "<PREVIOUS_ACTIONS>\n",
    "{previous_actions}\n",
    "</PREVIOUS_ACTIONS>\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4a52001d-e882-42ad-8c8a-1bed7e435b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"how do I do well on module 1\"\n",
    "\n",
    "max_iterations = 3\n",
    "iteration_number = 0\n",
    "search_queries = []\n",
    "search_results = []\n",
    "previous_actions = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "d55f6e25-5d48-453e-a2ad-8daed7a07c90",
   "metadata": {},
   "outputs": [],
   "source": [
    "context = build_context(search_results)\n",
    "\n",
    "prompt = prompt_template.format(\n",
    "    question = question,\n",
    "    context = context,\n",
    "    search_queries = \"\\n\".join(search_queries),\n",
    "    previous_actions = \"\\n\".join([json.dumps(a) for a in previous_actions]),\n",
    "    max_iterations=max_iterations,\n",
    "    iteration_number=iteration_number,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "324e17b1-72d8-4d65-9424-9a6c80f4acd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_json = llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "7542a4d2-2702-4a48-b494-2a38997a1490",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = json.loads(answer_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c6d7af6f-8e7a-489c-a5bd-54d3d86669c0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'action': 'SEARCH',\n",
       " 'reasoning': 'I need to gather more specific insights and advice on succeeding in Module 1, especially since the previous search did not yield any relevant results related to strategies for success in the module.',\n",
       " 'keywords': ['succeeding in Module 1',\n",
       "  'Module 1 study tips',\n",
       "  'best practices for Module 1']}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "48b0257d-e366-428c-98f9-e67f3abea160",
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_actions.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "4c012b71-037b-4663-8788-39546624eb79",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'action': 'SEARCH',\n",
       "  'reasoning': 'I want to find specific tips and resources related to success strategies for module 1 to better inform the student.',\n",
       "  'keywords': ['how to succeed in module 1',\n",
       "   'tips for module 1',\n",
       "   'module 1 best practices']},\n",
       " {'action': 'SEARCH',\n",
       "  'reasoning': 'I need to gather more specific insights and advice on succeeding in Module 1, especially since the previous search did not yield any relevant results related to strategies for success in the module.',\n",
       "  'keywords': ['succeeding in Module 1',\n",
       "   'Module 1 study tips',\n",
       "   'best practices for Module 1']}]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previous_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "95ff6a22-10bb-46e8-8ec4-e50021eb04e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = answer['keywords']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "2a52d8a7-7e4f-4169-af75-984f6341ec34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['succeeding in Module 1', 'Module 1 study tips', 'best practices for Module 1']\n"
     ]
    }
   ],
   "source": [
    "print(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1c4ddbe5-e3c0-4a45-8c52-3f53472cff90",
   "metadata": {},
   "outputs": [],
   "source": [
    "for kw in keywords:\n",
    "    search_queries.append(kw)\n",
    "    sr = search(kw)\n",
    "    search_results.extend(sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "437f178a-21fd-4974-875b-eeebe47d1f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_results = dedub(search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "062d4761-c4ae-4659-8622-537b4751a9c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(search_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc4d319-c0c7-4688-9571-1a52ce84203d",
   "metadata": {},
   "source": [
    "##### Running iterations manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c032302d-da5c-4b0f-b2e5-2aa8f5f4faac",
   "metadata": {},
   "outputs": [],
   "source": [
    "iteration_number = 3\n",
    "\n",
    "context = build_context(search_results)\n",
    "\n",
    "prompt = prompt_template.format(\n",
    "    question = question,\n",
    "    context = context,\n",
    "    search_queries = \"\\n\".join(search_queries),\n",
    "    previous_actions = \"\\n\".join([json.dumps(a) for a in previous_actions]),\n",
    "    max_iterations=max_iterations,\n",
    "    iteration_number=iteration_number,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "f5da2459-6d51-4518-9582-094a215e3598",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are a course teaching assistant.\n",
      "\n",
      "You are given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "\n",
      "The CONTEXT is build with the documents from our FAQ database.\n",
      "SEARCH_QUERIES contains the queries that were used to retrieve the documents\n",
      "from FAQ to and add them to the context.\n",
      "PREVIOUS_ACTIONS contains the actions you already performed.\n",
      "\n",
      "At the beginning the CONTEXT is empty.\n",
      "\n",
      "You can perform the following actions:\n",
      "\n",
      "- Search in the FAQ database to get more data for the CONTEXT\n",
      "- Answer the question using the CONTEXT\n",
      "- Answer the question using your own knowledge\n",
      "\n",
      "For the SEARCH action, build search requests based on the CONTEXT and the QUESTION.\n",
      "Carefully analyze the CONTEXT and generate the requests to deeply explore the topic.\n",
      "\n",
      "Don't use search queries used at the previous iterations.\n",
      "\n",
      "Don't repeat previously performed actions.\n",
      "\n",
      "Don't perform more than 3 iterations for a given student question.\n",
      "The current iteration number: 3. If we exceed the allowed number\n",
      "of iterations, give the best possible answer with the provided information.\n",
      "\n",
      "Output templates:\n",
      "\n",
      "If you want to perform search, use this template:\n",
      "\n",
      "{\n",
      "\"action\" : \"SEARCH\",\n",
      "\"reasoning\" : \"<add your reasoning here>\",\n",
      "\"keywords\" : [\"search query 1\", \"search query 2\", ...]\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\" : \"ANSWER_CONTEXT\",\n",
      "\"answer\" : \"<your answer>\",\n",
      "\"source\" :\"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\" : \"ANSWER\",\n",
      "\"answer\" : \"<your answer>\",\n",
      "\"source\" : \"OWN_KNOWLEDGE\"\n",
      "}\n",
      "\n",
      "<QUESTION>\n",
      "how do I do well on module 1\n",
      "</QUESTION>\n",
      "\n",
      "<SEARCH_QUERIES>\n",
      "how to succeed in module 1\n",
      "tips for module 1\n",
      "module 1 best practices\n",
      "succeeding in Module 1\n",
      "Module 1 study tips\n",
      "best practices for Module 1\n",
      "</SEARCH_QUERIES>\n",
      "\n",
      "<CONTEXT>\n",
      "section: Module 5: pyspark\n",
      "question: Py4JJavaError - ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`\n",
      "answer: You need to look for the Py4J file and note the version of the filename. Once you know the version, you can update the export command accordingly, this is how you check yours:\n",
      "` ls ${SPARK_HOME}/python/lib/ ` and then you add it in the export command, mine was:\n",
      "export PYTHONPATH=”${SPARK_HOME}/python/lib/Py4J-0.10.9.5-src.zip:${PYTHONPATH}”\n",
      "Make sure that the version under `${SPARK_HOME}/python/lib/` matches the filename of py4j or you will encounter `ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`.\n",
      "For instance, if the file under `${SPARK_HOME}/python/lib/` was `py4j-0.10.9.3-src.zip`.\n",
      "Then the export PYTHONPATH statement above should be changed to `export PYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH\"` appropriately.\n",
      "Additionally, you can check for the version of ‘py4j’ of the spark you’re using from here and update as mentioned above.\n",
      "~ Abhijit Chakraborty: Sometimes, even with adding the correct version of py4j might not solve the problem. Simply run pip install py4j and problem should be resolved.\n",
      "\n",
      "section: Module 4: analytics engineering with dbt\n",
      "question: DBT - Error: No module named 'pytz' while setting up dbt with docker\n",
      "answer: Following dbt with BigQuery on Docker readme.md, after `docker-compose build` and `docker-compose run dbt-bq-dtc init`, encountered error `ModuleNotFoundError: No module named 'pytz'`\n",
      "Solution:\n",
      "Add `RUN python -m pip install --no-cache pytz` in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`\n",
      "\n",
      "section: Module 5: pyspark\n",
      "question: Module Not Found Error in Jupyter Notebook .\n",
      "answer: Even after installing pyspark correctly on linux machine (VM ) as per course instructions, faced a module not found error in jupyter notebook .\n",
      "The solution which worked for me(use following in jupyter notebook) :\n",
      "!pip install findspark\n",
      "import findspark\n",
      "findspark.init()\n",
      "Thereafter , import pyspark and create spark contex<<t as usual\n",
      "None of the solutions above worked for me till I ran !pip3 install pyspark instead !pip install pyspark.\n",
      "Filter based on conditions based on multiple columns\n",
      "from pyspark.sql.functions import col\n",
      "new_final.filter((new_final.a_zone==\"Murray Hill\") & (new_final.b_zone==\"Midwood\")).show()\n",
      "Krishna Anand\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Postgres - ModuleNotFoundError: No module named 'psycopg2'\n",
      "answer: Issue:\n",
      "e…\n",
      "Solution:\n",
      "pip install psycopg2-binary\n",
      "If you already have it, you might need to update it:\n",
      "pip install psycopg2-binary --upgrade\n",
      "Other methods, if the above fails:\n",
      "if you are getting the “ ModuleNotFoundError: No module named 'psycopg2' “ error even after the above installation, then try updating conda using the command conda update -n base -c defaults conda. Or if you are using pip, then try updating it before installing the psycopg packages i.e\n",
      "First uninstall the psycopg package\n",
      "Then update conda or pip\n",
      "Then install psycopg again using pip.\n",
      "if you are still facing error with r pcycopg2 and showing pg_config not found then you will have to install postgresql. in MAC it is brew install postgresql\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Python - SQLALchemy - TypeError 'module' object is not callable\n",
      "answer: create_engine('postgresql://root:root@localhost:5432/ny_taxi')  I get the error \"TypeError: 'module' object is not callable\"\n",
      "Solution:\n",
      "conn_string = \"postgresql+psycopg://root:root@localhost:5432/ny_taxi\"\n",
      "engine = create_engine(conn_string)\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Docker - cs to store all code in your default Linux distro to get the best out of file system performance (since Docker runs on WSL2 backend by default for Windows 10 Home / Windows 11 Home users).\n",
      "answer: More info in the Docker Docs on Best Practises\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Python - SQLAlchemy - ModuleNotFoundError: No module named 'psycopg2'.\n",
      "answer: Error raised during the jupyter notebook’s cell execution:\n",
      "engine = create_engine('postgresql://root:root@localhost:5432/ny_taxi').\n",
      "Solution: Need to install Python module “psycopg2”. Can be installed by Conda or pip.\n",
      "</CONTEXT>\n",
      "\n",
      "<PREVIOUS_ACTIONS>\n",
      "{\"action\": \"SEARCH\", \"reasoning\": \"I want to find specific tips and resources related to success strategies for module 1 to better inform the student.\", \"keywords\": [\"how to succeed in module 1\", \"tips for module 1\", \"module 1 best practices\"]}\n",
      "{\"action\": \"SEARCH\", \"reasoning\": \"I need to gather more specific insights and advice on succeeding in Module 1, especially since the previous search did not yield any relevant results related to strategies for success in the module.\", \"keywords\": [\"succeeding in Module 1\", \"Module 1 study tips\", \"best practices for Module 1\"]}\n",
      "</PREVIOUS_ACTIONS>\n"
     ]
    }
   ],
   "source": [
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "68f347d2-2e0a-4416-b368-3fb4cda40c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_json = llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "4ddf7ced-88d1-4890-b5a6-5869d566d0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"To do well in Module 1, which focuses on Docker and Terraform, consider the following strategies:\\n\\n1. **Understand the Basics:** Make sure you have a solid understanding of Docker concepts such as containers, images, and orchestration. Familiarize yourself with Terraform's infrastructure as code approach and its basic commands.\\n\\n2. **Hands-On Practice:** Engage in practical exercises by setting up your own Docker containers and using Terraform to manage infrastructure. The more you practice, the better you'll grasp these tools.\\n\\n3. **Utilize Resources:** Leverage the course materials, documentation, and online tutorials to reinforce your learning. Community forums can also provide assistance if you encounter challenges.\\n\\n4. **Manage Dependencies:** Be aware of common issues such as 'ModuleNotFoundError' for libraries like Psycopg2 or other dependencies. Make sure to install and update them as needed.\\n\\n5. **Collaborate:** If possible, work with peers to share insights and tackle challenges together. Collaboration can deepen your understanding of difficult concepts.\\n\\nBy following these tips, you should be able to navigate Module 1 successfully and build a strong foundation for future modules.\",\n",
      "\"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(answer_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "287ebd52-aed7-4fc8-b444-e2d64f3616b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = json.loads(answer_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "71818a41-5295-46eb-8e8b-539911729bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To do well in Module 1, which focuses on Docker and Terraform, consider the following strategies:\n",
      "\n",
      "1. **Understand the Basics:** Make sure you have a solid understanding of Docker concepts such as containers, images, and orchestration. Familiarize yourself with Terraform's infrastructure as code approach and its basic commands.\n",
      "\n",
      "2. **Hands-On Practice:** Engage in practical exercises by setting up your own Docker containers and using Terraform to manage infrastructure. The more you practice, the better you'll grasp these tools.\n",
      "\n",
      "3. **Utilize Resources:** Leverage the course materials, documentation, and online tutorials to reinforce your learning. Community forums can also provide assistance if you encounter challenges.\n",
      "\n",
      "4. **Manage Dependencies:** Be aware of common issues such as 'ModuleNotFoundError' for libraries like Psycopg2 or other dependencies. Make sure to install and update them as needed.\n",
      "\n",
      "5. **Collaborate:** If possible, work with peers to share insights and tackle challenges together. Collaboration can deepen your understanding of difficult concepts.\n",
      "\n",
      "By following these tips, you should be able to navigate Module 1 successfully and build a strong foundation for future modules.\n"
     ]
    }
   ],
   "source": [
    "print(answer['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94ea4d11-c744-4949-b55f-5c2eab79ac9e",
   "metadata": {},
   "source": [
    "#### Automating in a loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "571e850b-daf5-4ad8-84bf-cb33af51b321",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION #0...\n",
      "You are a course teaching assistant.\n",
      "\n",
      "You are given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "\n",
      "The CONTEXT is build with the documents from our FAQ database.\n",
      "SEARCH_QUERIES contains the queries that were used to retrieve the documents\n",
      "from FAQ to and add them to the context.\n",
      "PREVIOUS_ACTIONS contains the actions you already performed.\n",
      "\n",
      "At the beginning the CONTEXT is empty.\n",
      "\n",
      "You can perform the following actions:\n",
      "\n",
      "- Search in the FAQ database to get more data for the CONTEXT\n",
      "- Answer the question using the CONTEXT\n",
      "- Answer the question using your own knowledge\n",
      "\n",
      "For the SEARCH action, build search requests based on the CONTEXT and the QUESTION.\n",
      "Carefully analyze the CONTEXT and generate the requests to deeply explore the topic.\n",
      "\n",
      "Don't use search queries used at the previous iterations.\n",
      "\n",
      "Don't repeat previously performed actions.\n",
      "\n",
      "Don't perform more than 3 iterations for a given student question.\n",
      "The current iteration number: 0. If we exceed the allowed number\n",
      "of iterations, give the best possible answer with the provided information.\n",
      "\n",
      "Output templates:\n",
      "\n",
      "If you want to perform search, use this template:\n",
      "\n",
      "{\n",
      "\"action\" : \"SEARCH\",\n",
      "\"reasoning\" : \"<add your reasoning here>\",\n",
      "\"keywords\" : [\"search query 1\", \"search query 2\", ...]\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\" : \"ANSWER_CONTEXT\",\n",
      "\"answer\" : \"<your answer>\",\n",
      "\"source\" :\"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\" : \"ANSWER\",\n",
      "\"answer\" : \"<your answer>\",\n",
      "\"source\" : \"OWN_KNOWLEDGE\"\n",
      "}\n",
      "\n",
      "<QUESTION>\n",
      "what do I need to do to be succesful at module 1?\n",
      "</QUESTION>\n",
      "\n",
      "<SEARCH_QUERIES>\n",
      "\n",
      "</SEARCH_QUERIES>\n",
      "\n",
      "<CONTEXT>\n",
      "\n",
      "</CONTEXT>\n",
      "\n",
      "<PREVIOUS_ACTIONS>\n",
      "\n",
      "</PREVIOUS_ACTIONS>\n",
      "{\n",
      "  \"action\": \"SEARCH\",\n",
      "  \"reasoning\": \"Since the student is asking about success strategies for Module 1, it's important to find information related to Module 1 goals, requirements, and recommended study methods to improve their chances of success.\",\n",
      "  \"keywords\": [\n",
      "    \"Module 1 success tips\",\n",
      "    \"Module 1 requirements\",\n",
      "    \"how to excel in Module 1\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "ITERATION #1...\n",
      "You are a course teaching assistant.\n",
      "\n",
      "You are given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "\n",
      "The CONTEXT is build with the documents from our FAQ database.\n",
      "SEARCH_QUERIES contains the queries that were used to retrieve the documents\n",
      "from FAQ to and add them to the context.\n",
      "PREVIOUS_ACTIONS contains the actions you already performed.\n",
      "\n",
      "At the beginning the CONTEXT is empty.\n",
      "\n",
      "You can perform the following actions:\n",
      "\n",
      "- Search in the FAQ database to get more data for the CONTEXT\n",
      "- Answer the question using the CONTEXT\n",
      "- Answer the question using your own knowledge\n",
      "\n",
      "For the SEARCH action, build search requests based on the CONTEXT and the QUESTION.\n",
      "Carefully analyze the CONTEXT and generate the requests to deeply explore the topic.\n",
      "\n",
      "Don't use search queries used at the previous iterations.\n",
      "\n",
      "Don't repeat previously performed actions.\n",
      "\n",
      "Don't perform more than 3 iterations for a given student question.\n",
      "The current iteration number: 1. If we exceed the allowed number\n",
      "of iterations, give the best possible answer with the provided information.\n",
      "\n",
      "Output templates:\n",
      "\n",
      "If you want to perform search, use this template:\n",
      "\n",
      "{\n",
      "\"action\" : \"SEARCH\",\n",
      "\"reasoning\" : \"<add your reasoning here>\",\n",
      "\"keywords\" : [\"search query 1\", \"search query 2\", ...]\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\" : \"ANSWER_CONTEXT\",\n",
      "\"answer\" : \"<your answer>\",\n",
      "\"source\" :\"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\" : \"ANSWER\",\n",
      "\"answer\" : \"<your answer>\",\n",
      "\"source\" : \"OWN_KNOWLEDGE\"\n",
      "}\n",
      "\n",
      "<QUESTION>\n",
      "what do I need to do to be succesful at module 1?\n",
      "</QUESTION>\n",
      "\n",
      "<SEARCH_QUERIES>\n",
      "Module 1 requirements\n",
      "Module 1 success tips\n",
      "how to excel in Module 1\n",
      "</SEARCH_QUERIES>\n",
      "\n",
      "<CONTEXT>\n",
      "section: Module 5: pyspark\n",
      "question: Module Not Found Error in Jupyter Notebook .\n",
      "answer: Even after installing pyspark correctly on linux machine (VM ) as per course instructions, faced a module not found error in jupyter notebook .\n",
      "The solution which worked for me(use following in jupyter notebook) :\n",
      "!pip install findspark\n",
      "import findspark\n",
      "findspark.init()\n",
      "Thereafter , import pyspark and create spark contex<<t as usual\n",
      "None of the solutions above worked for me till I ran !pip3 install pyspark instead !pip install pyspark.\n",
      "Filter based on conditions based on multiple columns\n",
      "from pyspark.sql.functions import col\n",
      "new_final.filter((new_final.a_zone==\"Murray Hill\") & (new_final.b_zone==\"Midwood\")).show()\n",
      "Krishna Anand\n",
      "\n",
      "section: Module 5: pyspark\n",
      "question: Py4JJavaError - ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`\n",
      "answer: You need to look for the Py4J file and note the version of the filename. Once you know the version, you can update the export command accordingly, this is how you check yours:\n",
      "` ls ${SPARK_HOME}/python/lib/ ` and then you add it in the export command, mine was:\n",
      "export PYTHONPATH=”${SPARK_HOME}/python/lib/Py4J-0.10.9.5-src.zip:${PYTHONPATH}”\n",
      "Make sure that the version under `${SPARK_HOME}/python/lib/` matches the filename of py4j or you will encounter `ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`.\n",
      "For instance, if the file under `${SPARK_HOME}/python/lib/` was `py4j-0.10.9.3-src.zip`.\n",
      "Then the export PYTHONPATH statement above should be changed to `export PYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH\"` appropriately.\n",
      "Additionally, you can check for the version of ‘py4j’ of the spark you’re using from here and update as mentioned above.\n",
      "~ Abhijit Chakraborty: Sometimes, even with adding the correct version of py4j might not solve the problem. Simply run pip install py4j and problem should be resolved.\n",
      "\n",
      "section: Module 4: analytics engineering with dbt\n",
      "question: DBT - Error: No module named 'pytz' while setting up dbt with docker\n",
      "answer: Following dbt with BigQuery on Docker readme.md, after `docker-compose build` and `docker-compose run dbt-bq-dtc init`, encountered error `ModuleNotFoundError: No module named 'pytz'`\n",
      "Solution:\n",
      "Add `RUN python -m pip install --no-cache pytz` in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Postgres - ModuleNotFoundError: No module named 'psycopg2'\n",
      "answer: Issue:\n",
      "e…\n",
      "Solution:\n",
      "pip install psycopg2-binary\n",
      "If you already have it, you might need to update it:\n",
      "pip install psycopg2-binary --upgrade\n",
      "Other methods, if the above fails:\n",
      "if you are getting the “ ModuleNotFoundError: No module named 'psycopg2' “ error even after the above installation, then try updating conda using the command conda update -n base -c defaults conda. Or if you are using pip, then try updating it before installing the psycopg packages i.e\n",
      "First uninstall the psycopg package\n",
      "Then update conda or pip\n",
      "Then install psycopg again using pip.\n",
      "if you are still facing error with r pcycopg2 and showing pg_config not found then you will have to install postgresql. in MAC it is brew install postgresql\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Python - SQLALchemy - TypeError 'module' object is not callable\n",
      "answer: create_engine('postgresql://root:root@localhost:5432/ny_taxi')  I get the error \"TypeError: 'module' object is not callable\"\n",
      "Solution:\n",
      "conn_string = \"postgresql+psycopg://root:root@localhost:5432/ny_taxi\"\n",
      "engine = create_engine(conn_string)\n",
      "\n",
      "section: Module 6: streaming with kafka\n",
      "question: Module “kafka” not found when trying to run producer.py\n",
      "answer: Solution from Alexey: create a virtual environment and run requirements.txt and the python files in that environment.\n",
      "To create a virtual env and install packages (run only once)\n",
      "python -m venv env\n",
      "source env/bin/activate\n",
      "pip install -r ../requirements.txt\n",
      "To activate it (you'll need to run it every time you need the virtual env):\n",
      "source env/bin/activate\n",
      "To deactivate it:\n",
      "deactivate\n",
      "This works on MacOS, Linux and Windows - but for Windows the path is slightly different (it's env/Scripts/activate)\n",
      "Also the virtual environment should be created only to run the python file. Docker images should first all be up and running.\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Python - SQLAlchemy - ModuleNotFoundError: No module named 'psycopg2'.\n",
      "answer: Error raised during the jupyter notebook’s cell execution:\n",
      "engine = create_engine('postgresql://root:root@localhost:5432/ny_taxi').\n",
      "Solution: Need to install Python module “psycopg2”. Can be installed by Conda or pip.\n",
      "</CONTEXT>\n",
      "\n",
      "<PREVIOUS_ACTIONS>\n",
      "{\"action\": \"SEARCH\", \"reasoning\": \"Since the student is asking about success strategies for Module 1, it's important to find information related to Module 1 goals, requirements, and recommended study methods to improve their chances of success.\", \"keywords\": [\"Module 1 success tips\", \"Module 1 requirements\", \"how to excel in Module 1\"]}\n",
      "</PREVIOUS_ACTIONS>\n",
      "{\n",
      "  \"action\": \"SEARCH\",\n",
      "  \"reasoning\": \"There's a need to gather more information specifically about success strategies or study tips for Module 1, as my previous search did not yield relevant insights related to success in Module 1.\",\n",
      "  \"keywords\": [\n",
      "    \"Module 1 tips for success\",\n",
      "    \"how to succeed in Module 1\",\n",
      "    \"best practices for Module 1\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "ITERATION #2...\n",
      "You are a course teaching assistant.\n",
      "\n",
      "You are given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "\n",
      "The CONTEXT is build with the documents from our FAQ database.\n",
      "SEARCH_QUERIES contains the queries that were used to retrieve the documents\n",
      "from FAQ to and add them to the context.\n",
      "PREVIOUS_ACTIONS contains the actions you already performed.\n",
      "\n",
      "At the beginning the CONTEXT is empty.\n",
      "\n",
      "You can perform the following actions:\n",
      "\n",
      "- Search in the FAQ database to get more data for the CONTEXT\n",
      "- Answer the question using the CONTEXT\n",
      "- Answer the question using your own knowledge\n",
      "\n",
      "For the SEARCH action, build search requests based on the CONTEXT and the QUESTION.\n",
      "Carefully analyze the CONTEXT and generate the requests to deeply explore the topic.\n",
      "\n",
      "Don't use search queries used at the previous iterations.\n",
      "\n",
      "Don't repeat previously performed actions.\n",
      "\n",
      "Don't perform more than 3 iterations for a given student question.\n",
      "The current iteration number: 2. If we exceed the allowed number\n",
      "of iterations, give the best possible answer with the provided information.\n",
      "\n",
      "Output templates:\n",
      "\n",
      "If you want to perform search, use this template:\n",
      "\n",
      "{\n",
      "\"action\" : \"SEARCH\",\n",
      "\"reasoning\" : \"<add your reasoning here>\",\n",
      "\"keywords\" : [\"search query 1\", \"search query 2\", ...]\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\" : \"ANSWER_CONTEXT\",\n",
      "\"answer\" : \"<your answer>\",\n",
      "\"source\" :\"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\" : \"ANSWER\",\n",
      "\"answer\" : \"<your answer>\",\n",
      "\"source\" : \"OWN_KNOWLEDGE\"\n",
      "}\n",
      "\n",
      "<QUESTION>\n",
      "what do I need to do to be succesful at module 1?\n",
      "</QUESTION>\n",
      "\n",
      "<SEARCH_QUERIES>\n",
      "Module 1 requirements\n",
      "how to succeed in Module 1\n",
      "how to excel in Module 1\n",
      "Module 1 success tips\n",
      "Module 1 tips for success\n",
      "best practices for Module 1\n",
      "</SEARCH_QUERIES>\n",
      "\n",
      "<CONTEXT>\n",
      "section: Module 5: pyspark\n",
      "question: Module Not Found Error in Jupyter Notebook .\n",
      "answer: Even after installing pyspark correctly on linux machine (VM ) as per course instructions, faced a module not found error in jupyter notebook .\n",
      "The solution which worked for me(use following in jupyter notebook) :\n",
      "!pip install findspark\n",
      "import findspark\n",
      "findspark.init()\n",
      "Thereafter , import pyspark and create spark contex<<t as usual\n",
      "None of the solutions above worked for me till I ran !pip3 install pyspark instead !pip install pyspark.\n",
      "Filter based on conditions based on multiple columns\n",
      "from pyspark.sql.functions import col\n",
      "new_final.filter((new_final.a_zone==\"Murray Hill\") & (new_final.b_zone==\"Midwood\")).show()\n",
      "Krishna Anand\n",
      "\n",
      "section: Module 5: pyspark\n",
      "question: Py4JJavaError - ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`\n",
      "answer: You need to look for the Py4J file and note the version of the filename. Once you know the version, you can update the export command accordingly, this is how you check yours:\n",
      "` ls ${SPARK_HOME}/python/lib/ ` and then you add it in the export command, mine was:\n",
      "export PYTHONPATH=”${SPARK_HOME}/python/lib/Py4J-0.10.9.5-src.zip:${PYTHONPATH}”\n",
      "Make sure that the version under `${SPARK_HOME}/python/lib/` matches the filename of py4j or you will encounter `ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`.\n",
      "For instance, if the file under `${SPARK_HOME}/python/lib/` was `py4j-0.10.9.3-src.zip`.\n",
      "Then the export PYTHONPATH statement above should be changed to `export PYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH\"` appropriately.\n",
      "Additionally, you can check for the version of ‘py4j’ of the spark you’re using from here and update as mentioned above.\n",
      "~ Abhijit Chakraborty: Sometimes, even with adding the correct version of py4j might not solve the problem. Simply run pip install py4j and problem should be resolved.\n",
      "\n",
      "section: Module 4: analytics engineering with dbt\n",
      "question: DBT - Error: No module named 'pytz' while setting up dbt with docker\n",
      "answer: Following dbt with BigQuery on Docker readme.md, after `docker-compose build` and `docker-compose run dbt-bq-dtc init`, encountered error `ModuleNotFoundError: No module named 'pytz'`\n",
      "Solution:\n",
      "Add `RUN python -m pip install --no-cache pytz` in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Postgres - ModuleNotFoundError: No module named 'psycopg2'\n",
      "answer: Issue:\n",
      "e…\n",
      "Solution:\n",
      "pip install psycopg2-binary\n",
      "If you already have it, you might need to update it:\n",
      "pip install psycopg2-binary --upgrade\n",
      "Other methods, if the above fails:\n",
      "if you are getting the “ ModuleNotFoundError: No module named 'psycopg2' “ error even after the above installation, then try updating conda using the command conda update -n base -c defaults conda. Or if you are using pip, then try updating it before installing the psycopg packages i.e\n",
      "First uninstall the psycopg package\n",
      "Then update conda or pip\n",
      "Then install psycopg again using pip.\n",
      "if you are still facing error with r pcycopg2 and showing pg_config not found then you will have to install postgresql. in MAC it is brew install postgresql\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Python - SQLALchemy - TypeError 'module' object is not callable\n",
      "answer: create_engine('postgresql://root:root@localhost:5432/ny_taxi')  I get the error \"TypeError: 'module' object is not callable\"\n",
      "Solution:\n",
      "conn_string = \"postgresql+psycopg://root:root@localhost:5432/ny_taxi\"\n",
      "engine = create_engine(conn_string)\n",
      "\n",
      "section: Module 6: streaming with kafka\n",
      "question: Module “kafka” not found when trying to run producer.py\n",
      "answer: Solution from Alexey: create a virtual environment and run requirements.txt and the python files in that environment.\n",
      "To create a virtual env and install packages (run only once)\n",
      "python -m venv env\n",
      "source env/bin/activate\n",
      "pip install -r ../requirements.txt\n",
      "To activate it (you'll need to run it every time you need the virtual env):\n",
      "source env/bin/activate\n",
      "To deactivate it:\n",
      "deactivate\n",
      "This works on MacOS, Linux and Windows - but for Windows the path is slightly different (it's env/Scripts/activate)\n",
      "Also the virtual environment should be created only to run the python file. Docker images should first all be up and running.\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Python - SQLAlchemy - ModuleNotFoundError: No module named 'psycopg2'.\n",
      "answer: Error raised during the jupyter notebook’s cell execution:\n",
      "engine = create_engine('postgresql://root:root@localhost:5432/ny_taxi').\n",
      "Solution: Need to install Python module “psycopg2”. Can be installed by Conda or pip.\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Docker - cs to store all code in your default Linux distro to get the best out of file system performance (since Docker runs on WSL2 backend by default for Windows 10 Home / Windows 11 Home users).\n",
      "answer: More info in the Docker Docs on Best Practises\n",
      "</CONTEXT>\n",
      "\n",
      "<PREVIOUS_ACTIONS>\n",
      "{\"action\": \"SEARCH\", \"reasoning\": \"Since the student is asking about success strategies for Module 1, it's important to find information related to Module 1 goals, requirements, and recommended study methods to improve their chances of success.\", \"keywords\": [\"Module 1 success tips\", \"Module 1 requirements\", \"how to excel in Module 1\"]}\n",
      "{\"action\": \"SEARCH\", \"reasoning\": \"There's a need to gather more information specifically about success strategies or study tips for Module 1, as my previous search did not yield relevant insights related to success in Module 1.\", \"keywords\": [\"Module 1 tips for success\", \"how to succeed in Module 1\", \"best practices for Module 1\"]}\n",
      "</PREVIOUS_ACTIONS>\n",
      "{\n",
      "  \"action\": \"ANSWER\",\n",
      "  \"answer\": \"To be successful in Module 1, focus on understanding the fundamental concepts of Docker and Terraform, as these are crucial for your learning in this module. Regularly engage with the course materials, practice consistently through exercises and projects, and utilize resources like forums or study groups for discussing challenges. Make sure to follow the best practices recommended for Docker usage, such as optimizing file system performance when running Docker on Windows 10 Home or Windows 11 using WSL2. Lastly, keep an eye on your code and environment setups to avoid common pitfalls related to missing modules or configuration issues.\",\n",
      "  \"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "question = \"what do I need to do to be succesful at module 1?\"\n",
    "\n",
    "search_queries = []\n",
    "search_results = []\n",
    "previous_actions = []\n",
    "\n",
    "iteration = 0\n",
    "\n",
    "while True:\n",
    "    print(f'ITERATION #{iteration}...')\n",
    "\n",
    "    context = build_context(search_results)\n",
    "    prompt = prompt_template.format(\n",
    "        question = question,\n",
    "        context = context,\n",
    "        search_queries = \"\\n\".join(search_queries),\n",
    "        previous_actions = \"\\n\".join([json.dumps(a) for a in previous_actions]),\n",
    "        max_iterations = 3,\n",
    "        iteration_number = iteration\n",
    "    )\n",
    "\n",
    "    print(prompt)\n",
    "\n",
    "    answer_json = llm(prompt)\n",
    "    answer = json.loads(answer_json)\n",
    "    print(json.dumps(answer, indent=2))\n",
    "\n",
    "    previous_actions.append(answer)\n",
    "\n",
    "    action = answer['action']\n",
    "    if action != 'SEARCH':\n",
    "        break\n",
    "\n",
    "    keywords = answer['keywords']\n",
    "    search_queries = list(set(search_queries) | set(keywords))\n",
    "\n",
    "    for k in keywords:\n",
    "        res = search(k)\n",
    "        search_results.extend(res)\n",
    "\n",
    "    search_results = dedub(search_results)\n",
    "\n",
    "    iteration = iteration + 1\n",
    "    if iteration >= 4:\n",
    "        break\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "4d145cf5-3fc3-4297-98ad-e0d604963e45",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'action': 'ANSWER',\n",
       " 'answer': \"To be successful in Module 1, which focuses on Docker and Terraform, here are some general tips:\\n\\n1. **Understand the Basics of Docker:** Familiarize yourself with fundamental concepts such as images, containers, and orchestration. Make sure you are comfortable with Docker commands and workflows.\\n\\n2. **Learn Terraform Fundamentals:** Ensure you grasp Terraform's configuration language and how it manages infrastructure as code. Practice writing Terraform configurations and understand how to apply them.\\n\\n3. **Hands-On Practice:** Engage in as much hands-on practice as possible. Create your own Docker containers and configurations with Terraform for different scenarios to solidify your understanding.\\n\\n4. **Debugging Skills:** Develop debugging skills for both Docker and Terraform. Learn how to troubleshoot common issues that arise when building containers or managing infrastructure.\\n\\n5. **Follow Best Practices:** Refer to the Docker and Terraform documentation for best practices in deployment and management. This will help you avoid common pitfalls.\\n\\n6. **Engage with the Community:** Participate in forums or study groups related to Docker and Terraform. Discussing problems and solutions with peers can provide valuable insights and improve your learning experience.\\n\\n7. **Stay Organized:** Keep your projects organized, maintain clear documentation for your work, and manage your environment efficiently.\\n\\nBy following these guidelines, you can enhance your learning experience and increase your chances of success in Module 1.\",\n",
       " 'source': 'OWN_KNOWLEDGE'}"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "80a1bf41-4585-42e3-9490-e9611fb077b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1893bd9c-67ec-446e-8c60-acbc47b301d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def agentic_search(question):\n",
    "    search_queries = []\n",
    "    search_results = []\n",
    "    previous_actions = []\n",
    "    \n",
    "    iteration = 0\n",
    "    \n",
    "    while True:\n",
    "        print(f'ITERATION #{iteration}...')\n",
    "    \n",
    "        context = build_context(search_results)\n",
    "        prompt = prompt_template.format(\n",
    "            question = question,\n",
    "            context = context,\n",
    "            search_queries = \"\\n\".join(search_queries),\n",
    "            previous_actions = \"\\n\".join([json.dumps(a) for a in previous_actions]),\n",
    "            max_iterations = 3,\n",
    "            iteration_number = iteration\n",
    "        )\n",
    "    \n",
    "        print(prompt)\n",
    "    \n",
    "        answer_json = llm(prompt)\n",
    "        answer = json.loads(answer_json)\n",
    "        print(json.dumps(answer, indent=2))\n",
    "    \n",
    "        previous_actions.append(answer)\n",
    "    \n",
    "        action = answer['action']\n",
    "        if action != 'SEARCH':\n",
    "            break\n",
    "    \n",
    "        keywords = answer['keywords']\n",
    "        search_queries = list(set(search_queries) | set(keywords))\n",
    "    \n",
    "        for k in keywords:\n",
    "            res = search(k)\n",
    "            search_results.extend(res)\n",
    "    \n",
    "        search_results = dedub(search_results)\n",
    "    \n",
    "        iteration = iteration + 1\n",
    "        if iteration >= 4:\n",
    "            break\n",
    "        print()\n",
    "        \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "ed378ea2-a4d2-4b5e-8aa0-d8395c10df35",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION #0...\n",
      "You are a course teaching assistant.\n",
      "\n",
      "You are given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "\n",
      "The CONTEXT is build with the documents from our FAQ database.\n",
      "SEARCH_QUERIES contains the queries that were used to retrieve the documents\n",
      "from FAQ to and add them to the context.\n",
      "PREVIOUS_ACTIONS contains the actions you already performed.\n",
      "\n",
      "At the beginning the CONTEXT is empty.\n",
      "\n",
      "You can perform the following actions:\n",
      "\n",
      "- Search in the FAQ database to get more data for the CONTEXT\n",
      "- Answer the question using the CONTEXT\n",
      "- Answer the question using your own knowledge\n",
      "\n",
      "For the SEARCH action, build search requests based on the CONTEXT and the QUESTION.\n",
      "Carefully analyze the CONTEXT and generate the requests to deeply explore the topic.\n",
      "\n",
      "Don't use search queries used at the previous iterations.\n",
      "\n",
      "Don't repeat previously performed actions.\n",
      "\n",
      "Don't perform more than 3 iterations for a given student question.\n",
      "The current iteration number: 0. If we exceed the allowed number\n",
      "of iterations, give the best possible answer with the provided information.\n",
      "\n",
      "Output templates:\n",
      "\n",
      "If you want to perform search, use this template:\n",
      "\n",
      "{\n",
      "\"action\" : \"SEARCH\",\n",
      "\"reasoning\" : \"<add your reasoning here>\",\n",
      "\"keywords\" : [\"search query 1\", \"search query 2\", ...]\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\" : \"ANSWER_CONTEXT\",\n",
      "\"answer\" : \"<your answer>\",\n",
      "\"source\" :\"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\" : \"ANSWER\",\n",
      "\"answer\" : \"<your answer>\",\n",
      "\"source\" : \"OWN_KNOWLEDGE\"\n",
      "}\n",
      "\n",
      "<QUESTION>\n",
      "how do I prepare for the course?\n",
      "</QUESTION>\n",
      "\n",
      "<SEARCH_QUERIES>\n",
      "\n",
      "</SEARCH_QUERIES>\n",
      "\n",
      "<CONTEXT>\n",
      "\n",
      "</CONTEXT>\n",
      "\n",
      "<PREVIOUS_ACTIONS>\n",
      "\n",
      "</PREVIOUS_ACTIONS>\n",
      "{\n",
      "  \"action\": \"SEARCH\",\n",
      "  \"reasoning\": \"I need to gather specific advice or guidelines on course preparation from the FAQ database to provide the student with the most relevant and accurate information.\",\n",
      "  \"keywords\": [\n",
      "    \"course preparation\",\n",
      "    \"how to prepare for the course\",\n",
      "    \"student preparation tips\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "ITERATION #1...\n",
      "You are a course teaching assistant.\n",
      "\n",
      "You are given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "\n",
      "The CONTEXT is build with the documents from our FAQ database.\n",
      "SEARCH_QUERIES contains the queries that were used to retrieve the documents\n",
      "from FAQ to and add them to the context.\n",
      "PREVIOUS_ACTIONS contains the actions you already performed.\n",
      "\n",
      "At the beginning the CONTEXT is empty.\n",
      "\n",
      "You can perform the following actions:\n",
      "\n",
      "- Search in the FAQ database to get more data for the CONTEXT\n",
      "- Answer the question using the CONTEXT\n",
      "- Answer the question using your own knowledge\n",
      "\n",
      "For the SEARCH action, build search requests based on the CONTEXT and the QUESTION.\n",
      "Carefully analyze the CONTEXT and generate the requests to deeply explore the topic.\n",
      "\n",
      "Don't use search queries used at the previous iterations.\n",
      "\n",
      "Don't repeat previously performed actions.\n",
      "\n",
      "Don't perform more than 3 iterations for a given student question.\n",
      "The current iteration number: 1. If we exceed the allowed number\n",
      "of iterations, give the best possible answer with the provided information.\n",
      "\n",
      "Output templates:\n",
      "\n",
      "If you want to perform search, use this template:\n",
      "\n",
      "{\n",
      "\"action\" : \"SEARCH\",\n",
      "\"reasoning\" : \"<add your reasoning here>\",\n",
      "\"keywords\" : [\"search query 1\", \"search query 2\", ...]\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\" : \"ANSWER_CONTEXT\",\n",
      "\"answer\" : \"<your answer>\",\n",
      "\"source\" :\"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\" : \"ANSWER\",\n",
      "\"answer\" : \"<your answer>\",\n",
      "\"source\" : \"OWN_KNOWLEDGE\"\n",
      "}\n",
      "\n",
      "<QUESTION>\n",
      "how do I prepare for the course?\n",
      "</QUESTION>\n",
      "\n",
      "<SEARCH_QUERIES>\n",
      "course preparation\n",
      "how to prepare for the course\n",
      "student preparation tips\n",
      "</SEARCH_QUERIES>\n",
      "\n",
      "<CONTEXT>\n",
      "section: General course-related questions\n",
      "question: Course - When will the course start?\n",
      "answer: The purpose of this document is to capture frequently asked technical questions\n",
      "The exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\n",
      "Subscribe to course public Google Calendar (it works from Desktop only).\n",
      "Register before the course starts using this link.\n",
      "Join the course Telegram channel with announcements.\n",
      "Don’t forget to register in DataTalks.Club's Slack and join the channel.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Course - Can I follow the course after it finishes?\n",
      "answer: Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\n",
      "You can also continue looking at the homeworks and continue preparing for the next cohort. I guess you can also start working on your final capstone project.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Certificate - Can I follow the course in a self-paced mode and get a certificate?\n",
      "answer: No, you can only get a certificate if you finish the course with a “live” cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Is it possible to use tool “X” instead of the one tool you use in the course?\n",
      "answer: Yes, this applies if you want to use Airflow or Prefect instead of Mage, AWS or Snowflake instead of GCP products or Tableau instead of Metabase or Google data studio.\n",
      "The course covers 2 alternative data stacks, one using GCP and one using local installation of everything. You can use one of them or use your tool of choice.\n",
      "Should you consider it instead of the one tool you use? That we can’t support you if you choose to use a different stack, also you would need to explain the different choices of tool for the peer review of your capstone project.\n",
      "\n",
      "section: General course-related questions\n",
      "question: How do I use Git / GitHub for this course?\n",
      "answer: After you create a GitHub account, you should clone the course repo to your local machine using the process outlined in this video: Git for Everybody: How to Clone a Repository from GitHub\n",
      "Having this local repository on your computer will make it easy for you to access the instructors’ code and make pull requests (if you want to add your own notes or make changes to the course content).\n",
      "You will probably also create your own repositories that host your notes, versions of your file, to do this. Here is a great tutorial that shows you how to do this: https://www.atlassian.com/git/tutorials/setting-up-a-repository\n",
      "Remember to ignore large database, .csv, and .gz files, and other files that should not be saved to a repository. Use .gitignore for this: https://www.atlassian.com/git/tutorials/saving-changes/gitignore NEVER store passwords or keys in a git repo (even if that repo is set to private).\n",
      "This is also a great resource: https://dangitgit.com/\n",
      "\n",
      "section: Module 5: pyspark\n",
      "question: Hadoop - Exception in thread \"main\" java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\n",
      "answer: If you are seeing this (or similar) error when attempting to write to parquet, it is likely an issue with your path variables.\n",
      "For Windows, create a new User Variable “HADOOP_HOME” that points to your Hadoop directory. Then add “%HADOOP_HOME%\\bin” to the PATH variable.\n",
      "Additional tips can be found here: https://stackoverflow.com/questions/41851066/exception-in-thread-main-java-lang-unsatisfiedlinkerror-org-apache-hadoop-io\n",
      "\n",
      "section: Module 3: Data Warehousing\n",
      "question: GCS Bucket - I query my dataset and get a Bad character (ASCII 0) error?\n",
      "answer: Check the Schema\n",
      "You might have a wrong formatting\n",
      "Try to upload the CSV.GZ files without formatting or going through pandas via wget\n",
      "See this Slack conversation for helpful tips\n",
      "\n",
      "section: Module 2: Workflow Orchestration\n",
      "question: Docker - 2.2.2 Configure Mage\n",
      "answer: Issue : Docker containers exit instantly with code 132, upon docker compose up\n",
      "Mage documentation has it listing the cause as \"older architecture\" .\n",
      "This might be a hardware issue, so unless you have another computer, you can't solve it without purchasing a new one, so the next best solution is a VM.\n",
      "This is from a student running on a VirtualBox VM, Ubuntu 22.04.3 LTS, Docker version 25.0.2. So not having the context on how the vbox was spin up with (CPU, RAM, network, etc), it’s really inconclusive at this time.\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: GCP - The project to be billed is associated with an absent billing account\n",
      "answer: If you receive the error: “Error 403: The project to be billed is associated with an absent billing account., accountDisabled” It is most likely because you did not enter YOUR project ID. The snip below is from video 1.3.2\n",
      "The value you enter here will be unique to each student. You can find this value on your GCP Dashboard when you login.\n",
      "Ashish Agrawal\n",
      "Another possibility is that you have not linked your billing account to your current project\n",
      "</CONTEXT>\n",
      "\n",
      "<PREVIOUS_ACTIONS>\n",
      "{\"action\": \"SEARCH\", \"reasoning\": \"I need to gather specific advice or guidelines on course preparation from the FAQ database to provide the student with the most relevant and accurate information.\", \"keywords\": [\"course preparation\", \"how to prepare for the course\", \"student preparation tips\"]}\n",
      "</PREVIOUS_ACTIONS>\n",
      "{\n",
      "  \"action\": \"SEARCH\",\n",
      "  \"reasoning\": \"Since the previous search did not yield any specific preparation tips or guidelines for the course, I will perform a new search to find additional advice on how to prepare for the course effectively.\",\n",
      "  \"keywords\": [\n",
      "    \"getting started with the course\",\n",
      "    \"tips for new students\",\n",
      "    \"course readiness advice\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "ITERATION #2...\n",
      "You are a course teaching assistant.\n",
      "\n",
      "You are given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "\n",
      "The CONTEXT is build with the documents from our FAQ database.\n",
      "SEARCH_QUERIES contains the queries that were used to retrieve the documents\n",
      "from FAQ to and add them to the context.\n",
      "PREVIOUS_ACTIONS contains the actions you already performed.\n",
      "\n",
      "At the beginning the CONTEXT is empty.\n",
      "\n",
      "You can perform the following actions:\n",
      "\n",
      "- Search in the FAQ database to get more data for the CONTEXT\n",
      "- Answer the question using the CONTEXT\n",
      "- Answer the question using your own knowledge\n",
      "\n",
      "For the SEARCH action, build search requests based on the CONTEXT and the QUESTION.\n",
      "Carefully analyze the CONTEXT and generate the requests to deeply explore the topic.\n",
      "\n",
      "Don't use search queries used at the previous iterations.\n",
      "\n",
      "Don't repeat previously performed actions.\n",
      "\n",
      "Don't perform more than 3 iterations for a given student question.\n",
      "The current iteration number: 2. If we exceed the allowed number\n",
      "of iterations, give the best possible answer with the provided information.\n",
      "\n",
      "Output templates:\n",
      "\n",
      "If you want to perform search, use this template:\n",
      "\n",
      "{\n",
      "\"action\" : \"SEARCH\",\n",
      "\"reasoning\" : \"<add your reasoning here>\",\n",
      "\"keywords\" : [\"search query 1\", \"search query 2\", ...]\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\" : \"ANSWER_CONTEXT\",\n",
      "\"answer\" : \"<your answer>\",\n",
      "\"source\" :\"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\" : \"ANSWER\",\n",
      "\"answer\" : \"<your answer>\",\n",
      "\"source\" : \"OWN_KNOWLEDGE\"\n",
      "}\n",
      "\n",
      "<QUESTION>\n",
      "how do I prepare for the course?\n",
      "</QUESTION>\n",
      "\n",
      "<SEARCH_QUERIES>\n",
      "course preparation\n",
      "getting started with the course\n",
      "student preparation tips\n",
      "tips for new students\n",
      "how to prepare for the course\n",
      "course readiness advice\n",
      "</SEARCH_QUERIES>\n",
      "\n",
      "<CONTEXT>\n",
      "section: General course-related questions\n",
      "question: Course - When will the course start?\n",
      "answer: The purpose of this document is to capture frequently asked technical questions\n",
      "The exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\n",
      "Subscribe to course public Google Calendar (it works from Desktop only).\n",
      "Register before the course starts using this link.\n",
      "Join the course Telegram channel with announcements.\n",
      "Don’t forget to register in DataTalks.Club's Slack and join the channel.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Course - Can I follow the course after it finishes?\n",
      "answer: Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\n",
      "You can also continue looking at the homeworks and continue preparing for the next cohort. I guess you can also start working on your final capstone project.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Certificate - Can I follow the course in a self-paced mode and get a certificate?\n",
      "answer: No, you can only get a certificate if you finish the course with a “live” cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Is it possible to use tool “X” instead of the one tool you use in the course?\n",
      "answer: Yes, this applies if you want to use Airflow or Prefect instead of Mage, AWS or Snowflake instead of GCP products or Tableau instead of Metabase or Google data studio.\n",
      "The course covers 2 alternative data stacks, one using GCP and one using local installation of everything. You can use one of them or use your tool of choice.\n",
      "Should you consider it instead of the one tool you use? That we can’t support you if you choose to use a different stack, also you would need to explain the different choices of tool for the peer review of your capstone project.\n",
      "\n",
      "section: General course-related questions\n",
      "question: How do I use Git / GitHub for this course?\n",
      "answer: After you create a GitHub account, you should clone the course repo to your local machine using the process outlined in this video: Git for Everybody: How to Clone a Repository from GitHub\n",
      "Having this local repository on your computer will make it easy for you to access the instructors’ code and make pull requests (if you want to add your own notes or make changes to the course content).\n",
      "You will probably also create your own repositories that host your notes, versions of your file, to do this. Here is a great tutorial that shows you how to do this: https://www.atlassian.com/git/tutorials/setting-up-a-repository\n",
      "Remember to ignore large database, .csv, and .gz files, and other files that should not be saved to a repository. Use .gitignore for this: https://www.atlassian.com/git/tutorials/saving-changes/gitignore NEVER store passwords or keys in a git repo (even if that repo is set to private).\n",
      "This is also a great resource: https://dangitgit.com/\n",
      "\n",
      "section: Module 5: pyspark\n",
      "question: Hadoop - Exception in thread \"main\" java.lang.UnsatisfiedLinkError: org.apache.hadoop.io.nativeio.NativeIO$Windows.access0(Ljava/lang/String;I)Z\n",
      "answer: If you are seeing this (or similar) error when attempting to write to parquet, it is likely an issue with your path variables.\n",
      "For Windows, create a new User Variable “HADOOP_HOME” that points to your Hadoop directory. Then add “%HADOOP_HOME%\\bin” to the PATH variable.\n",
      "Additional tips can be found here: https://stackoverflow.com/questions/41851066/exception-in-thread-main-java-lang-unsatisfiedlinkerror-org-apache-hadoop-io\n",
      "\n",
      "section: Module 3: Data Warehousing\n",
      "question: GCS Bucket - I query my dataset and get a Bad character (ASCII 0) error?\n",
      "answer: Check the Schema\n",
      "You might have a wrong formatting\n",
      "Try to upload the CSV.GZ files without formatting or going through pandas via wget\n",
      "See this Slack conversation for helpful tips\n",
      "\n",
      "section: Module 2: Workflow Orchestration\n",
      "question: Docker - 2.2.2 Configure Mage\n",
      "answer: Issue : Docker containers exit instantly with code 132, upon docker compose up\n",
      "Mage documentation has it listing the cause as \"older architecture\" .\n",
      "This might be a hardware issue, so unless you have another computer, you can't solve it without purchasing a new one, so the next best solution is a VM.\n",
      "This is from a student running on a VirtualBox VM, Ubuntu 22.04.3 LTS, Docker version 25.0.2. So not having the context on how the vbox was spin up with (CPU, RAM, network, etc), it’s really inconclusive at this time.\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: GCP - The project to be billed is associated with an absent billing account\n",
      "answer: If you receive the error: “Error 403: The project to be billed is associated with an absent billing account., accountDisabled” It is most likely because you did not enter YOUR project ID. The snip below is from video 1.3.2\n",
      "The value you enter here will be unique to each student. You can find this value on your GCP Dashboard when you login.\n",
      "Ashish Agrawal\n",
      "Another possibility is that you have not linked your billing account to your current project\n",
      "\n",
      "section: Module 4: analytics engineering with dbt\n",
      "question: dbt job - Triggered by pull requests is disabled when I try to create a new Continuous Integration job in dbt cloud.\n",
      "answer: Solution:\n",
      "Check if you’re on the Developer Plan. As per the prerequisites, you'll need to be enrolled in the Team Plan or Enterprise Plan to set up a CI Job in dbt Cloud.\n",
      "So If you're on the Developer Plan, you'll need to upgrade to utilise CI Jobs.\n",
      "Note from another user: I’m in the Team Plan (trial period) but the option is still disabled. What worked for me instead was this. It works for the Developer (free) plan.\n",
      "\n",
      "section: General course-related questions\n",
      "question: How to troubleshoot issues\n",
      "answer: The first step is to try to solve the issue on your own. Get used to solving problems and reading documentation. This will be a real life skill you need when employed. [ctrl+f] is your friend, use it! It is a universal shortcut and works in all apps/browsers.\n",
      "What does the error say? There will often be a description of the error or instructions on what is needed or even how to fix it. I have even seen a link to the solution. Does it reference a specific line of your code?\n",
      "Restart app or server/pc.\n",
      "Google it, use ChatGPT, Bing AI etc.\n",
      "It is going to be rare that you are the first to have the problem, someone out there has posted the fly issue and likely the solution.\n",
      "Search using: <technology> <problem statement>. Example: pgcli error column c.relhasoids does not exist.\n",
      "There are often different solutions for the same problem due to variation in environments.\n",
      "Check the tech’s documentation. Use its search if available or use the browsers search function.\n",
      "Try uninstall (this may remove the bad actor) and reinstall of application or reimplementation of action. Remember to restart the server/pc for reinstalls.\n",
      "Sometimes reinstalling fails to resolve the issue but works if you uninstall first.\n",
      "Post your question to Stackoverflow. Read the Stackoverflow guide on posting good questions.\n",
      "https://stackoverflow.com/help/how-to-ask\n",
      "This will be your real life. Ask an expert in the future (in addition to coworkers).\n",
      "Ask in Slack\n",
      "Before asking a question,\n",
      "Check Pins (where the shortcut to the repo and this FAQ is located)\n",
      "Use the slack app’s search function\n",
      "Use the bot @ZoomcampQABot to do the search for you\n",
      "check the FAQ (this document), use search [ctrl+f]\n",
      "When asking a question, include as much information as possible:\n",
      "What are you coding on? What OS?\n",
      "What command did you run, which video did you follow? Etc etc\n",
      "What error did you get? Does it have a line number to the “offending” code and have you check it for typos?\n",
      "What have you tried that did not work? This answer is crucial as without it, helpers would ask you to do the suggestions in the error log first. Or just read this FAQ document.\n",
      "DO NOT use screenshots, especially don’t take pictures from a phone.\n",
      "DO NOT tag instructors, it may discourage others from helping you. Copy and paste errors; if it’s long, just post it in a reply to your thread.\n",
      "Use ``` for formatting your code.\n",
      "Use the same thread for the conversation (that means reply to your own thread).\n",
      "DO NOT create multiple posts to discuss the issue.\n",
      "learYou may create a new post if the issue reemerges down the road. Describe what has changed in the environment.\n",
      "Provide additional information in the same thread of the steps you have taken for resolution.\n",
      "Take a break and come back later. You will be amazed at how often you figure out the solution after letting your brain rest. Get some fresh air, workout, play a video game, watch a tv show, whatever allows your brain to not think about it for a little while or even until the next day.\n",
      "Remember technology issues in real life sometimes take days or even weeks to resolve.\n",
      "If somebody helped you with your problem and it's not in the FAQ, please add it there. It will help other students.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Environment - Is the course [Windows/mac/Linux/...] friendly?\n",
      "answer: Yes! Linux is ideal but technically it should not matter. Students last year used all 3 OSes successfully\n",
      "</CONTEXT>\n",
      "\n",
      "<PREVIOUS_ACTIONS>\n",
      "{\"action\": \"SEARCH\", \"reasoning\": \"I need to gather specific advice or guidelines on course preparation from the FAQ database to provide the student with the most relevant and accurate information.\", \"keywords\": [\"course preparation\", \"how to prepare for the course\", \"student preparation tips\"]}\n",
      "{\"action\": \"SEARCH\", \"reasoning\": \"Since the previous search did not yield any specific preparation tips or guidelines for the course, I will perform a new search to find additional advice on how to prepare for the course effectively.\", \"keywords\": [\"getting started with the course\", \"tips for new students\", \"course readiness advice\"]}\n",
      "</PREVIOUS_ACTIONS>\n",
      "{\n",
      "  \"action\": \"ANSWER\",\n",
      "  \"answer\": \"To prepare for the course, ensure you register before the start date (January 15, 2024), and join the course's public Google Calendar and Telegram channel for announcements. Familiarize yourself with the course materials available via DataTalks.Club\\u2019s Slack and make sure to have your tools set up as outlined in the course guidelines. It's recommended to have a good path setup for any necessary tools like GitHub. Overall, being organized and proactive in setting up your environment will greatly enhance your experience in the course.\",\n",
      "  \"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'action': 'ANSWER',\n",
       " 'answer': \"To prepare for the course, ensure you register before the start date (January 15, 2024), and join the course's public Google Calendar and Telegram channel for announcements. Familiarize yourself with the course materials available via DataTalks.Club’s Slack and make sure to have your tools set up as outlined in the course guidelines. It's recommended to have a good path setup for any necessary tools like GitHub. Overall, being organized and proactive in setting up your environment will greatly enhance your experience in the course.\",\n",
       " 'source': 'OWN_KNOWLEDGE'}"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agentic_search(\"how do I prepare for the course?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "d0c7c43f-333d-4f47-bde4-f4dd6340e406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To prepare for the course, ensure you register before the start date (January 15, 2024), and join the course's public Google Calendar and Telegram channel for announcements. Familiarize yourself with the course materials available via DataTalks.Club’s Slack and make sure to have your tools set up as outlined in the course guidelines. It's recommended to have a good path setup for any necessary tools like GitHub. Overall, being organized and proactive in setting up your environment will greatly enhance your experience in the course.\n"
     ]
    }
   ],
   "source": [
    "print(_['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d66ff8de-57b9-4c6b-999f-4e7330f47a9e",
   "metadata": {},
   "source": [
    "#### Function calling (\"tool use\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "872f005f-75b5-4697-b2db-e9c0553d90c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    boost = {'question': 3.0, 'section': 0.5}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={'course': 'data-engineering-zoomcamp'},\n",
    "        boost_dict=boost,\n",
    "        num_results=5,\n",
    "        output_ids=True\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1aafca73-9e92-4309-a94e-5c21ccf5abca",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_tool = {\n",
    "    \"type\" : \"function\",\n",
    "    \"name\" : \"search\",\n",
    "    \"description\" : \"Search the FAQ database\",\n",
    "    \"parameters\" : {\n",
    "        \"type\" : \"object\",\n",
    "        \"properties\" : {\n",
    "            \"query\" : {\n",
    "                \"type\" : \"string\",\n",
    "                \"description\" : \"Search query text to look up in the course FAQ.\"\n",
    "            }\n",
    "        },\n",
    "        \"required\" : [\"query\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5e957dd0-7207-4d55-a3a7-41b128877eac",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ResponseFunctionToolCall(arguments='{\"query\":\"how to do well in module 1\"}', call_id='call_LUkEw4fkKuiIX6QqHidYL6fm', name='search', type='function_call', id='fc_68a500dd625c8193a779ab9ef1212e7f06004efefa7d53d2', status='completed')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"how do I do well in module 1?\"\n",
    "\n",
    "developer_prompt = \"\"\"\n",
    "You're a course teaching assistant.\n",
    "You're given a question from a course student and your task is to answer it.\n",
    "\"\"\".strip()\n",
    "\n",
    "tools = [search_tool]\n",
    "\n",
    "chat_messages = [\n",
    "    {\"role\" : \"developer\", \"content\" : developer_prompt},\n",
    "    {\"role\" : \"user\", \"content\" : question}\n",
    "]\n",
    "\n",
    "response = client.responses.create(\n",
    "    model = 'gpt-4o-mini',\n",
    "    input = chat_messages,\n",
    "    tools = tools\n",
    ")\n",
    "response.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c47f3535-5751-40f4-8200-370e2a20c11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "calls = response.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c034929b-2514-4eec-89bb-b1b3a012da63",
   "metadata": {},
   "outputs": [],
   "source": [
    "call = calls[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9d9914ab-84e8-4fac-8c3a-0d01873b68d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_name = call.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6e66f0a6-e6c6-4b38-bc72-4cd6669e528d",
   "metadata": {},
   "outputs": [],
   "source": [
    "arguments = json.loads(call.arguments)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b30e17b9-1cd5-44f6-915f-b3929cb92100",
   "metadata": {},
   "source": [
    "#### globabls - Note\n",
    "globals() returns a dictionary that represents the current global symbol table.\n",
    "This dictionary contains all global names (variables, functions, classes, imports, etc.) that are defined in the current module.\n",
    "- globals(): Fast & flexible, but risky and too open.\n",
    "- getattr(tools, \"hello\") is similar to globals()[\"hello\"], but scoped to the tools module instead of everything in your global namespace. If all the functions live inside a specific module (say tools.py).\n",
    "- production systems usually use a dispatcher dictionary: FastAPI doesn’t use globals() or getattr directly. Instead, it uses a registry of routes (functions) attached to an APIRouter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2926d571-17c1-47c5-8b0f-e5d109bd3b48",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.search(query)>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "globals()[f_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9df8daa9-d41f-462a-8855-33c2a2465127",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.search(query)>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#gives a pointer to the search functions same as above\n",
    "globals()['search']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3e966964-18b7-47bd-b0ec-0619043e8b41",
   "metadata": {},
   "outputs": [],
   "source": [
    "f = globals()[f_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6a2ba89e-99a2-423f-9da6-dac748f42826",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#it passes all the arguments to the function search or f here\n",
    "search_results = f(**arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "66c733cd-8e1b-44dd-8e73-03bf6ef33729",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_messages.append(call)\n",
    "\n",
    "chat_messages.append({\n",
    "    \"type\": \"function_call_output\",\n",
    "    \"call_id\": call.call_id,\n",
    "    \"output\": json.dumps(search_results),\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6fadefbc-74d9-4040-85a5-faa00c11f645",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'developer',\n",
       "  'content': \"You're a course teaching assistant.\\nYou're given a question from a course student and your task is to answer it.\"},\n",
       " {'role': 'user', 'content': 'how do I do well in module 1?'},\n",
       " ResponseFunctionToolCall(arguments='{\"query\":\"how to do well in module 1\"}', call_id='call_LUkEw4fkKuiIX6QqHidYL6fm', name='search', type='function_call', id='fc_68a500dd625c8193a779ab9ef1212e7f06004efefa7d53d2', status='completed'),\n",
       " {'type': 'function_call_output',\n",
       "  'call_id': 'call_LUkEw4fkKuiIX6QqHidYL6fm',\n",
       "  'output': '[{\"text\": \"Even after installing pyspark correctly on linux machine (VM ) as per course instructions, faced a module not found error in jupyter notebook .\\\\nThe solution which worked for me(use following in jupyter notebook) :\\\\n!pip install findspark\\\\nimport findspark\\\\nfindspark.init()\\\\nThereafter , import pyspark and create spark contex<<t as usual\\\\nNone of the solutions above worked for me till I ran !pip3 install pyspark instead !pip install pyspark.\\\\nFilter based on conditions based on multiple columns\\\\nfrom pyspark.sql.functions import col\\\\nnew_final.filter((new_final.a_zone==\\\\\"Murray Hill\\\\\") & (new_final.b_zone==\\\\\"Midwood\\\\\")).show()\\\\nKrishna Anand\", \"section\": \"Module 5: pyspark\", \"question\": \"Module Not Found Error in Jupyter Notebook .\", \"course\": \"data-engineering-zoomcamp\", \"_id\": 322}, {\"text\": \"You need to look for the Py4J file and note the version of the filename. Once you know the version, you can update the export command accordingly, this is how you check yours:\\\\n` ls ${SPARK_HOME}/python/lib/ ` and then you add it in the export command, mine was:\\\\nexport PYTHONPATH=\\\\u201d${SPARK_HOME}/python/lib/Py4J-0.10.9.5-src.zip:${PYTHONPATH}\\\\u201d\\\\nMake sure that the version under `${SPARK_HOME}/python/lib/` matches the filename of py4j or you will encounter `ModuleNotFoundError: No module named \\'py4j\\'` while executing `import pyspark`.\\\\nFor instance, if the file under `${SPARK_HOME}/python/lib/` was `py4j-0.10.9.3-src.zip`.\\\\nThen the export PYTHONPATH statement above should be changed to `export PYTHONPATH=\\\\\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH\\\\\"` appropriately.\\\\nAdditionally, you can check for the version of \\\\u2018py4j\\\\u2019 of the spark you\\\\u2019re using from here and update as mentioned above.\\\\n~ Abhijit Chakraborty: Sometimes, even with adding the correct version of py4j might not solve the problem. Simply run pip install py4j and problem should be resolved.\", \"section\": \"Module 5: pyspark\", \"question\": \"Py4JJavaError - ModuleNotFoundError: No module named \\'py4j\\'` while executing `import pyspark`\", \"course\": \"data-engineering-zoomcamp\", \"_id\": 323}, {\"text\": \"Following dbt with BigQuery on Docker readme.md, after `docker-compose build` and `docker-compose run dbt-bq-dtc init`, encountered error `ModuleNotFoundError: No module named \\'pytz\\'`\\\\nSolution:\\\\nAdd `RUN python -m pip install --no-cache pytz` in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`\", \"section\": \"Module 4: analytics engineering with dbt\", \"question\": \"DBT - Error: No module named \\'pytz\\' while setting up dbt with docker\", \"course\": \"data-engineering-zoomcamp\", \"_id\": 299}, {\"text\": \"create_engine(\\'postgresql://root:root@localhost:5432/ny_taxi\\')  I get the error \\\\\"TypeError: \\'module\\' object is not callable\\\\\"\\\\nSolution:\\\\nconn_string = \\\\\"postgresql+psycopg://root:root@localhost:5432/ny_taxi\\\\\"\\\\nengine = create_engine(conn_string)\", \"section\": \"Module 1: Docker and Terraform\", \"question\": \"Python - SQLALchemy - TypeError \\'module\\' object is not callable\", \"course\": \"data-engineering-zoomcamp\", \"_id\": 124}, {\"text\": \"Error raised during the jupyter notebook\\\\u2019s cell execution:\\\\nengine = create_engine(\\'postgresql://root:root@localhost:5432/ny_taxi\\').\\\\nSolution: Need to install Python module \\\\u201cpsycopg2\\\\u201d. Can be installed by Conda or pip.\", \"section\": \"Module 1: Docker and Terraform\", \"question\": \"Python - SQLAlchemy - ModuleNotFoundError: No module named \\'psycopg2\\'.\", \"course\": \"data-engineering-zoomcamp\", \"_id\": 125}]'}]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0de6a68e-5c07-44a6-ad0c-a7832b486e34",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ResponseOutputMessage(id='msg_68a500e413008193add3807914e838d906004efefa7d53d2', content=[ResponseOutputText(annotations=[], text=\"To excel in Module 1 of your course, here are some tips:\\n\\n1. **Understand Key Concepts**: Familiarize yourself with Docker and Terraform, as they are the primary tools in this module. Review the foundational concepts and their applications in cloud infrastructure.\\n\\n2. **Practice Hands-On**: Execute the commands and examples provided in the module. Set up a local environment to reinforce your learning through practical application.\\n\\n3. **Troubleshoot Common Errors**: Be aware of common errors, such as issues with SQLAlchemy or Docker setup. For example:\\n   - If you encounter `ModuleNotFoundError: No module named 'psycopg2'`, ensure that you have installed the `psycopg2` module via pip or conda.\\n   - For SQLAlchemy-related issues, verify your connection string format.\\n\\n4. **Engagement with Course Material**: Actively participate in discussions and seek help when needed. Utilize forums or Q&A sections to clarify doubts.\\n\\n5. **Utilize Resources**: Make use of additional resources like documentation, tutorials, and practice exams to deepen your understanding.\\n\\nBy following these strategies, you can enhance your performance and gain a thorough understanding of Module 1. Good luck!\", type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#invoke the call one more time\n",
    "\n",
    "response = client.responses.create(\n",
    "    model = 'gpt-4o-mini',\n",
    "    input = chat_messages,\n",
    "    tools = tools\n",
    ")\n",
    "response.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4d2ab484-b7ed-4ccc-9d40-1f8dbc45cbe6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To excel in Module 1 of your course, here are some tips:\n",
      "\n",
      "1. **Understand Key Concepts**: Familiarize yourself with Docker and Terraform, as they are the primary tools in this module. Review the foundational concepts and their applications in cloud infrastructure.\n",
      "\n",
      "2. **Practice Hands-On**: Execute the commands and examples provided in the module. Set up a local environment to reinforce your learning through practical application.\n",
      "\n",
      "3. **Troubleshoot Common Errors**: Be aware of common errors, such as issues with SQLAlchemy or Docker setup. For example:\n",
      "   - If you encounter `ModuleNotFoundError: No module named 'psycopg2'`, ensure that you have installed the `psycopg2` module via pip or conda.\n",
      "   - For SQLAlchemy-related issues, verify your connection string format.\n",
      "\n",
      "4. **Engagement with Course Material**: Actively participate in discussions and seek help when needed. Utilize forums or Q&A sections to clarify doubts.\n",
      "\n",
      "5. **Utilize Resources**: Make use of additional resources like documentation, tutorials, and practice exams to deepen your understanding.\n",
      "\n",
      "By following these strategies, you can enhance your performance and gain a thorough understanding of Module 1. Good luck!\n"
     ]
    }
   ],
   "source": [
    "print(response.output[0].content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f266f5a-427d-4348-9403-a8fa015d18e4",
   "metadata": {},
   "source": [
    "#### Multiple calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "92a19cd2-6e4b-4e30-8707-0fdbee3ccac3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ResponseFunctionToolCall(arguments='{\"query\":\"how to do well in module 1\"}', call_id='call_ql6UQ8GpACFYp0hC7MneeuHK', name='search', type='function_call', id='fc_68a500ec45a481908e53132ca8d294a30e31cba901a82261', status='completed'),\n",
       " ResponseFunctionToolCall(arguments='{\"query\":\"tips for succeeding in module 1\"}', call_id='call_PY7wa4qgBQ04lyLmbbmHgg3z', name='search', type='function_call', id='fc_68a500eca2088190874218dd3a4f979a0e31cba901a82261', status='completed'),\n",
       " ResponseFunctionToolCall(arguments='{\"query\":\"module 1 study strategies\"}', call_id='call_1kdJucH0VbecP1B8LjTewWbd', name='search', type='function_call', id='fc_68a500ed09e08190983c5090dd667c270e31cba901a82261', status='completed')]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"how do I do well in module 1?\"\n",
    "\n",
    "developer_prompt = \"\"\"\n",
    "You're a course teaching assistant.\n",
    "You're given a question from a course student and your task is to answer it.\n",
    "If you look up something in FAQ, convert the student question into multiple queries.\n",
    "\"\"\".strip()\n",
    "\n",
    "tools = [search_tool]\n",
    "\n",
    "chat_messages = [\n",
    "    {\"role\" : \"developer\", \"content\" : developer_prompt},\n",
    "    {\"role\" : \"user\", \"content\" : question}\n",
    "]\n",
    "\n",
    "response = client.responses.create(\n",
    "    model = 'gpt-4o-mini',\n",
    "    input = chat_messages,\n",
    "    tools = tools\n",
    ")\n",
    "response.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a583faaa-8489-41ed-90ed-4bc9c18bf645",
   "metadata": {},
   "outputs": [],
   "source": [
    "calls = response.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "139c669d-90fd-46a6-8a2d-9a41cbda484d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for call in calls:\n",
    "    f_name = call.name\n",
    "    arguments = json.loads(call.arguments)\n",
    "    f = globals()[f_name]\n",
    "    results = f(**arguments)\n",
    "    chat_messages.append(call)\n",
    "    \n",
    "    chat_messages.append({\n",
    "        \"type\" : \"function_call_output\",\n",
    "        \"call_id\" : call.call_id,\n",
    "        \"output\" : json.dumps(results),\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0d2f06ba-64a1-4fb7-8c02-d38cff8eed0e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ResponseOutputMessage(id='msg_68a500ef39dc8190b1bce0bdec5498600e31cba901a82261', content=[ResponseOutputText(annotations=[], text=\"To do well in Module 1 of the course, here's a set of strategies and tips you might find helpful:\\n\\n1. **Understand the Basics**:\\n   - Make sure you have a strong foundational understanding of Docker and Terraform, as they are critical tools for this module. Review the introductory materials thoroughly.\\n\\n2. **Hands-On Practice**:\\n   - Set up your local development environment as instructed. Create a sample project using Docker and Terraform to solidify your understanding.\\n\\n3. **Utilize Resources**:\\n   - Your course likely provides resources like video lectures, reading materials, and forums. Don't hesitate to use these to clarify concepts.\\n\\n4. **Seek Help When Needed**:\\n   - If you encounter errors like `ModuleNotFoundError` for PostgreSQL dependencies, seek solutions promptly. For instance, if you get errors related to `psycopg2`, ensure you use:\\n     ```bash\\n     pip install psycopg2-binary\\n     ```\\n   - If issues persist, consult the FAQs or post in course forums for guidance.\\n\\n5. **Experiment and Innovate**:\\n   - Modify example code provided in the lectures to see what changes. This will deepen your comprehension and highlight the practical applications of what you learn.\\n\\n6. **Stay Organized**:\\n   - Keep notes of what you learn each week, especially tricky concepts or errors encountered. This will help you in revision before assessments.\\n\\n7. **Group Study**:\\n   - If possible, study with peers. Discussing topics and solving problems together can enhance understanding.\\n\\nBy following these tips, actively engaging with the course content, and practicing consistently, you'll be better positioned to succeed in Module 1. If you have specific topics or areas you find challenging, feel free to ask for targeted help!\", type='output_text', logprobs=[])], role='assistant', status='completed', type='message')]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = client.responses.create(\n",
    "    model = 'gpt-4o-mini',\n",
    "    input = chat_messages,\n",
    "    tools = tools\n",
    ")\n",
    "response.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "abdf677b-bfe0-420c-882c-aeddb1446a1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "To do well in Module 1 of the course, here's a set of strategies and tips you might find helpful:\n",
      "\n",
      "1. **Understand the Basics**:\n",
      "   - Make sure you have a strong foundational understanding of Docker and Terraform, as they are critical tools for this module. Review the introductory materials thoroughly.\n",
      "\n",
      "2. **Hands-On Practice**:\n",
      "   - Set up your local development environment as instructed. Create a sample project using Docker and Terraform to solidify your understanding.\n",
      "\n",
      "3. **Utilize Resources**:\n",
      "   - Your course likely provides resources like video lectures, reading materials, and forums. Don't hesitate to use these to clarify concepts.\n",
      "\n",
      "4. **Seek Help When Needed**:\n",
      "   - If you encounter errors like `ModuleNotFoundError` for PostgreSQL dependencies, seek solutions promptly. For instance, if you get errors related to `psycopg2`, ensure you use:\n",
      "     ```bash\n",
      "     pip install psycopg2-binary\n",
      "     ```\n",
      "   - If issues persist, consult the FAQs or post in course forums for guidance.\n",
      "\n",
      "5. **Experiment and Innovate**:\n",
      "   - Modify example code provided in the lectures to see what changes. This will deepen your comprehension and highlight the practical applications of what you learn.\n",
      "\n",
      "6. **Stay Organized**:\n",
      "   - Keep notes of what you learn each week, especially tricky concepts or errors encountered. This will help you in revision before assessments.\n",
      "\n",
      "7. **Group Study**:\n",
      "   - If possible, study with peers. Discussing topics and solving problems together can enhance understanding.\n",
      "\n",
      "By following these tips, actively engaging with the course content, and practicing consistently, you'll be better positioned to succeed in Module 1. If you have specific topics or areas you find challenging, feel free to ask for targeted help!\n"
     ]
    }
   ],
   "source": [
    "print(response.output[0].content[0].text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d46f58-7e33-438d-bf41-28e24ccb923a",
   "metadata": {},
   "source": [
    "#### refactor and make it like a chat interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bac490d6-a950-4287-9c51-e5208cf67aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    boost = {'question': 3.0, 'section': 0.5}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={'course': 'data-engineering-zoomcamp'},\n",
    "        boost_dict=boost,\n",
    "        num_results=5,\n",
    "        output_ids=True\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77b3b7b4-5693-49ca-a119-8703c1bb5363",
   "metadata": {},
   "outputs": [],
   "source": [
    "search_tool = {\n",
    "    \"type\" : \"function\",\n",
    "    \"name\" : \"search\",\n",
    "    \"description\" : \"Search the FAQ database\",\n",
    "    \"parameters\" : {\n",
    "        \"type\" : \"object\",\n",
    "        \"properties\" : {\n",
    "            \"query\" : {\n",
    "                \"type\" : \"string\",\n",
    "                \"description\" : \"Search query text to look up in the course FAQ.\"\n",
    "            }\n",
    "        },\n",
    "        \"required\" : [\"query\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "110ebf9e-1b88-4682-99ee-514e50009bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_call(tool_call_response):\n",
    "    function_name = tool_call_response.name\n",
    "    arguments = json.loads(tool_call_response.arguments)\n",
    "\n",
    "    f = globals()[function_name]\n",
    "    result = f(**arguments)\n",
    "\n",
    "    return {\n",
    "        \"type\" : \"function_call_output\",\n",
    "        \"call_id\" :tool_call_response.call_id,\n",
    "        \"output\" : json.dumps(result, indent=2),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "470ce66f-2902-431d-aaff-7778478e2c75",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ResponseFunctionToolCall(arguments='{\"query\":\"how to do well in module 1\"}', call_id='call_o44gqRJ16NwuOqgEHZft8cgB', name='search', type='function_call', id='fc_68a429a9a5c08194ab642276ca29ecc50a874687391c6768', status='completed'),\n",
       " ResponseFunctionToolCall(arguments='{\"query\":\"tips for success in module 1\"}', call_id='call_bWvunwnrR8itpoKkG1FTuZzb', name='search', type='function_call', id='fc_68a429a9f6388194a17be362d6171cf30a874687391c6768', status='completed')]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "question = \"how do I do well in module 1?\"\n",
    "\n",
    "developer_prompt = \"\"\"\n",
    "You're a course teaching assistant.\n",
    "You're given a question from a course student and your task is to answer it.\n",
    "If you look up something in FAQ, convert the student question into multiple queries.\n",
    "\"\"\".strip()\n",
    "\n",
    "tools = [search_tool]\n",
    "\n",
    "chat_messages = [\n",
    "    {\"role\" : \"developer\", \"content\" : developer_prompt},\n",
    "    {\"role\" : \"user\", \"content\" : question}\n",
    "]\n",
    "\n",
    "response = client.responses.create(\n",
    "    model = 'gpt-4o-mini',\n",
    "    input = chat_messages,\n",
    "    tools = tools\n",
    ")\n",
    "response.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "48d438f9-a90d-46ba-9e2e-16cf0c803b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "for call in calls:\n",
    "    result = do_call(call)\n",
    "    chat_messages.append(call)\n",
    "    chat_messages.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2018b5df-85c6-4b6e-96c7-816b7c77ad1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.responses.create(\n",
    "    model = 'gpt-4o-mini',\n",
    "    input = chat_messages,\n",
    "    tools = tools\n",
    ")\n",
    "response.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f1712f-98b1-41d4-a6c8-00236f561824",
   "metadata": {},
   "outputs": [],
   "source": [
    "#We dont know in advance what kind of output our function call will give us\n",
    "#it might do another function call or give the final answer or result\n",
    "#so check first\n",
    "\n",
    "for entry in response.output:\n",
    "    chat_messages.append(entry)\n",
    "    print(entry.type)\n",
    "\n",
    "    if entry.type == 'function_call':\n",
    "        result = do_call(entry)\n",
    "        chat_messages.append(result)\n",
    "    elif entry.typ == 'message':\n",
    "        print(entry.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d08ca5a4-099d-48e2-ae2b-4664f73913f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "developer_prompt = \"\"\"\n",
    "You're a course teaching assistant.\n",
    "You're given a question from a course student and your task is to answer it.\n",
    "\n",
    "Use FAQ if your own knowledge is not sufficient to answer the question.\n",
    "when using FAQ, perform deep topic exploration: make one request to FAQ,\n",
    "and then based on the results, make more requests.\n",
    "\n",
    "At the end of each response, ask the user a follow up question based on your answer.\n",
    "\"\"\".strip()\n",
    "\n",
    "chat_messages = [\n",
    "    {\"role\": \"developer\", \"content\": developer_prompt},\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb3f340-407a-4433-afb3-95c3b03ca5a4",
   "metadata": {},
   "source": [
    "### Chatbot like interface"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5b69da82-3477-4f1a-b29d-1b20ff1fe26d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " how do I do well in module 1?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function_call: ResponseFunctionToolCall(arguments='{\"query\":\"do well in module 1\"}', call_id='call_LNQJxidfCkHOd4oPrxCe8ndZ', name='search', type='function_call', id='fc_68a504b8998081908475119ac00933910c6579135a790449', status='completed')\n",
      "\n",
      "function_call: ResponseFunctionToolCall(arguments='{\"query\":\"tips for success in module 1\"}', call_id='call_8t4UDtkbZwOYw1HIrmlVgVWD', name='search', type='function_call', id='fc_68a504b9bc8881908fd4a4588ae110a20c6579135a790449', status='completed')\n",
      "\n",
      "To do well in Module 1, which covers Docker and Terraform as part of the Data Engineering Zoomcamp, here are some key tips:\n",
      "\n",
      "1. **Understand Key Concepts**: Familiarize yourself with the essentials of Docker and Terraform. Focus on what containers are, how orchestration works, and the syntax and usage of Terraform scripts.\n",
      "\n",
      "2. **Hands-On Practice**: Engage deeply with the practical exercises provided in the module. Setting up Docker containers and writing Terraform scripts will solidify your understanding.\n",
      "\n",
      "3. **Install Required Dependencies**: Ensure that you have all necessary Python packages installed correctly. For instance, if you encounter a `ModuleNotFoundError`, make sure you install any missing modules, such as `psycopg2` using:\n",
      "   ```bash\n",
      "   pip install psycopg2-binary\n",
      "   ```\n",
      "   If issues persist, consider updating your package manager or reinstalling the required package.\n",
      "\n",
      "4. **Set Up Your Environment**: Properly configure your development environment. This can mean ensuring Docker is running and your Python environment is set up correctly.\n",
      "\n",
      "5. **Review Error Messages**: When you encounter errors (like `ModuleNotFoundError`), feedback and solutions from peers can help. For instance, understanding how to correctly use SQLAlchemy with the PostgreSQL connection string can prevent frustrating errors.\n",
      "\n",
      "6. **Refer to Resource Materials**: Utilize course materials and online resources to clarify any doubts. Don’t hesitate to refer back to documentation or tutorials on Docker and Terraform.\n",
      "\n",
      "7. **Ask for Help**: If you're stuck, reach out to fellow students or instructors for clarification. Collaborating and discussing topics with others can enhance your understanding.\n",
      "\n",
      "8. **Stay Consistent**: Allocate regular time each week to study and practice. Regular engagement with the content helps reinforce learning.\n",
      "\n",
      "Would you like specific advice on any of these tips or do you have a particular aspect of the module you want to focus on?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " Docker\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function_call: ResponseFunctionToolCall(arguments='{\"query\":\"Docker tips for success in module 1\"}', call_id='call_NCZAmV3AvqmhPUKsB54g4EEx', name='search', type='function_call', id='fc_68a504f030948190bc9b06647bb6c5a60c6579135a790449', status='completed')\n",
      "\n",
      "To excel in Docker as part of Module 1, consider the following tips:\n",
      "\n",
      "1. **Installation and Setup**:\n",
      "   - Ensure Docker is correctly installed on your machine. Verify that Docker is running smoothly by executing `docker --version` and `docker-compose --version`. \n",
      "\n",
      "2. **Understand Docker Basics**:\n",
      "   - Familiarize yourself with fundamental Docker concepts like images, containers, volumes, and networks. This foundational knowledge will aid in understanding more complex operations later in your learning.\n",
      "\n",
      "3. **Use Docker-Compose**:\n",
      "   - Make use of `docker-compose` to manage multi-container applications. Write clear `docker-compose.yml` files defining your services, volumes, and networks.\n",
      "\n",
      "4. **Debugging**:\n",
      "   - If you encounter errors, like `ModuleNotFoundError`, ensure that all required modules are specified in your Dockerfile. For Python apps, remember to add necessary libraries like this:\n",
      "     ```dockerfile\n",
      "     RUN python -m pip install --no-cache-dir <module_name>\n",
      "     ```\n",
      "   - An example includes installing `pytz` or `psycopg2`, which are common dependencies in data engineering projects.\n",
      "\n",
      "5. **Testing Your Containers**:\n",
      "   - After building your Docker images, test your containers thoroughly. Run your application inside the container to ensure that it works as expected.\n",
      "\n",
      "6. **Using Docker Commands**: \n",
      "   - Get comfortable with essential Docker commands:\n",
      "     - `docker build`: to build an image from a Dockerfile.\n",
      "     - `docker run`: to run a container from an image.\n",
      "     - `docker ps`: to list running containers.\n",
      "     - `docker exec`: to execute commands in a running container (useful for debugging).\n",
      "\n",
      "7. **Stay Organized**:\n",
      "   - Keep your Dockerfiles and configurations organized. Use comments to clarify complex sections, which will help you and your peers understand your setup.\n",
      "\n",
      "8. **Seek Help and Resources**:\n",
      "   - Utilize community forums, documentation, and course materials for any doubts. Engaging with peers can also provide new insights.\n",
      "\n",
      "9. **Practice Consistently**:\n",
      "   - Regularly engage with Docker-based projects or exercises to reinforce your skills and understanding.\n",
      "\n",
      "Do you have any specific questions about Docker, or is there a certain area you would like to explore further?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " file permissions \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "function_call: ResponseFunctionToolCall(arguments='{\"query\":\"Docker file permissions issues\"}', call_id='call_hzTwUf7JiYZZf3GCrz7EntsD', name='search', type='function_call', id='fc_68a50550e25c819090c1567e942acfc50c6579135a790449', status='completed')\n",
      "\n",
      "When working with Docker, especially in environments like WSL (Windows Subsystem for Linux), file permissions can be a common issue. Here are some tips to handle file permissions effectively in Docker:\n",
      "\n",
      "### Common Issues and Solutions\n",
      "\n",
      "1. **Permission Errors with Mounted Volumes**:\n",
      "   - If you encounter errors like `could not change permissions of directory /var/lib/postgresql/data`, it may be due to the way Windows handles permissions compared to Linux.\n",
      "   - **Solution**: Utilize Docker volumes instead of binding mounts. Create a named volume in Docker:\n",
      "     ```bash\n",
      "     docker volume create my_volume_name\n",
      "     ```\n",
      "     Then, reference this volume in your `docker-compose.yml` or `docker run` command.\n",
      "\n",
      "2. **Check User Permissions**:\n",
      "   - Ensure that the user running the Docker command has the correct permissions. You might need to specify the user in the Docker container:\n",
      "     ```yaml\n",
      "     user: \"0:0\"  # Run as root user inside the container\n",
      "     ```\n",
      "   - This is particularly helpful for database services where ownership matters.\n",
      "\n",
      "3. **Use Appropriate Filesystem**:\n",
      "   - Avoid using Windows filesystem paths for Docker mounts. Instead, use `Docker volumes` to prevent permission conflicts between Windows and Linux environments:\n",
      "     ```yaml\n",
      "     volumes:\n",
      "       - pg-data:/var/lib/postgresql/data\n",
      "     ```\n",
      "\n",
      "4. **File Permission Settings**:\n",
      "   - If you need to change permissions of files within the Docker container after mapping, you can use the `chmod` command. \n",
      "   - For example, if you have issues with SSH keys, ensure they're in the correct permission format:\n",
      "     ```bash\n",
      "     chmod 600 ~/.ssh/id_rsa  # For private keys\n",
      "     ```\n",
      "\n",
      "5. **Debugging Permission Issues**:\n",
      "   - Check the current permissions in the container by running:\n",
      "     ```bash\n",
      "     docker exec -it <container_id> bash\n",
      "     ls -l /path/to/your/directory\n",
      "     ```\n",
      "   - This command can help you understand what permissions are currently set and if they need modification.\n",
      "\n",
      "6. **Using Sudo with SSH Keys**:\n",
      "   - If you're facing permission issues while using SSH with keys, ensure that the keys’ permissions are correctly set or try using `sudo` to elevate privileges if necessary.\n",
      "\n",
      "7. **AWS WSL2 Users**:\n",
      "   - If you're using WSL2 and running into permission issues, check that you are executing Docker commands from within the correct Linux environment instead of directly from Windows.\n",
      "\n",
      "### Summary\n",
      "To avoid file permission issues in Docker, prioritize using Docker volumes, check which user owns a given directory, and ensure proper permissions are set on any files shared between your host and containers.\n",
      "\n",
      "Do you have a specific permission issue you're encountering, or would you like more details on any of the solutions I mentioned?\n",
      "\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      " stop\n"
     ]
    }
   ],
   "source": [
    "while True: #main Q&A loop\n",
    "    question = input()\n",
    "    if question == 'stop':\n",
    "        break\n",
    "\n",
    "    message = {\"role\" : \"user\", \"content\": question}\n",
    "    chat_messages.append(message)\n",
    "\n",
    "    while True:#request-response loop - query API till get a message\n",
    "        response = client.responses.create(\n",
    "            model = 'gpt-4o-mini',\n",
    "            input = chat_messages,\n",
    "            tools = tools\n",
    "        )\n",
    "\n",
    "        has_tool_calls = False\n",
    "\n",
    "        for entry in response.output:\n",
    "            chat_messages.append(entry)\n",
    "\n",
    "            if entry.type == 'function_call':\n",
    "                print('function_call:', entry)\n",
    "                print()\n",
    "                result = do_call(entry)\n",
    "                chat_messages.append(result)\n",
    "                has_tool_calls = True\n",
    "                \n",
    "            elif entry.type == 'message':\n",
    "                print(entry.content[0].text)\n",
    "                print()\n",
    "                \n",
    "        if not has_tool_calls:\n",
    "            break\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b519749a-0021-4865-9e2c-2ad37c2dfd23",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "\n",
    "PATH = Path().resolve().parent\n",
    "\n",
    "if str(PATH) not in sys.path:\n",
    "    sys.path.append(str(PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c47a8459-3a6c-4f0e-983b-e365c64aa776",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'function',\n",
       "  'name': 'search',\n",
       "  'description': 'Search the FAQ database',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'query': {'type': 'string',\n",
       "     'description': 'Search query text to look up in the course FAQ.'}},\n",
       "   'required': ['query'],\n",
       "   'additionalProperties': False}}]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from script import chat_assistant\n",
    "\n",
    "tools = chat_assistant.Tools()\n",
    "tools.add_tool(search, search_tool)\n",
    "\n",
    "tools.get_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f2f6ac68-a710-4bdf-a2f8-f630f8acef2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "developer_prompt = \"\"\"\n",
    "You're a course teaching assistant. \n",
    "You're given a question from a course student and your task is to answer it.\n",
    "\n",
    "Use FAQ if your own knowledge is not sufficient to answer the question.\n",
    "\n",
    "At the end of each response, ask the user a follow up question based on your answer.\n",
    "\"\"\".strip()\n",
    "\n",
    "chat_interface = chat_assistant.chatInterface()\n",
    "chat = chat_assistant.ChatAssistant(\n",
    "    tools = tools, \n",
    "    developer_prompt = developer_prompt,\n",
    "    chat_interface = chat_interface,\n",
    "    client = client,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d10ff8d2-7089-4d71-9882-b53fe8860c2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: how to excel in data engineering?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search({\"query\":\"how to excel in data engineering\"})</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>ResponseFunctionToolCall(arguments='{\"query\":\"how to excel in data engineering\"}', call_id='call_gpvRZK8iVQkDrmqmeI6nIcTl', name='search', type='function_call', id='fc_68a80b051144819088a6958c2985b6470cae1d4c56816968', status='completed')</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>[\n",
       "  {\n",
       "    \"text\": \"Assuming you downloaded the Mage repo in the week 2 folder of the Data Engineering Zoomcamp, you might want to include your mage copy, demo pipelines and homework within your personal copy of the Data Engineering Zoomcamp repo. This will not work by default, because GitHub sees them as two separate repositories, and one does not track the other. To add the Mage files to your main DE Zoomcamp repo, you will need to:\\nMove the contents of the .gitignore file in your main .gitignore.\\nUse the terminal to cd into the Mage folder and:\\nrun \\u201cgit remote remove origin\\u201d to de-couple the Mage repo,\\nrun \\u201crm -rf .git\\u201d to delete local git files,\\nrun \\u201cgit add .\\u201d to add the current folder as changes to stage, commit and push.\",\n",
       "    \"section\": \"Module 2: Workflow Orchestration\",\n",
       "    \"question\": \"Git - How do I include the files in the Mage repo (including exercise files and homework) in a personal copy of the Data Engineering Zoomcamp repo?\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 191\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"Copy the file found in the Java example: data-engineering-zoomcamp/week_6_stream_processing/java/kafka_examples/src/main/resources/rides.csv\",\n",
       "    \"section\": \"Module 6: streaming with kafka\",\n",
       "    \"question\": \"data-engineering-zoomcamp/week_6_stream_processing/python/resources/rides.csv is missing\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 377\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"This error appeared when running the command: docker build -t taxi_ingest:v001 .\\nWhen feeding the database with the data the user id of the directory ny_taxi_postgres_data was changed to 999, so my user couldn\\u2019t access it when running the above command. Even though this is not the problem here it helped to raise the error due to the permission issue.\\nSince at this point we only need the files Dockerfile and ingest_data.py, to fix this error one can run the docker build command on a different directory (having only these two files).\\nA more complete explanation can be found here: https://stackoverflow.com/questions/41286028/docker-build-error-checking-context-cant-stat-c-users-username-appdata\\nYou can fix the problem by changing the permission of the directory on ubuntu with following command:\\nsudo chown -R $USER dir_path\\nOn windows follow the link: https://thegeekpage.com/take-ownership-of-a-file-folder-through-command-prompt-in-windows-10/ \\n\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tAdded by\\n\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tKenan Arslanbay\",\n",
       "    \"section\": \"Module 1: Docker and Terraform\",\n",
       "    \"question\": \"Docker - build error: error checking context: 'can't stat '/home/user/repos/data-engineering/week_1_basics_n_setup/2_docker_sql/ny_taxi_postgres_data''.\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 67\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"You don't need it. You're accepted. You can also just start learning and submitting homework without registering. It is not checked against any registered list. Registration is just to gauge interest before the start date.\",\n",
       "    \"section\": \"General course-related questions\",\n",
       "    \"question\": \"Course - I have registered for the Data Engineering Bootcamp. When can I expect to receive the confirmation email?\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 3\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"I initially followed data-engineering-zoomcamp/03-data-warehouse/extras/web_to_gcs.py at main \\u00b7 DataTalksClub/data-engineering-bootcamp (github.com)\\nBut it was taking forever for the yellow trip data and when I tried to download and upload the parquet files directly to GCS, that works fine but when creating the Bigquery table, there was a schema inconsistency issue\\nThen I found another hack shared in the slack which was suggested by Victoria.\\n[Optional] Hack for loading data to BigQuery for Week 4 - YouTube\\nPlease watch until the end as there is few schema changes required to be done\",\n",
       "    \"section\": \"Module 4: analytics engineering with dbt\",\n",
       "    \"question\": \"Hack to load yellow and green trip data for 2019 and 2020\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 251\n",
       "  }\n",
       "]</pre>\n",
       "            </div>\n",
       "\n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>To excel in data engineering, here are some key strategies:</p>\n",
       "<ol>\n",
       "<li>\n",
       "<p><strong>Strong Foundation in Programming and Database Management</strong>: Proficiency in programming languages like Python or Java, and knowledge of SQL and NoSQL databases are essential.</p>\n",
       "</li>\n",
       "<li>\n",
       "<p><strong>Learn Data Warehousing and ETL Processes</strong>: Understand how data is collected, transformed, and stored. Familiarizing yourself with ETL tools and data warehousing concepts is critical.</p>\n",
       "</li>\n",
       "<li>\n",
       "<p><strong>Hands-on Experience with Data Pipelines</strong>: Build your own data pipelines using tools like Apache Kafka, Apache Airflow, or other workflow orchestration tools to automate processes.</p>\n",
       "</li>\n",
       "<li>\n",
       "<p><strong>Familiarity with Big Data Technologies</strong>: Gain experience with big data technologies such as Hadoop, Spark, and cloud platforms like AWS, GCP, or Azure.</p>\n",
       "</li>\n",
       "<li>\n",
       "<p><strong>Stay Updated with Industry Trends</strong>: Follow data engineering blogs, attend webinars, and participate in community forums to keep yourself informed about the latest tools and practices.</p>\n",
       "</li>\n",
       "<li>\n",
       "<p><strong>Work on Real Projects</strong>: Apply your skills in real-world scenarios through internships, freelance work, or personal projects. This hands-on experience is invaluable.</p>\n",
       "</li>\n",
       "<li>\n",
       "<p><strong>Develop a Data Mindset</strong>: Understand business requirements and focus on deriving actionable insights from data. This can enhance your ability to work with stakeholders effectively.</p>\n",
       "</li>\n",
       "</ol>\n",
       "<p>Would you like specific resources or courses to help you get started on any of these points?</p></div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: stop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat ended\n"
     ]
    }
   ],
   "source": [
    "chat.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52cd8874-cb5f-41df-9f49-9b766804586b",
   "metadata": {},
   "source": [
    "#### Multiple tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "02d9022a-5e17-427b-a154-8dd56f944eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_entry(question, answer):\n",
    "    doc = {\n",
    "        'question': question,\n",
    "        'text' : answer,\n",
    "        'section' : 'user_added',\n",
    "        'course' : 'data-engineering-zoomcamp'\n",
    "    }\n",
    "    index.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "aba4050b-c822-4ac4-a6b3-411c92aa73cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_entry_description = {\n",
    "    \"type\" : \"function\",\n",
    "    \"name\" : \"add_entry\",\n",
    "    \"description\" : \"Add an entry to the FAQ database\",\n",
    "    \"parameters\" : {\n",
    "        \"type\" : \"object\",\n",
    "        \"properties\" : {\n",
    "            \"question\" : {\n",
    "                \"type\" : \"string\",\n",
    "                \"description\" : \"The question to be added to the FAQ database\",\n",
    "            },\n",
    "            \"answer\" : {\n",
    "                \"type\" : \"string\",\n",
    "                \"description\" : \"The answer to the question\",\n",
    "            }\n",
    "        },\n",
    "        \"required\" : [\"question\", \"answer\"],\n",
    "        \"additionalProperties\" : False\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eef02582-5cba-4a3b-ab3d-9655a7646a83",
   "metadata": {},
   "outputs": [],
   "source": [
    "tools.add_tool(add_entry, add_entry_description)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d4e3ebf-5de5-4bee-9b55-d5f425a01301",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'type': 'function',\n",
       "  'name': 'search',\n",
       "  'description': 'Search the FAQ database',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'query': {'type': 'string',\n",
       "     'description': 'Search query text to look up in the course FAQ.'}},\n",
       "   'required': ['query'],\n",
       "   'additionalProperties': False}},\n",
       " {'type': 'function',\n",
       "  'name': 'add_entry',\n",
       "  'description': 'Add an entry to the FAQ database',\n",
       "  'parameters': {'type': 'object',\n",
       "   'properties': {'question': {'type': 'string',\n",
       "     'description': 'The question to be added to the FAQ database'},\n",
       "    'answer': {'type': 'string', 'description': 'The answer to the question'}},\n",
       "   'required': ['question', 'answer'],\n",
       "   'additionalProperties': False}}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tools.get_tools()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b9ac8a0-04d2-4741-a46a-c1159bc2f687",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: how to master Docker permisions?\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search({\"query\":\"Docker permissions\"})</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>ResponseFunctionToolCall(arguments='{\"query\":\"Docker permissions\"}', call_id='call_yjoNXHWLyKirxtFtfLB8fnpB', name='search', type='function_call', id='fc_68a8113e201c81908528c39fb154975f07bbb7cb1cad6ed8', status='completed')</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>[\n",
       "  {\n",
       "    \"text\": \"Error:  initdb: error: could not change permissions of directory\\nIssue: WSL and Windows do not manage permissions in the same way causing conflict if using the Windows file system rather than the WSL file system.\\nSolution: Use Docker volumes.\\nWhy: Volume is used for storage of persistent data and not for use of transferring files. A local volume is unnecessary.\\nBenefit: This resolves permission issues and allows for better management of volumes.\\nNOTE: the \\u2018user:\\u2019 is not necessary if using docker volumes, but is if using local drive.\\n</>  docker-compose.yaml\\nservices:\\npostgres:\\nimage: postgres:15-alpine\\ncontainer_name: postgres\\nuser: \\\"0:0\\\"\\nenvironment:\\n- POSTGRES_USER=postgres\\n- POSTGRES_PASSWORD=postgres\\n- POSTGRES_DB=ny_taxi\\nvolumes:\\n- \\\"pg-data:/var/lib/postgresql/data\\\"\\nports:\\n- \\\"5432:5432\\\"\\nnetworks:\\n- pg-network\\npgadmin:\\nimage: dpage/pgadmin4\\ncontainer_name: pgadmin\\nuser: \\\"${UID}:${GID}\\\"\\nenvironment:\\n- PGADMIN_DEFAULT_EMAIL=email@some-site.com\\n- PGADMIN_DEFAULT_PASSWORD=pgadmin\\nvolumes:\\n- \\\"pg-admin:/var/lib/pgadmin\\\"\\nports:\\n- \\\"8080:80\\\"\\nnetworks:\\n- pg-network\\nnetworks:\\npg-network:\\nname: pg-network\\nvolumes:\\npg-data:\\nname: ingest_pgdata\\npg-admin:\\nname: ingest_pgadmin\",\n",
       "    \"section\": \"Module 1: Docker and Terraform\",\n",
       "    \"question\": \"WSL Docker directory permissions error\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 92\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"$ docker run -it\\\\\\n-e POSTGRES_USER=\\\"root\\\" \\\\\\n-e POSTGRES_PASSWORD=\\\"admin\\\" \\\\\\n-e POSTGRES_DB=\\\"ny_taxi\\\" \\\\\\n-v \\\"/mnt/path/to/ny_taxi_postgres_data\\\":\\\"/var/lib/postgresql/data\\\" \\\\\\n-p 5432:5432 \\\\\\npostgres:13\\nCCW\\nThe files belonging to this database system will be owned by user \\\"postgres\\\".\\nThis use The database cluster will be initialized with locale \\\"en_US.utf8\\\".\\nThe default databerrorase encoding has accordingly been set to \\\"UTF8\\\".\\nxt search configuration will be set to \\\"english\\\".\\nData page checksums are disabled.\\nfixing permissions on existing directory /var/lib/postgresql/data ... initdb: f\\nerror: could not change permissions of directory \\\"/var/lib/postgresql/data\\\": Operation not permitted  volume\\nOne way to solve this issue is to create a local docker volume and map it to postgres data directory /var/lib/postgresql/data\\nThe input dtc_postgres_volume_local must match in both commands below\\n$ docker volume create --name dtc_postgres_volume_local -d local\\n$ docker run -it\\\\\\n-e POSTGRES_USER=\\\"root\\\" \\\\\\n-e POSTGRES_PASSWORD=\\\"root\\\" \\\\\\n-e POSTGRES_DB=\\\"ny_taxi\\\" \\\\\\n-v dtc_postgres_volume_local:/var/lib/postgresql/data \\\\\\n-p 5432:5432\\\\\\npostgres:13\\nTo verify the above command works in (WSL2 Ubuntu 22.04, verified 2024-Jan), go to the Docker Desktop app and look under Volumes - dtc_postgres_volume_local would be listed there. The folder ny_taxi_postgres_data would however be empty, since we used an alternative config.\\nAn alternate error could be:\\ninitdb: error: directory \\\"/var/lib/postgresql/data\\\" exists but is not empty\\nIf you want to create a new database system, either remove or empthe directory \\\"/var/lib/postgresql/data\\\" or run initdb\\nwitls\",\n",
       "    \"section\": \"Module 1: Docker and Terraform\",\n",
       "    \"question\": \"1Docker - Could not change permissions of directory \\\"/var/lib/postgresql/data\\\": Operation not permitted\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 63\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"Issue when trying to run the GPC VM through SSH through WSL2,  probably because WSL2 isn\\u2019t looking for .ssh keys in the correct folder. My case I was trying to run this command in the terminal and getting an error\\nPC:/mnt/c/Users/User/.ssh$ ssh -i gpc [username]@[my external IP]\\nYou can try to use sudo before the command\\nSudo .ssh$ ssh -i gpc [username]@[my external IP]\\nYou can also try to cd to your folder and change the permissions for the private key SSH file.\\nchmod 600 gpc\\nIf that doesn\\u2019t work, create a .ssh folder in the home diretory of WSL2 and copy the content of windows .ssh folder to that new folder.\\ncd ~\\nmkdir .ssh\\ncp -r /mnt/c/Users/YourUsername/.ssh/* ~/.ssh/\\nYou might need to adjust the permissions of the files and folders in the .ssh directory.\",\n",
       "    \"section\": \"Module 1: Docker and Terraform\",\n",
       "    \"question\": \"WSL - Permissions too open at Windows\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 96\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"This happens on Ubuntu/Linux systems when trying to run the command to build the Docker container again.\\n$ docker build -t taxi_ingest:v001 .\\nA folder is created to host the Docker files. When the build command is executed again to rebuild the pipeline or create a new one the error is raised as there are no permissions on this new folder. Grant permissions by running this comtionmand;\\n$ sudo chmod -R 755 ny_taxi_postgres_data\\nOr use 777 if you still see problems. 755 grants write access to only the owner.\",\n",
       "    \"section\": \"Module 1: Docker and Terraform\",\n",
       "    \"question\": \"Docker - failed to solve with frontend dockerfile.v0: failed to read dockerfile: error from sender: open ny_taxi_postgres_data: permission denied.\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 70\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"error: could not change permissions of directory \\\"/var/lib/postgresql/data\\\": Operation not permitted  volume\\nif you have used the prev answer (just before this) and have created a local docker volume, then you need to tell the compose file about the named volume:\\nvolumes:\\ndtc_postgres_volume_local:  # Define the named volume here\\n# services mentioned in the compose file auto become part of the same network!\\nservices:\\nyour remaining code here . . .\\nnow use docker volume inspect dtc_postgres_volume_local to see the location by checking the value of Mountpoint\\nIn my case, after i ran docker compose up the mounting dir created was named \\u2018docker_sql_dtc_postgres_volume_local\\u2019 whereas it should have used the already existing \\u2018dtc_postgres_volume_local\\u2019\\nAll i did to fix this is that I renamed the existing \\u2018dtc_postgres_volume_local\\u2019 to \\u2018docker_sql_dtc_postgres_volume_local\\u2019 and removed the newly created one (just be careful when doing this)\\nrun docker compose up again and check if the table is there or not!\",\n",
       "    \"section\": \"Module 1: Docker and Terraform\",\n",
       "    \"question\": \"Docker-Compose - mounting error\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 78\n",
       "  }\n",
       "]</pre>\n",
       "            </div>\n",
       "\n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>To master Docker permissions, focus on several key areas:</p>\n",
       "<ol>\n",
       "<li>\n",
       "<p><strong>Understanding User and Group IDs</strong>: Docker containers run as root by default, which can lead to permission issues, especially when sharing volumes with the host file system. Using the <code>user</code> directive can help specify a non-root user.</p>\n",
       "</li>\n",
       "<li>\n",
       "<p><strong>Using Docker Volumes</strong>: When you encounter permission problems, especially on systems like WSL or Windows, it's often best to use Docker volumes instead of bind mounts. Volumes manage permissions better and can resolve many of the issues that arise from using the host file system.</p>\n",
       "</li>\n",
       "<li>\n",
       "<p><strong>Setting Correct Permissions</strong>: If you have issues related to directory permissions, ensure the directories and files have the right permissions set. You can modify permissions using commands like <code>chmod</code>, e.g., <code>chmod 755 directory_name</code>.</p>\n",
       "</li>\n",
       "<li>\n",
       "<p><strong>Check Docker Compose Configuration</strong>: If using Docker Compose, ensure your volume configuration is correctly set up in your <code>docker-compose.yml</code> file. This includes properly defining any named volumes and ensuring they are referenced in your services.</p>\n",
       "</li>\n",
       "<li>\n",
       "<p><strong>Testing and Debugging</strong>: Use commands like <code>docker volume inspect</code> and <code>docker logs</code> to troubleshoot permission issues. </p>\n",
       "</li>\n",
       "</ol>\n",
       "<p>Focusing on these aspects will help you develop a strong understanding of Docker permissions.</p>\n",
       "<p>Would you like to dive deeper into any specific area related to Docker permissions?</p></div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: file permisions with the host and mounting\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search({\"query\":\"file permissions with host and mounti...)</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>ResponseFunctionToolCall(arguments='{\"query\":\"file permissions with host and mounting in Docker\"}', call_id='call_ufHjBHiE7cYlCb4pxhSOEHEn', name='search', type='function_call', id='fc_68a811966420819093523f72a3b23bad07bbb7cb1cad6ed8', status='completed')</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>[\n",
       "  {\n",
       "    \"text\": \"error: could not change permissions of directory \\\"/var/lib/postgresql/data\\\": Operation not permitted  volume\\nif you have used the prev answer (just before this) and have created a local docker volume, then you need to tell the compose file about the named volume:\\nvolumes:\\ndtc_postgres_volume_local:  # Define the named volume here\\n# services mentioned in the compose file auto become part of the same network!\\nservices:\\nyour remaining code here . . .\\nnow use docker volume inspect dtc_postgres_volume_local to see the location by checking the value of Mountpoint\\nIn my case, after i ran docker compose up the mounting dir created was named \\u2018docker_sql_dtc_postgres_volume_local\\u2019 whereas it should have used the already existing \\u2018dtc_postgres_volume_local\\u2019\\nAll i did to fix this is that I renamed the existing \\u2018dtc_postgres_volume_local\\u2019 to \\u2018docker_sql_dtc_postgres_volume_local\\u2019 and removed the newly created one (just be careful when doing this)\\nrun docker compose up again and check if the table is there or not!\",\n",
       "    \"section\": \"Module 1: Docker and Terraform\",\n",
       "    \"question\": \"Docker-Compose - mounting error\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 78\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"Error:  initdb: error: could not change permissions of directory\\nIssue: WSL and Windows do not manage permissions in the same way causing conflict if using the Windows file system rather than the WSL file system.\\nSolution: Use Docker volumes.\\nWhy: Volume is used for storage of persistent data and not for use of transferring files. A local volume is unnecessary.\\nBenefit: This resolves permission issues and allows for better management of volumes.\\nNOTE: the \\u2018user:\\u2019 is not necessary if using docker volumes, but is if using local drive.\\n</>  docker-compose.yaml\\nservices:\\npostgres:\\nimage: postgres:15-alpine\\ncontainer_name: postgres\\nuser: \\\"0:0\\\"\\nenvironment:\\n- POSTGRES_USER=postgres\\n- POSTGRES_PASSWORD=postgres\\n- POSTGRES_DB=ny_taxi\\nvolumes:\\n- \\\"pg-data:/var/lib/postgresql/data\\\"\\nports:\\n- \\\"5432:5432\\\"\\nnetworks:\\n- pg-network\\npgadmin:\\nimage: dpage/pgadmin4\\ncontainer_name: pgadmin\\nuser: \\\"${UID}:${GID}\\\"\\nenvironment:\\n- PGADMIN_DEFAULT_EMAIL=email@some-site.com\\n- PGADMIN_DEFAULT_PASSWORD=pgadmin\\nvolumes:\\n- \\\"pg-admin:/var/lib/pgadmin\\\"\\nports:\\n- \\\"8080:80\\\"\\nnetworks:\\n- pg-network\\nnetworks:\\npg-network:\\nname: pg-network\\nvolumes:\\npg-data:\\nname: ingest_pgdata\\npg-admin:\\nname: ingest_pgadmin\",\n",
       "    \"section\": \"Module 1: Docker and Terraform\",\n",
       "    \"question\": \"WSL Docker directory permissions error\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 92\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"$ docker run -it\\\\\\n-e POSTGRES_USER=\\\"root\\\" \\\\\\n-e POSTGRES_PASSWORD=\\\"admin\\\" \\\\\\n-e POSTGRES_DB=\\\"ny_taxi\\\" \\\\\\n-v \\\"/mnt/path/to/ny_taxi_postgres_data\\\":\\\"/var/lib/postgresql/data\\\" \\\\\\n-p 5432:5432 \\\\\\npostgres:13\\nCCW\\nThe files belonging to this database system will be owned by user \\\"postgres\\\".\\nThis use The database cluster will be initialized with locale \\\"en_US.utf8\\\".\\nThe default databerrorase encoding has accordingly been set to \\\"UTF8\\\".\\nxt search configuration will be set to \\\"english\\\".\\nData page checksums are disabled.\\nfixing permissions on existing directory /var/lib/postgresql/data ... initdb: f\\nerror: could not change permissions of directory \\\"/var/lib/postgresql/data\\\": Operation not permitted  volume\\nOne way to solve this issue is to create a local docker volume and map it to postgres data directory /var/lib/postgresql/data\\nThe input dtc_postgres_volume_local must match in both commands below\\n$ docker volume create --name dtc_postgres_volume_local -d local\\n$ docker run -it\\\\\\n-e POSTGRES_USER=\\\"root\\\" \\\\\\n-e POSTGRES_PASSWORD=\\\"root\\\" \\\\\\n-e POSTGRES_DB=\\\"ny_taxi\\\" \\\\\\n-v dtc_postgres_volume_local:/var/lib/postgresql/data \\\\\\n-p 5432:5432\\\\\\npostgres:13\\nTo verify the above command works in (WSL2 Ubuntu 22.04, verified 2024-Jan), go to the Docker Desktop app and look under Volumes - dtc_postgres_volume_local would be listed there. The folder ny_taxi_postgres_data would however be empty, since we used an alternative config.\\nAn alternate error could be:\\ninitdb: error: directory \\\"/var/lib/postgresql/data\\\" exists but is not empty\\nIf you want to create a new database system, either remove or empthe directory \\\"/var/lib/postgresql/data\\\" or run initdb\\nwitls\",\n",
       "    \"section\": \"Module 1: Docker and Terraform\",\n",
       "    \"question\": \"1Docker - Could not change permissions of directory \\\"/var/lib/postgresql/data\\\": Operation not permitted\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 63\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"Mapping volumes on Windows could be tricky. The way it was done in the course video doesn\\u2019t work for everyone.\\nFirst, if yo\\nmove your data to some folder without spaces. E.g. if your code is in \\u201cC:/Users/Alexey Grigorev/git/\\u2026\\u201d, move it to \\u201cC:/git/\\u2026\\u201d\\nTry replacing the \\u201c-v\\u201d part with one of the following options:\\n-v /c:/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\\n-v //c:/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\\n-v /c/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\\n-v //c/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\\n--volume //driveletter/path/ny_taxi_postgres_data/:/var/lib/postgresql/data\\nwinpty docker run -it\\n-e POSTGRES_USER=\\\"root\\\"\\n-e POSTGRES_PASSWORD=\\\"root\\\"\\n-e POSTGRES_DB=\\\"ny_taxi\\\"\\n-v /c:/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\\n-p 5432:5432\\npostgres:1\\nTry adding winpty before the whole command\\n3\\nwin\\nTry adding quotes:\\n-v \\\"/c:/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\\\"\\n-v \\\"//c:/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\\\"\\n-v \\u201c/c/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\\\"\\n-v \\\"//c/some/path/ny_taxi_postgres_data:/var/lib/postgresql/data\\\"\\n-v \\\"c:\\\\some\\\\path\\\\ny_taxi_postgres_data\\\":/var/lib/postgresql/data\\nNote:  (Window) if it automatically creates a folder called \\u201cny_taxi_postgres_data;C\\u201d suggests you have problems with volume mapping, try deleting both folders and replacing \\u201c-v\\u201d part with other options. For me \\u201c//c/\\u201d works instead of \\u201c/c/\\u201d. And it will work by automatically creating a correct folder called \\u201cny_taxi_postgres_data\\u201d.\\nA possible solution to this error would be to use /\\u201d$(pwd)\\u201d/ny_taxi_postgres_data:/var/lib/postgresql/data (with quotes\\u2019 position varying as in the above list).\\nYes for windows use the command it works perfectly fine\\n-v /\\u201d$(pwd)\\u201d/ny_taxi_postgres_data:/var/lib/postgresql/data\\nImportant: note how the quotes are placed.\\nIf none of these options work, you can use a volume name instead of the path:\\n-v ny_taxi_postgres_data:/var/lib/postgresql/data\\nFor Mac: You can wrap $(pwd) with quotes like the highlighted.\\ndocker run -it \\\\\\n-e POSTGRES_USER=\\\"root\\\" \\\\\\n-e POSTGRES_PASSWORD=\\\"root\\\" \\\\\\n-e POSTGRES_DB=\\\"ny_taxi\\\" \\\\\\n-v \\\"$(pwd)\\\"/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n-p 5432:5432 \\\\\\nPostgres:13\\ndocker run -it \\\\\\n-e POSTGRES_USER=\\\"root\\\" \\\\\\n-e POSTGRES_PASSWORD=\\\"root\\\" \\\\\\n-e POSTGRES_DB=\\\"ny_taxi\\\" \\\\\\n-v \\\"$(pwd)\\\"/ny_taxi_postgres_data:/var/lib/postgresql/data \\\\\\n-p 5432:5432 \\\\\\npostgres:13\\nSource:https://stackoverflow.com/questions/48522615/docker-error-invalid-reference-format-repository-name-must-be-lowercase\",\n",
       "    \"section\": \"Module 1: Docker and Terraform\",\n",
       "    \"question\": \"Docker - invalid reference format: repository name must be lowercase (Mounting volumes with Docker on Windows)\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 64\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"Issue when trying to run the GPC VM through SSH through WSL2,  probably because WSL2 isn\\u2019t looking for .ssh keys in the correct folder. My case I was trying to run this command in the terminal and getting an error\\nPC:/mnt/c/Users/User/.ssh$ ssh -i gpc [username]@[my external IP]\\nYou can try to use sudo before the command\\nSudo .ssh$ ssh -i gpc [username]@[my external IP]\\nYou can also try to cd to your folder and change the permissions for the private key SSH file.\\nchmod 600 gpc\\nIf that doesn\\u2019t work, create a .ssh folder in the home diretory of WSL2 and copy the content of windows .ssh folder to that new folder.\\ncd ~\\nmkdir .ssh\\ncp -r /mnt/c/Users/YourUsername/.ssh/* ~/.ssh/\\nYou might need to adjust the permissions of the files and folders in the .ssh directory.\",\n",
       "    \"section\": \"Module 1: Docker and Terraform\",\n",
       "    \"question\": \"WSL - Permissions too open at Windows\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 96\n",
       "  }\n",
       "]</pre>\n",
       "            </div>\n",
       "\n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>When dealing with file permissions while mounting directories from the host in Docker, here are some important considerations:</p>\n",
       "<ol>\n",
       "<li><strong>Mount Types</strong>:</li>\n",
       "<li><strong>Bind Mounts</strong>: Use the host's directory directly. This can lead to permission issues because the user and group IDs within the container might not match those on the host. Permission issues can arise if the directory is created or owned by a different user.</li>\n",
       "<li>\n",
       "<p><strong>Docker Volumes</strong>: These are managed by Docker and typically do not have permission issues since they operate within Docker's internal storage mechanism. It's often recommended to create a Docker volume if you face permission errors with bind mounts.</p>\n",
       "</li>\n",
       "<li>\n",
       "<p><strong>Common Errors</strong>:</p>\n",
       "</li>\n",
       "<li>You might encounter the error <code>could not change permissions of directory \"/var/lib/postgresql/data\": Operation not permitted</code>. This suggests that the user executing commands in the container lacks the necessary permissions on the folder mounted from the host.</li>\n",
       "<li>\n",
       "<p>For Windows users, special care should be taken when mapping paths (e.g., using <code>/c/</code> or <code>//c/</code>), as improper formatting can lead to issues. It’s best to avoid spaces in directory names and always ensure the paths are correct.</p>\n",
       "</li>\n",
       "<li>\n",
       "<p><strong>Configuration Example</strong>:\n",
       "   To use Docker volumes effectively, you can modify your <code>docker-compose.yml</code> like this:</p>\n",
       "</li>\n",
       "</ol>\n",
       "<p><code>yaml\n",
       "   version: '3.8'\n",
       "   services:\n",
       "     postgres:\n",
       "       image: postgres:15-alpine\n",
       "       volumes:\n",
       "         - pg-data:/var/lib/postgresql/data\n",
       "   volumes:\n",
       "     pg-data:</code></p>\n",
       "<ol>\n",
       "<li><strong>Fixing Permission Problems</strong>:</li>\n",
       "<li>If you’re facing permission errors using bind mounts, consider changing the ownership or permissions of the host directory using:\n",
       "     <code>bash\n",
       "     sudo chown -R $(whoami):$(whoami) /path/to/your/directory\n",
       "     sudo chmod -R 755 /path/to/your/directory</code></li>\n",
       "<li>Also, ensure that when starting your container, it runs as a specific user that has access to that directory:\n",
       "     <code>bash\n",
       "     docker run -u \"$(id -u):$(id -g)\" -v /host/path:/container/path your_image</code></li>\n",
       "</ol>\n",
       "<p>Would you like further assistance on a specific error message or configuration regarding Docker mounts?</p></div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: add this to the FAQ database\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>add_entry({\"question\":\"How to manage file permissions whe...)</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>ResponseFunctionToolCall(arguments='{\"question\":\"How to manage file permissions when mounting directories from the host in Docker?\",\"answer\":\"1. **Mount Types**:  \\\\n   - **Bind Mounts**: Use the host\\'s directory directly. This can lead to permission issues because the user and group IDs within the container might not match those on the host.\\\\n   - **Docker Volumes**: These are managed by Docker and typically do not have permission issues since they operate within Docker’s internal storage.\\\\n\\\\n2. **Common Errors**:  \\\\n   - You might encounter `could not change permissions of directory \\\\\"/var/lib/postgresql/data\\\\\": Operation not permitted`. This suggests that the executing user in the container lacks necessary permissions.\\\\n   - On Windows, ensure paths are correctly formatted (e.g., using `/c/` or `//c/`) and avoid spaces in directory names.\\\\n\\\\n3. **Configuration Example**:  \\\\n   ```yaml  \\\\n   version: \\'3.8\\'  \\\\n   services:  \\\\n     postgres:  \\\\n       image: postgres:15-alpine  \\\\n       volumes:  \\\\n         - pg-data:/var/lib/postgresql/data  \\\\n   volumes:  \\\\n     pg-data:  \\\\n   ```\\\\n\\\\n4. **Fixing Permission Problems**:  \\\\n   - Change the ownership or permissions of the host directory:\\\\n     ```bash  \\\\n     sudo chown -R $(whoami):$(whoami) /path/to/your/directory  \\\\n     sudo chmod -R 755 /path/to/your/directory  \\\\n     ```\\\\n   - Ensure the container runs as a specific user with access:\\\\n     ```bash  \\\\n     docker run -u \\\\\"$(id -u):$(id -g)\\\\\" -v /host/path:/container/path your_image  \\\\n     ```\"}', call_id='call_i8dwrgwgm6S80VnRNDAVfRPY', name='add_entry', type='function_call', id='fc_68a8121797f88190ac6d65bdf812330b07bbb7cb1cad6ed8', status='completed')</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>null</pre>\n",
       "            </div>\n",
       "\n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>The information has been successfully added to the FAQ database. If you have any more topics to add or need further assistance, just let me know! </p>\n",
       "<p>Is there anything else you’d like to discuss regarding Docker or any other topic?</p></div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "You: stop\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chat ended\n"
     ]
    }
   ],
   "source": [
    "chat.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e3f9e3-2c8a-4b6c-b93e-bb5808844e4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
